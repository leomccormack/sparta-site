var suggestions=document.getElementById('suggestions'),userinput=document.getElementById('userinput');document.addEventListener('keydown',inputFocus);function inputFocus(a){a.keyCode===191&&(a.preventDefault(),userinput.focus()),a.keyCode===27&&(userinput.blur(),suggestions.classList.add('d-none'))}document.addEventListener('click',function(a){var b=suggestions.contains(a.target);b||suggestions.classList.add('d-none')}),document.addEventListener('keydown',suggestionFocus);function suggestionFocus(b){const d=suggestions.querySelectorAll('a'),e=[...d],a=e.indexOf(document.activeElement);let c=0;b.keyCode===38?(b.preventDefault(),c=a>0?a-1:0,d[c].focus()):b.keyCode===40&&(b.preventDefault(),c=a+1<e.length?a+1:a,d[c].focus())}(function(){var a=new FlexSearch.Document({tokenize:"forward",cache:100,document:{id:'id',store:["href","title","description"],index:["title","description","content"]}});a.add({id:0,href:"https://leomccormack.github.io/sparta-site/docs/plugins/overview/",title:"Overview",description:"What's included in the SPARTA installer?",content:'\u003cp\u003eThe SPARTA installer also includes the COMPASS suite, the HO-DirAC suite, CroPaC Binaural Decoder, and the HO-SIRR room impulse response renderer.\nThese plug-ins employ parametric processing and are signal-dependent. They aim to go beyond conventional linear Ambisonics algorithms, used by the baseline SPARTA plug-ins, by extracting meaningful parameters over time and subsequently employing them to map the input to the output in an adaptive and informed manner.\u003c/p\u003e\n\u003cp\u003eNote that example REAPER projects may be \u003ca href="https://github.com/leomccormack/sparta-reaper-examples"\u003e\u003cstrong\u003efound here\u003c/strong\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id="all-included-plug-ins"\u003eAll included plug-ins\u003c/h2\u003e\n\u003ch3 id="sparta-plug-ins"\u003eSPARTA plug-ins\u003c/h3\u003e\n\u003cp\u003eMore detailed descriptions can be found in \u003ca href="/docs/plugins/sparta-suite/"\u003eSPARTA Suite →\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href="../sparta-suite/#ambibin"\u003e\u003cstrong\u003esparta_ambiBIN\u003c/strong\u003e\u003c/a\u003e - A binaural ambisonic decoder (up to 7th order) with a built-in SOFA loader and head-tracking support via OSC messages. Includes: Least-Squares (LS), spatial re-sampling (SPR, virtual loudspeakers), time-alignment (TA), and magnitude least-squares (Mag-LS) decoding options.\u003c/li\u003e\n\u003cli\u003e\u003ca href="../sparta-suite/#ambidec"\u003e\u003cstrong\u003esparta_ambiDEC\u003c/strong\u003e\u003c/a\u003e - A frequency-dependent loudspeaker ambisonic decoder (up to 7th order) with user specifiable loudspeaker directions (up to 64), which may be optionally imported via JSON configuration files. Includes: All-Round (AllRAD), Energy-Preserving (EPAD), Spatial (SAD), and Mode-Matching (MMD) ambisonic decoding options. The loudspeaker signals may also be binauralised for headphone playback.\u003c/li\u003e\n\u003cli\u003e\u003ca href="../sparta-suite/#ambidrc"\u003e\u003cstrong\u003esparta_ambiDRC\u003c/strong\u003e\u003c/a\u003e - A frequency-dependent dynamic range compressor for ambisonic signals (up to 7th order).\u003c/li\u003e\n\u003cli\u003e\u003ca href="../sparta-suite/#ambienc"\u003e\u003cstrong\u003esparta_ambiENC\u003c/strong\u003e\u003c/a\u003e - An ambisonic encoder/panner (up to 7th order), with support for up to 64 input channels; the directions for which may also be imported via JSON configuration files.\u003c/li\u003e\n\u003cli\u003e\u003ca href="../sparta-suite/#ambiroomsim"\u003e\u003cstrong\u003esparta_ambiRoomSim\u003c/strong\u003e\u003c/a\u003e - A shoebox room simulator based on the image-source method, supporting multiple sources and ambisonic receivers.\u003c/li\u003e\n\u003cli\u003e\u003ca href="../sparta-suite/#array2sh"\u003e\u003cstrong\u003esparta_array2sh\u003c/strong\u003e\u003c/a\u003e - A microphone array Ambisonics encoder (up to 7th order), with presets for several commercially available A-format and higher-order microphone arrays. The plug-in can also present objective evaluation metrics for the currently selected configuration.\u003c/li\u003e\n\u003cli\u003e\u003ca href="../sparta-suite/#beamformer"\u003e\u003cstrong\u003esparta_beamformer\u003c/strong\u003e\u003c/a\u003e - A spherical harmonic domain beamforming plug-in with multiple beamforming strategies (up to 64 output beams).\u003c/li\u003e\n\u003cli\u003e\u003ca href="../sparta-suite/#binauraliser"\u003e\u003cstrong\u003esparta_binauraliser\u003c/strong\u003e\u003c/a\u003e - A binaural panner (up to 64 input channels) with a built-in SOFA loader and head-tracking support via OSC messages.\u003c/li\u003e\n\u003cli\u003e\u003ca href="../sparta-suite/#binauralisernf"\u003e\u003cstrong\u003esparta_binauraliserNF\u003c/strong\u003e\u003c/a\u003e - Binauraliser, with the addition of proximity filtering for near field sound sources.\u003c/li\u003e\n\u003cli\u003e\u003ca href="../sparta-suite/#decorrelator"\u003e\u003cstrong\u003esparta_decorrelator\u003c/strong\u003e\u003c/a\u003e - A simple multi-channel signal decorrelator (up to 64 input channels).\u003c/li\u003e\n\u003cli\u003e\u003ca href="../sparta-suite/#dirass"\u003e\u003cstrong\u003esparta_dirass\u003c/strong\u003e\u003c/a\u003e - A sound-field visualiser based on re-assigning the energy of beamformers. This re-assignment is based on DoA estimates extracted from \u0026ldquo;spatially-constrained\u0026rdquo; regions, which are centred around each beamformer look-direction.\u003c/li\u003e\n\u003cli\u003e\u003ca href="../sparta-suite/#matrixconv"\u003e\u003cstrong\u003esparta_matrixconv\u003c/strong\u003e\u003c/a\u003e - A basic matrix convolver with an optional partitioned convolution mode. The user need only specify the number of inputs and load the filters via a wav file.\u003c/li\u003e\n\u003cli\u003e\u003ca href="../sparta-suite/#multiconv"\u003e\u003cstrong\u003esparta_multiconv\u003c/strong\u003e\u003c/a\u003e - A basic multi-channel convolver with an optional partitioned convolution mode. Unlike \u0026ldquo;MatrixConv\u0026rdquo;, this plug-in does not perform any matrixing. Instead, each input channel is convolved with the respective filter; i.e. numInputs = numFilters = numOutputs.\u003c/li\u003e\n\u003cli\u003e\u003ca href="../sparta-suite/#6dofconv"\u003e\u003cstrong\u003esparta_6DoFconv\u003c/strong\u003e\u003c/a\u003e - A time-varying partitioned convolution multi-channel convolver for SOFA files containing RIRs with multiple listener positions.\u003c/li\u003e\n\u003cli\u003e\u003ca href="../sparta-suite/#panner"\u003e\u003cstrong\u003esparta_panner\u003c/strong\u003e\u003c/a\u003e - A 3D loudspeaker array panner using the VBAP method (up to 64 inputs and outputs).\u003c/li\u003e\n\u003cli\u003e\u003ca href="../sparta-suite/#powermap"\u003e\u003cstrong\u003esparta_powermap\u003c/strong\u003e\u003c/a\u003e - A sound-field visualisation plug-in based on ambisonic signals as input (up to 7th order), with PWD/MVDR/MUSIC/Min-Norm options.\u003c/li\u003e\n\u003cli\u003e\u003ca href="../sparta-suite/#rotator"\u003e\u003cstrong\u003esparta_rotator\u003c/strong\u003e\u003c/a\u003e - A flexible ambisonic rotator (up to 7th order) with head-tracking support via OSC messages.\u003c/li\u003e\n\u003cli\u003e\u003ca href="../sparta-suite/#sldoa"\u003e\u003cstrong\u003esparta_sldoa\u003c/strong\u003e\u003c/a\u003e - A frequency-dependent sound-field visualiser (up to 7th order), based on depicting the direction-of-arrival (DoA) estimates derived from spatially localised active-intensity vectors. The low frequency estimates are shown with blue icons, mid-frequencies with green, and high-frequencies with red.\u003c/li\u003e\n\u003cli\u003e\u003ca href="../sparta-suite/#spreader"\u003e\u003cstrong\u003esparta_spreader\u003c/strong\u003e\u003c/a\u003e - An arbitrary array (e.g. HRIRs or microphone array IRs) panner with coherent and incoherent spreading options.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id="compass-plug-ins"\u003eCOMPASS plug-ins\u003c/h3\u003e\n\u003cp\u003eMore detailed descriptions can be found in \u003ca href="/docs/plugins/compass-suite/"\u003eCOMPASS Suite →\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href="../compass-suite/#binaural"\u003e\u003cstrong\u003ecompass_binaural\u003c/strong\u003e\u003c/a\u003e - An adaptive binaural ambisonic decoder (up to 3rd order input) based on the parametric COMPASS model, with a built-in SOFA loader and head-tracking support via OSC messages.\u003c/li\u003e\n\u003cli\u003e\u003ca href="../compass-suite/#binauralvr"\u003e\u003cstrong\u003ecompass_binauralVR\u003c/strong\u003e\u003c/a\u003e - Same as the compass_binaural plugin, but also supporting listener translation around the receiver position and support for multiple simultaneous listeners.\u003c/li\u003e\n\u003cli\u003e\u003ca href="../compass-suite/#decoder"\u003e\u003cstrong\u003ecompass_decoder\u003c/strong\u003e\u003c/a\u003e - A parametrically enhanced loudspeaker ambisonic decoder (up to 3rd order input).\u003c/li\u003e\n\u003cli\u003e\u003ca href="../compass-suite/#6dof"\u003e\u003cstrong\u003ecompass_6dof\u003c/strong\u003e\u003c/a\u003e - A six degrees-of-freedom (6DoF) renderer based on multiple Ambisonic receivers as input, which supports listener translation both within and beyond the convex hull of the receiver arrangement.\u003c/li\u003e\n\u003cli\u003e\u003ca href="../compass-suite/#gravitator"\u003e\u003cstrong\u003ecompass_gravitator\u003c/strong\u003e\u003c/a\u003e - A parametric sound-field focussing plug-in.\u003c/li\u003e\n\u003cli\u003e\u003ca href="../compass-suite/#sidechain"\u003e\u003cstrong\u003ecompass_sidechain\u003c/strong\u003e\u003c/a\u003e - A plug-in that manipulates the spatial properties of one Ambisonic recording based on the spatial analysis of a different recording.\u003c/li\u003e\n\u003cli\u003e\u003ca href="../compass-suite/#spatedit"\u003e\u003cstrong\u003ecompass_spatedit\u003c/strong\u003e\u003c/a\u003e - A flexible spatial editing plug-in.\u003c/li\u003e\n\u003cli\u003e\u003ca href="../compass-suite/#tracker"\u003e\u003cstrong\u003ecompass_tracker\u003c/strong\u003e\u003c/a\u003e - A multiple target acoustic tracker which can optionally steer a beamformer towards each target.\u003c/li\u003e\n\u003cli\u003e\u003ca href="../compass-suite/#upmixer"\u003e\u003cstrong\u003ecompass_upmixer\u003c/strong\u003e\u003c/a\u003e - An Ambisonic upmixer (1-3rd order input, 2-7th order output).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id="ho-dirac-plug-ins"\u003eHO-DirAC plug-ins\u003c/h3\u003e\n\u003cp\u003eMore detailed descriptions can be found in \u003ca href="/docs/plugins/hodirac-suite/"\u003eHO-DirAC Suite →\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eNote these plug-ins are released under their own \u003ca href="../hodirac-suite/#license"\u003elicensing terms\u003c/a\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href="../hodirac-suite/#binaural"\u003e\u003cstrong\u003ehodirac_binaural\u003c/strong\u003e\u003c/a\u003e - An adaptive binaural ambisonic decoder (up to 3rd order input) based on the parametric HO-DirAC model, with a built-in SOFA loader and head-tracking support via OSC messages.\u003c/li\u003e\n\u003cli\u003e\u003ca href="../hodirac-suite/#decoder"\u003e\u003cstrong\u003ehodirac_decoder\u003c/strong\u003e\u003c/a\u003e - A parametrically enhanced loudspeaker ambisonic decoder (up to 3rd order input).\u003c/li\u003e\n\u003cli\u003e\u003ca href="../hodirac-suite/#upmixer"\u003e\u003cstrong\u003ehodirac_upmixer\u003c/strong\u003e\u003c/a\u003e - An Ambisonic upmixer (1-3rd order input, 2-7th order output).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id="other-plug-ins"\u003eOther plug-ins\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href="../hades/#plug-in-description"\u003e\u003cstrong\u003ehades_renderer\u003c/strong\u003e\u003c/a\u003e - A flexible microphone array to binaural renderer.\u003c/li\u003e\n\u003cli\u003e\u003ca href="../cropac-binaural/#plug-in-description"\u003e\u003cstrong\u003ecropac_binaural\u003c/strong\u003e\u003c/a\u003e - An adaptive binaural first.order ambisonic decoder based on the parametric CroPaC model, with a built-in SOFA loader and head-tracking support via OSC messages.\u003c/li\u003e\n\u003cli\u003e\u003ca href="../hosirr/#application-description"\u003e\u003cstrong\u003eHOSIRR\u003c/strong\u003e\u003c/a\u003e - An Ambisonic room impulse response (RIR) renderer for arbitrary loudspeaker setups, based on the Higher-order Spatial Impulse Response Rendering (HO-SIRR) algorithm.\u003c/li\u003e\n\u003cli\u003e\u003ca href="../ultrasonicsuperhearing/#UltrasonicSuperHearing"\u003e\u003cstrong\u003eUltrasonicSuperHearing\u003c/strong\u003e\u003c/a\u003e - A plug-in for the binaural reproduction of ultrasonic sound sources captured using a 6-sensor ultrasonic microphone array, which uses direction-of-arrival estimation and pitch-down-shifting prior to binauralisation in order to provide the appropriate localisation cues.\u003c/li\u003e\n\u003c/ul\u003e\n'}).add({id:1,href:"https://leomccormack.github.io/sparta-site/docs/plugins/sparta-suite/",title:"SPARTA suite",description:"Plug-in descriptions for the SPARTA suite.",content:'\u003ch2 id="plug-in-descriptions"\u003ePlug-in descriptions\u003c/h2\u003e\n\u003cp\u003eAll plug-ins are tested using REAPER (64-bit), which is a very affordable and flexible DAW and is the most recommended host for these plug-ins; although, other multi-channel friendly hosts (such as MaxMSP, Plogue Bidule) are also known to work. Currently, the plug-ins support sampling rates of 44.1 or 48kHz. All Ambisonics-related plug-ins conform to the Ambisonic Channel Number (ACN) ordering convention and offer support for both orthonormalised (N3D) and semi-normalised (SN3D) normalisation schemes (note that the AmbiX format refers to ACN/SN3D). The maximum transform order for these plug-ins is 7.\u003c/p\u003e\n\u003cp\u003eYou may hover your mouse cursor over any of the combo boxes/sliders/toggle buttons etc., in order to be presented with helpful tooltips regarding the purpose of the respective parameter.\u003c/p\u003e\n\u003cp\u003eThanks to the efforts of Daniel Rudrich, the relevant plug-ins also support importing and exporting loudspeaker, source, and sensors directions via .json configuration files; thus allowing for cross-compatibility between SPARTA and the IEM Ambisonics plug-in suite. More information regarding the structure of these files can be \u003ca href="https://plugins.iem.at/docs/configurationfiles/"\u003efound here\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe default HRIR set is an 836-point simulation of a Kemar Dummy head, courtesy of Genelec AuralID.\u003c/p\u003e\n\u003ch3 id="ambibin"\u003eAmbiBIN\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="AmbiBIN_GUI.png" alt="AmbiBIN_GUI" caption="\u003cem\u003eAmbiBIN_GUI\u003c/em\u003e" class="border-0" style="max-width: 80%;"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eA binaural Ambisonic decoder for headphone playback of Ambisonic signals (aka spherical harmonic or B-format signals), with a built-in rotator and head-tracking support via OSC messages. The plug-in also allows the user to import their own HRIRs via the SOFA standard. The plug-in offers a variety of different decoding methods, including: Least-Squares (LS), Spatial re-sampling (SPR, virtual loudspeakers), Time-Alignment (TA) [10], and Magnitude Least-Squares (MagLS) [11]. It can also impose a diffuse-coherence constraint for the current decoder, as described in [10].\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack, Archontis Politis, and Christoph Hold.\u003c/p\u003e\n\u003ch3 id="ambidec"\u003eAmbiDEC\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="AmbiDEC_GUI.png" alt="" style="max-width: 80%;"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eA frequency-dependent Ambisonic decoder for loudspeakers. The loudspeaker directions can be user-specified for up to 64 channels, or alternatively presets for popular 2D and 3D set-ups can be selected. For headphone reproduction, the loudspeaker audio is convolved with interpolated HRTFs for each loudspeaker direction (i.e. virtual loudspeaker decoding). The plug-in also permits importing custom HRIRs via the SOFA standard.\u003c/p\u003e\n\u003cp\u003eThe plug-in employs a dual decoding approach, whereby different decoder settings may be selected for the low and high frequencies; the cross-over frequency may be dictated by the user. Several ambisonic decoders have been integrated, including the All-Round Ambisonic Decoder (AllRAD) [1] and Energy-Preserving Ambisonic Decoder (EPAD) [2]. The popular max-rE weighting/spatial-tapering [1] may also be enabled for either decoder. Furthermore, in the case of non-ideal Ambisonic signals as input (i.e. those derived from physical/simulated microphone arrays), the decoding order may be specified for the appropriate frequency ranges; energy-preserving (EP) or amplitude-preserving (AP) normalisation is used to maintain consistent loudness between different decoding orders. This feature may, of course, also be used creatively.\u003c/p\u003e\n\u003cp\u003eNote that when the loudspeakers are uniformly distributed (e.g. a t-design), the included decoding approaches, except for AllRAD, all become equivalent. The benefits of the Mode-Matching decoding (MMD), AllRAD and EPAD approaches may be observed for non-uniform arrangements (22.x for example).\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Archontis Politis.\u003c/p\u003e\n\u003ch3 id="ambidrc"\u003eAmbiDRC\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="AmbiDRC_GUI.png" alt="" style="max-width: 70%;"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eThe AmbiDRC plug-in is related to \u003ca href="../../help/related-publications/mccormack2017fft.pdf"\u003e\u003cstrong\u003ethis publication\u003c/strong\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eA frequency-dependent Ambisonic dynamic range compressor (DRC). The gain factors are derived by analysing the omnidirectional component for each frequency band, which are then applied also to the higher-order components. The spatial properties of the original signals remains unchanged; although, your perception of them after decoding may change. The implementation also keeps track of the frequency-dependent gain factors for the omnidirectional component over time, which is then plotted on the user interface for visual feedback.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack.\u003c/p\u003e\n\u003ch3 id="ambienc"\u003eAmbiENC\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="AmbiENC_GUI.png" alt="" style="max-width: 80%"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eA bare-bones Ambisonic encoder which takes input signals (up to 64 channels) and encodes them into Ambisonic signals at specified directions. These Ambisonic signals describe a synthetic sound scene, where the spatial resolution of the sound scene is determined by the encoding order. Several presets have been included for convenience, which allow for 22.x etc. audio to be encoded into 1-7th order Ambisonics, for example. The panning window is also fully mouse driven, and uses an equirectangular representation of the sphere to depict the azimuth and elevation angles of each source.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack.\u003c/p\u003e\n\u003ch3 id="ambiroomsim"\u003eAmbiRoomSim\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="ambiRoomSim_GUI.png" alt="" style="max-width: 95%"  /\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eAn Ambisonic encoder which includes room reflections. It is based on the image source method using a shoebox room model. It permits multiple sources, and also multiple Ambisonic receivers up to 64 channels in total; e.g. 16x first-order or 4x third-order receivers, or 1x 7th order receiver. The output receiver channels are stacked, e.g. 1-4 channels are for the 1st first-order receiver, 5-8 for the second etc.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack.\u003c/p\u003e\n\u003ch3 id="array2sh"\u003eArray2SH\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="Array2SH_GUI.png" alt="" style="max-width: 95%"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eThe Array2SH plug-in is related to \u003ca href="../../help/related-publications/mccormack2018real.pdf"\u003e\u003cstrong\u003ethis publication\u003c/strong\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u0026lsquo;Array2SH\u0026rsquo; spatially encodes spherical/cylindrical array signals into spherical harmonic (SH) signals, which are also referred to as Ambisonic or B-Format signals. The plug-in uses analytical descriptors, which ascertain the frequency and order-dependent influence that the physical properties of the array have on the plane-waves arriving at its surface. The plug-in allows the user to specify: the array type (spherical or cylindrical), whether the array has an open or rigid enclosure, the radius of the array, the radius of the sensors (in cases where they protrude out from the array), the sensor coordinates (up to 64 channels), sensor directivity (omni-dipole-cardioid), the speed of sound, and the acoustical admittance of the array material (in the case of rigid arrays). The plug-in then determines the order-dependent equalisation curves that need to be imposed onto the initial spherical harmonic signals estimate, in order to remove the influence of the array itself. However, especially for higher-orders, this generally results in a large amplification of the low frequencies (including the sensor noise at these frequencies that accompanies it); therefore, four different regularisation approaches have been integrated into the plug-in, which allow the user to make a compromise between noise amplification and transform accuracy. These target and regularised equalisation curves are depicted on the user interface to provide visual feedback.\u003c/p\u003e\n\u003cp\u003eThe plug-in also allows the user to \u0026lsquo;Analyse\u0026rsquo; the spatial encoding performance using objective measures described in [7,8], namely: the spatial correlation and the level difference. Here, the encoding matrices are applied to a simulated array, which is described by multichannel transfer functions of plane waves for 812 points on the surface of the spherical/cylindrical array. The resulting encoded array responses should ideally resemble spherical harmonic functions at the grid points. The spatial correlation is then derived by comparing the patterns of these responses with the patterns of ideal spherical harmonics, where \u0026lsquo;1\u0026rsquo; means they are perfect, and \u0026lsquo;0\u0026rsquo; completely uncorrelated; the spatial aliasing frequency can therefore be observed for each order, as the point where the spatial correlation tends towards 0. The level difference is then the mean level difference over all directions (diffuse level difference) between the ideal and simulated components. One can observe that higher permitted amplification limits [Max Gain (dB)] will result in noisier signals; however, this will also result in a wider frequency range of useful spherical harmonic components at each order. This analysis is primarily based on code written for publication [8], which compared the performance of various regularisation approaches of encoding filters, based on both theoretical and measured array responses.\u003c/p\u003e\n\u003cp\u003eNote that this ability to balance the noise amplification with the accuracy of the spatial encoding (to better suit a given application) is very important, for example: the perceived fidelity of Ambisonic decoded audio can be rather poor if the noise amplification is set too high; therefore, typically a much lower amplification regularisation limit is used in Ambisonics reproduction when compared to sound-field visualisation algorithms, or beamformers that employ appropriate post-filtering.\u003c/p\u003e\n\u003cp\u003eFor convenience, the specifications for several commercially available microphone arrays have been integrated as presets; including: MH Acoustic\u0026rsquo;s Eigenmike, the Zylia array, and various A-format microphone arrays. Additionally, by releasing this plug-in, one now has the ability to build/3D print their own spherical and cylindrical array, while having a convenient means of obtaining the corresponding spherical harmonic signals; for example, a four capsule open-body hydrophone array was presented in [9], which utilised this Array2SH plug-in as the first step in visualising and auralising an underwater sound scene in real-time.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack, Symeon Delikaris-Manias and Archontis Politis.\u003c/p\u003e\n\u003ch3 id="beamformer"\u003eBeamformer\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="Beamformer_GUI.png" alt="" style="max-width: 80%"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eA simple beamforming plug-in, currently including the following static beam patterns: cardioid, hyper-cardioid, or max_rE weighted hyper-cardioid.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack.\u003c/p\u003e\n\u003ch3 id="binauraliser"\u003eBinauraliser\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="Binauraliser_GUI.png" alt="" style="max-width: 95%"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cdiv class="alert alert-warning d-flex" role="alert"\u003e\n  \u003cdiv class="flex-shrink-1 alert-icon"\u003e👉 \u003c/div\u003e\n  \n    \u003cdiv class="w-100"\u003ePlease note that this plug-in is only suitable for HRTF-based convolution. \u003c/div\u003e\n  \n\u003c/div\u003e\n\n\u003cp\u003eA plug-in which convolves input audio (up to 64 channels) with interpolated HRTFs in the time-frequency domain. The HRTFs are interpolated by applying amplitude-normalised VBAP gains [4] to the HRTF magnitude responses and the estimated inter-aural time differences (ITDs) individually, before being re-combined. The plug-in also allows the user to specify an external SOFA file for the convolution. Presets for popular 2D and 3D formats are included for convenience; however, the directions for up to 64 channels can be independently controlled. Head-tracking is also supported via OSC messages in the same manner as with the Rotator plug-in.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Archontis Politis.\u003c/p\u003e\n\u003ch3 id="binauralisernf"\u003eBinauraliserNF\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="BinauraliserNF_GUI.png" alt="" style="max-width: 95%"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cdiv class="alert alert-warning d-flex" role="alert"\u003e\n  \u003cdiv class="flex-shrink-1 alert-icon"\u003e👉 \u003c/div\u003e\n  \n    \u003cdiv class="w-100"\u003ePlease note that this plug-in is only suitable for HRTF-based convolution. \u003c/div\u003e\n  \n\u003c/div\u003e\n\n\u003cp\u003eThis plug-in mirrors has the functionality of the \u003ca href="#binauraliser"\u003e\u003cstrong\u003eBinauraliser\u003c/strong\u003e\u003c/a\u003e, with the addition of proximity filtering which reproduces the binaural effects of nearby sound sources. The implementation follows a shelving filter scheme as described in [15].\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Michael McCrea, Leo McCormack, and Sebastian Schlecht.\u003c/p\u003e\n\u003ch3 id="decorrelator"\u003eDecorrelator\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="decorrelator_GUI.png" alt="" style="max-width: 65%"/\u003e \u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eA simple multi-channel signal decorrelator (up to 64 channels) based on randomised time-frequency delays and cascaded all-pass filters.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack.\u003c/p\u003e\n\u003ch3 id="dirass"\u003eDirASS\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="DirASS_GUI.png" alt="" style="max-width: 90%"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eThe DirASS plug-in is related to these publications: \u003ca href="../../help/related-publications/mccormack2019applications.pdf"\u003e\u003cstrong\u003eLink1\u003c/strong\u003e\u003c/a\u003e, \u003ca href="../../help/related-publications/mccormack2019sharpening.pdf"\u003e\u003cstrong\u003eLink2\u003c/strong\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eA sound-field visualiser, which is based on the directional re-assignment of beamformer energy. This energy re-assignment is based on local DoA estimates for each scanning direction, and may be quantised to the nearest direction or upscaled to a higher-order than the input; resulting in sharper activity-maps. For example, a second-order input may be displayed with (up to) 20th order output resolution. The plug-in also allows the user to place real-time video footage behind the activity-map, in order to create a make-shift acoustic camera.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Archontis Politis.\u003c/p\u003e\n\u003ch3 id="matrixconv"\u003eMatrixConv\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="MatrixConv_GUI.png" alt="" style="max-width: 70%"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eA simple matrix convolver with an (optional) partitioned-convolution mode. The matrix of filters should be concatenated for each output channel and loaded as a .wav file. You need only inform the plug-in of the number of input channels, and it will take care of the rest.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExample 1, \u003cstrong\u003espatial reverberation\u003c/strong\u003e: if you have a B-Format/Ambisonic room impulse response (RIR), you may convolve it with a monophonic input signal and the output will exhibit (much of) the spatial characteristics of the measured room. Simply load this Ambisonic RIR into the plug-in and set the number of input channels to 1. You may then decode the resulting Ambisonic output to your loudspeaker array (e.g. using SPARTA|AmbiDEC) or to headphones (e.g. using SPARTA|AmbiBIN). However, please note that the limitations of lower-order Ambisonics for signals (namely, colouration and poor spatial accuracy) will also be present with lower-order Ambisonic RIRs; at least, when applied in this manner. Consider referring to Example 3, for a more spatially accurate method of reproducing the spatial characteristics of rooms, which are captured as B-Format/Ambisonic RIRs.\u003c/li\u003e\n\u003cli\u003eExample 2, \u003cstrong\u003emicrophone array to Ambisonics encoding\u003c/strong\u003e: if you have a matrix of filters to go from an Eigenmike (32 channel) recording to 4th order Ambisonics (25 channel), then the plugin requires a 25-channel wav file to be loaded, and the number of input channels to be set to 32. In this case: the first 32 filters will map the input to the first output channel, filters 33-64 will map the input to the second output channel, \u0026hellip; , and the last 32 filters will map the input to the 25th output channel. An example of such an encoding matrix may be downloaded from \u003ca href="http://research.spa.aalto.fi/projects/sparta_vsts/extras/M_eigen2sh_radinv_o4_ACN_N3D_15dB_max.wav"\u003ehere\u003c/a\u003e. Note that these example filters employ the ACN/\u003cstrong\u003eN3D\u003c/strong\u003e convention, Tikhonov regularisation, and 15dB of maximum gain amplification; using the Matlab scripts from \u003ca href="https://github.com/polarch/Spherical-Array-Processing"\u003e\u003cstrong\u003ehere\u003c/strong\u003e\u003c/a\u003e. This should be the same as SPARTA|Array2SH when it is set to the Eigenmike preset and default settings (except N3D not SN3D).\u003c/li\u003e\n\u003cli\u003eExample 3, \u003cstrong\u003emore advanced spatial reverberation\u003c/strong\u003e: if you have a monophonic recording of a trumpet and you wish to reproduce it as if it were in your favourite concert hall, first measure a B-Format/Ambisonic room impulse response (RIR) of the hall, and then convert this Ambisonic RIR to your loudspeaker set-up using \u003ca href="../hosirr"\u003eHO-SIRR\u003c/a\u003e. Then load the resulting rendered loudspeaker array RIR into the plug-in and set the number of input channels to 1. Note that you may prefer to use HO-SIRR (which is a parametric renderer), to convert your arbitrary order B-Format/Ambisonic IRs to arbitrary loudspeaker array IRs, as the resulting output will generally be much more spatially accurate when compared to linear (non-parametric) Ambisonic decoding; as described in Example 1. For the curious reader, an example of a 12point T-design loudspeaker array IR, made using a simulation [12] of the Vienna Musikverein concert hall, may be downloaded from \u003ca href="http://research.spa.aalto.fi/projects/sparta_vsts/extras/Vienna__12point-Tdesign.wav"\u003e\u003cstrong\u003ehere\u003c/strong\u003e\u003c/a\u003e. To listen to the convolved output, either arrange 12 loudspeakers in a t-design for the playback (a bit cumbersome), or use the SPARTA|Binauraliser plug-in set to \u0026ldquo;T-Design (12)\u0026rdquo; and listen over headphones.\u003c/li\u003e\n\u003cli\u003eExample 4, \u003cstrong\u003evirtual monitoring of a multichannel setup\u003c/strong\u003e: if you have a set of binaural head-related impulse responses (BRIRs) which correspond to the loudspeaker directions of a measured listening room, you may use this 2 x L matrix of filters to reproduce loudspeaker mixes (L-channels) over headphones. Simply concatenate the BRIRs for each input channel into a two channel wav file and load them into the plugin, then set the number of inputs to be the number of BRIRs/loudspeakers in the mix.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Archontis Politis.\u003c/p\u003e\n\u003ch3 id="multiconv"\u003eMultiConv\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="MultiConv_GUI.png" alt="" style="max-width: 70%"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eA simple multi-channel convolver with an (optional) partitioned-convolution mode. The plugin will convolve each input channel with the respective filter up to the maximum of 64 channels/filters. The filters are loaded as a multi-channel .wav file.\u003c/p\u003e\n\u003cp\u003ePlease note that this is not to be confused with the MatrixConv plug-in. For this plug-in, the number inputs = the number of filters = the number of outputs. i.e. no matrixing is applied.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExample, \u003cstrong\u003eheadphone equalisation\u003c/strong\u003e: post binauraliser/AmbiBIN etc., you may minimise the effect that your headphones have on the binaural output, by also convolving with (regularised) inverse filters. These filters may either be based on measurements of your own head and headphones, or you may download generic ones. For example, you can find equalisation filters for many commercially available headphones from \u003ca href="https://audiogroup.web.th-koeln.de/ku100hrir.html"\u003e\u003cstrong\u003ehere\u003c/strong\u003e\u003c/a\u003e; which have been measured using a dummy head (more information can be found in [13]).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Archontis Politis.\u003c/p\u003e\n\u003ch3 id="6dofconv"\u003e6DoFconv\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="6DoFconv_GUI.png" alt="" style="max-width: 90%"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eA time-varying partitioned convolution multi-channel convolver for SOFA files containing RIRs with multiple listener positions.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExample, \u003cstrong\u003espatial reverberation\u003c/strong\u003e: if you have a SOFA file of a dataset of B-format/Ambisonic spatial room impulse responses (SRIRs) with varying listener position, you may convolve one with a monophonic input signal, whereby the listener position (X, Y and Z coordinates) determines the SRIR selection. You may then decode the resulting Ambisonic output to your loudspeaker array (e.g. using SPARTA|AmbiDEC) or to headphones (e.g. using SPARTA|AmbiBIN). An example of SOFA files in a compatible format is the coupled room transition dataset (\u003ca href="https://doi.org/10.5281/zenodo.4095493"\u003ehttps://doi.org/10.5281/zenodo.4095493\u003c/a\u003e). For this, you may choose to use OSC signals in the format \\xyzypr (denoting positional coordinates x, y, z and rotation yaw, pitch and roll) to control the listener position and orientation (enable rotation of the output).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis plug-in was developed by Rapolas Daugintis, Thomas McKenzie, Nils Meyer-Kahlen and Leo McCormack.\u003c/p\u003e\n\u003ch3 id="panner"\u003ePanner\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="Panner_GUI.png" alt="" style="max-width: 95%"/\u003e \u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eA frequency-dependent 3D panner based on the Vector-base Amplitude Panning (VBAP) method [4]. Presets for popular 2D and 3D formats are included for convenience; however, the directions for up to 64 channels can be independently controlled for both inputs and outputs; allowing, for example, 9.x input audio to be panned for a 22.2 setup. The panning is frequency-dependent to accommodate the method described in [5], which allows for more consistent loudness when sources are panned in-between the loudspeaker directions.\u003c/p\u003e\n\u003cp\u003eSet the \u0026ldquo;Room Coeff\u0026rdquo; parameter to 0 for standard power-normalisation, 0.5 for a listening room, and 1 for an anechoic chamber.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack, Archontis Politis and Ville Pulkki.\u003c/p\u003e\n\u003ch3 id="powermap"\u003ePowerMap\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="Powermap_GUI.png" alt="" style="max-width: 90%"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eThe PowerMap plug-in is a modified version of the plug-in described in \u003ca href="../../help/related-publications/mccormack2017parametric.pdf"\u003e\u003cstrong\u003ethis publication\u003c/strong\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u0026lsquo;PowerMap\u0026rsquo; is a plug-in that represents the relative sound energy, or the statistical likelihood of a source, arriving at the listening position from a particular direction, using a colour gradient; where yellow indicates high sound energy/likelihood and blue indicates low sound energy/likelihood. The plug-in integrates a variety of different approaches, including: standard Plane-Wave Decomposition (PWD) beamformer-based, Minimum-Variance Distortionless Response (MVDR) beamformer-based, Multiple Signal Classification (MUSIC) pseudo-spectrum-based, and the Cross-Pattern Coherence (CroPaC) algorithm [3]; all of which are written to operate on Ambisonic signals up to 7th order. Note that the analysis order per frequency band is entirely user definable, and presets for higher order microphone arrays have been included for convenience, which provide some rough starting values. The plug-in utilises a 812 point uniformly-distributed spherical grid, which is then interpolated into a 2D powermap using amplitude-normalised VBAP gains (i.e. triangular interpolation). The plug-in also allows the user to place real-time video footage behind the activity-map, in order to create a make-shift acoustic camera.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Symeon Delikaris-Manias.\u003c/p\u003e\n\u003ch3 id="rotator"\u003eRotator\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="Rotator_GUI.png" alt="" style="max-width: 70%"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eThis plug-in applies a Ambisonic rotation matrix [6] to the input Ambisonic signals. The rotation angles can be controlled using a head tracker via OSC messages. Simply configure the headtracker to send a vector: \u0026lsquo;\\ypr[3]\u0026rsquo; to OSC port 9000 (default); where \\ypr[0], \\ypr[1], \\ypr[2] are the yaw-pitch-roll angles, in degrees, respectively. The angles can also be flipped +/- in order to support a wider range of devices. The rotation order (yaw-pitch-roll (default) or roll-pitch-yaw) can also be specified. Alternatively, the rotation can be based on a Quaternion by sending vector: \u0026lsquo;\\quaternion[4]\'; where \\quaternion[0], \\quaternion[1], \\quaternion[2], \\quaternion[3], are the W, X, Y, Z parts of the Quaternion, respectively.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack.\u003c/p\u003e\n\u003ch3 id="sldoa"\u003eSLDoA\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="SLDoA_GUI.png" alt="" style="max-width: 80%"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eThe SLDoA plug-in is related to these publications: \u003ca href="../../help/related-publications/mccormack2018real.pdf"\u003e\u003cstrong\u003eLink1\u003c/strong\u003e\u003c/a\u003e, \u003ca href="../../help/related-publications/mccormack2019applications.pdf"\u003e\u003cstrong\u003eLink2\u003c/strong\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eA spatially localised direction-of-arrival (DoA) estimator. The plug-in first uses VBAP beam patterns (for directions that are uniformly distributed on the surface of a sphere) to obtain spatially-biased zeroth and first-order signals, which are subsequently used for the active-intensity vector estimation; therefore, allowing for DoA estimation in several spatially-constrained sectors for each sub-band. The low frequency estimates are then depicted with blue icons, mid-frequencies with green, and high-frequencies with red. The size of the icon and its opacity correspond to the energy of the sector, which are normalised and scaled in ascending order for each frequency band. The plug-in employs two times as many sectors as the analysis order, with the exception of the first-order analysis, which uses the traditional active-intensity approach. The analysis order per frequency band is user definable, as is the frequency range at which to analyse. This approach to sound-field visualisation/DoA estimation represents a much more computationally efficient option, when compared to the algorithms that are integrated into the \u0026lsquo;Powermap\u0026rsquo; plug-in, for instance. The plug-in also allows the user to place real-time video footage behind the activity-map, in order to create a make-shift acoustic camera.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Symeon Delikaris-Manias.\u003c/p\u003e\n\u003ch3 id="spreader"\u003eSpreader\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="spreader_GUI.png" alt="" style="max-width: 95%"/\u003e \u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eThe Spreader plug-in is related to \u003ca href="../../help/related-publications/mccormack2021rendering.pdf"\u003e\u003cstrong\u003ethis publication\u003c/strong\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThis plugin uses an optimised framework for rendering spread sound sources over an arbitrary playback system, as described in [14]. In this particular implementation, the directional responses for the target system are loaded as impulse responses via the SOFA format (e.g., using HRIRs for binaural, or microphone array IRs for creating a synthetic recording). The algorithm is then tasked with mixing the input mono signals to produce the appropriate multi-channel signals (e.g. 2 for binaural playback, 4 or 32 for a synthetic tetrahedral microphone or Eigenmike array recording), such that the output signals create a diffuse, or rather: \u0026ldquo;incoherently spead\u0026rdquo; sound source. The solution is optimised to introduce decorrelated signal energy into the output only to the degree that is required to fulfill the target model. Therefore, the signal fidelity of the input signal is largely retained. The algorithm proposed in [14] is denoted as \u0026ldquo;OM\u0026rdquo; in this plug-in. For comparison, an unconstrained spatial covariance matching alternative is provided under the name \u0026ldquo;EVD\u0026rdquo;, which also fulfills the target model, but does so without the constraints for preserving signal fidelity. Additionally, a coherent spreading baseline (\u0026ldquo;BL\u0026rdquo;), which is commonly employed in the industry (e.g. game audio engines) is included, which can sound very unnatural when compared to the incoherent spreading alternatives.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Archontis Politis.\u003c/p\u003e\n\u003ch2 id="about-the-developers"\u003eAbout the developers\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLeo McCormack\u003c/strong\u003e: a doctoral candidate at Aalto University.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSymeon Delikaris-Manias\u003c/strong\u003e: post doctorate researcher at Aalto University, specialising in compact microphone array processing for DoA estimation and sound-field reproduction. His doctoral research included work on the Cross-Pattern Coherence (CroPaC) algorithm, which is a spatial post-filter optimised for high noise/reverberant environments.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eArchontis Politis\u003c/strong\u003e: post doctorate researcher at Tampere University, specialising in spatial sound recording and reproduction, acoustic scene analysis and microphone array processing.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVille Pulkki\u003c/strong\u003e: Professor at Aalto University, known for VBAP, SIRR, DirAC and eccentric behaviour.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eChristoph Hold\u003c/strong\u003e: a doctoral candidate at Aalto University.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="license"\u003eLicense\u003c/h2\u003e\n\u003cp\u003eAll of the plug-ins in the SPARTA suite may be used for academic, personal, and/or commercial use. The source code may also be used for commercial purposes, provided that the terms of the GPLv3 license are fulfilled. This requires that the original code and/or any derived works must also be open-sourced and made available under the same GPLv3 license, if it is to be used for commercial purposes.\u003c/p\u003e\n\u003ch2 id="references"\u003eReferences\u003c/h2\u003e\n\u003cp\u003e[1] Zotter, F., Frank, M. (2012). \u003cstrong\u003eAll-Round Ambisonic Panning and Decoding.\u003c/strong\u003e \u003cbr/\u003e Journal of the Audio Engineering Society, 60(10), 807-820.\u003c/p\u003e\n\u003cp\u003e[2] Zotter, F., Pomberger, H., Noisternig, M. (2012). \u003cstrong\u003eEnergy-Preserving Ambisonic Decoding.\u003c/strong\u003e \u003cbr\u003e Acta Acustica United with Acustica, 98(1), 37-47.\u003c/p\u003e\n\u003cp\u003e[3] Delikaris-Manias, S., Pulkki, V. (2013). \u003cstrong\u003eCross pattern coherence algorithm for spatial filtering applications utilizing microphone arrays.\u003c/strong\u003e \u003cbr\u003e IEEE Transactions on Audio, Speech, and Language Processing, 21(11), 2356-2367.\u003c/p\u003e\n\u003cp\u003e[4] Pulkki, V. (1997). \u003cstrong\u003eVirtual Sound Source Positioning Using Vector Base Amplitude Panning.\u003c/strong\u003e \u003cbr\u003eJournal of the Audio Engineering Society, 45(6), 456-466.\u003c/p\u003e\n\u003cp\u003e[5] Laitinen, M., Vilkamo, J., Jussila, K., Politis, A., Pulkki, V. (2014). \u003cstrong\u003eGain normalization in amplitude panning as a function of frequency and room reverberance.\u003c/strong\u003e \u003cbr\u003e55th International Conference of the AES. Helsinki, Finland.\u003c/p\u003e\n\u003cp\u003e[6] Ivanic, J., Ruedenberg, K. (1998). \u003cstrong\u003eRotation Matrices for Real Spherical Harmonics. Direct Determination by Recursion Page: Additions and Corrections.\u003c/strong\u003e \u003c/b\u003e \u003cbr\u003eJournal of Physical Chemistry A, 102(45), 9099-9100.\u003c/p\u003e\n\u003cp\u003e[7] Moreau, S., Daniel, J., Bertet, S. (2006). \u003cstrong\u003e3D sound field recording with higher order ambisonics-objective measurements and validation of spherical microphone.\u003c/strong\u003e \u003cbr\u003e in Audio Engineering Society Convention 120, Audio Engineering Society\u003c/p\u003e\n\u003cp\u003e[8] Politis, A., Gamper, H. (2017). \u003cstrong\u003eComparing Modelled And Measurement-Based Spherical Harmonic Encoding Filters For Spherical Microphone Arrays.\u003c/strong\u003e \u003cbr\u003e In IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA).\u003c/p\u003e\n\u003cp\u003e[9] Delikaris-Manias, S., McCormack, L., Huhtakallio, I., and Pulkki, V. (2018) \u003cstrong\u003eReal-time underwater spatial audio: a feasibility study.\u003c/strong\u003e \u003cbr\u003e in Audio Engineering Society Convention 144, Audio Engineering Society.\u003c/p\u003e\n\u003cp\u003e[10] Zaunschirm, M., Schörkhuber, C., and Höldrich, R. (2018). \u003cstrong\u003eBinaural rendering of Ambisonic signals by head-related impulse response time alignment and a diffuseness constraint.\u003c/strong\u003e \u003cbr\u003e The Journal of the Acoustical Society of America, 143(6), 3616-3627.\u003c/p\u003e\n\u003cp\u003e[11] Schörkhuber, C., Zaunschirm, M., and Höldrich, R. (2018). \u003cstrong\u003eBinaural Rendering of Ambisonic Signals via Magnitude Least Squares.\u003c/strong\u003e \u003cbr\u003e In Proceedings of the DAGA (Vol. 44).\u003c/p\u003e\n\u003cp\u003e[12] Favrot, S. and Buchholz, J.M., (2019).  \u003cstrong\u003eLoRA: A loudspeaker-based room auralization system.\u003c/strong\u003e \u003cbr\u003e Acta Acustica united with Acustica, 96(2), pp.364-375.\u003c/p\u003e\n\u003cp\u003e[13] Bernschutz, B., (2013). \u003cstrong\u003eA spherical far field HRIR/HRTF compilation of the Neumann KU 100.\u003c/strong\u003e \u003cbr\u003e In Proceedings of the 40th Italian (AIA) annual conference on acoustics and the 39th German annual conference on acoustics (DAGA) conference on acoustics (p. 29). AIA/DAGA.\u003c/p\u003e\n\u003cp\u003e[14] McCormack, L. Politis, A., and Pulkki, V., 2021, October. \u003cstrong\u003eRendering of source spread for arbitrary playback setups based on spatial covariance matching.\u003c/strong\u003e In 2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). IEEE.\u003c/p\u003e\n\u003cp\u003e[15] Spagnol, S., Tavazzi, E., and Avanzini, F. \u003cstrong\u003e“Distance rendering and perception of nearby virtual sound sources with a near-field filter model.”\u003c/strong\u003e \u003cbr/\u003e Applied Acoustics, vol. 115, pp. 61–73, Jan. 2017.\u003c/p\u003e\n\u003ch2 id="other-included-plug-ins"\u003eOther included plug-ins\u003c/h2\u003e\n\u003cp\u003eThe SPARTA installer also includes:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe COMPASS suite \u003ca href="/docs/plugins/compass-suite/"\u003eCOMPASS →\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eThe HO-DirAC suite \u003ca href="/docs/plugins/hodirac-suite/"\u003eHO-DirAC →\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eThe HO-SIRR application \u003ca href="/docs/plugins/hosirr/"\u003eHO-SIRR →\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eThe CroPaC-Binaural decoder \u003ca href="/docs/plugins/cropac-binaural/"\u003eCroPaC-Binaural →\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n'}).add({id:2,href:"https://leomccormack.github.io/sparta-site/docs/plugins/compass-suite/",title:"COMPASS suite",description:"Plug-in descriptions for the COMPASS suite.",content:'\u003ch2 id="plug-in-descriptions"\u003ePlug-in descriptions\u003c/h2\u003e\n\u003cp\u003eCOMPASS VSTs is a collection of flexible VST audio plug-ins for spatial audio production, manipulation, and reproduction, developed by Dr. Archontis Politis, Leo McCormack and Dr. Sakari Tervo in the Acoustics Lab at Aalto University.\u003c/p\u003e\n\u003cp\u003eCOMPASS is a framework for parametric spatial audio processing of sound scenes captured in the Ambisonics format. Parametric methods, such as Directional Audio Coding (DirAC) or HARPEX have gained notoriety recently for being able to achieve sharpness or envelopment beyond first or lower-order traditional Ambisonics playback, using the same lower-order Ambisonics signals. Contrary to the time-invariant linear processing of Ambisonics, which does not consider the sound components that comprise the sound scene, parametric methods assume a sound-field model for the sound scene and track the model parameters in the Ambisonics recording, in both time and frequency. The parameters are then used to render or upmix the sound scene flexibly to any playback system, without the constraints of lower-order Ambisonics. Furthermore, the spatial parameters allow flexible manipulation of the sound scene content in ways that are not possible with traditional Ambisonics processing.\u003c/p\u003e\n\u003cp\u003eThe COMPASS framework has been developed by Dr. Archontis Politis with contributions from Dr. Sakari Tervo and Leo McCormack, and published in \u003ca href="../../help/related-publications/politis2018compass.pdf"\u003e[1]\u003c/a\u003e. The method is quite general in its model and estimates multiple direct sound components in every time-frequency block, and an ambient component capturing reverberation and other diffuse sounds. Here is a table of the COMPASS model compared to other published parametric techniques (note that M is the number of channels):\u003c/p\u003e\n\u003cp\u003e\u003cimg src="parametric_ambisonic_models.png" alt="" style="max-width: 95%"\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003eIn COMPASS, the ambient component is also spatial and can have directionality, contrary to previous models that force it to be isotropic. The VST plugins apply this framework to different spatial audio production tasks. Note that the plugins are still work in progress and we expect to keep improving them in the future, however, we believe that they can already prove useful to users and creators.\u003c/p\u003e\n\u003ch3 id="decoder"\u003eDecoder\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="Decoder_GUI.png" alt="" style="max-width: 85%"\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003eThe COMPASS decoder is a parametric decoder for first, second, and third-order Ambisonics to arbitrary loudspeaker setups. The plugin offers the following functionality:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUser-specified loudspeaker angles for up to 64 channels, or alternatively, presets for popular 2D and 3D set-ups.\u003c/li\u003e\n\u003cli\u003eHeadphone binaural monitoring of the loudspeaker outputs, with support for user-provided personalised binaural filters (HRTFs) in the SOFA format.\u003c/li\u003e\n\u003cli\u003eBalance control between the extracted direct sound components and the ambient component, in frequency bands.\u003c/li\u003e\n\u003cli\u003eMixing control between fully parametric decoding and linear Ambisonic decoding, in frequency bands.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe \u0026ldquo;Diffuse-to-Direct\u0026rdquo; control allows the user to give more prominence to the direct sound components (an effect similar to subtle dereverberation), or to the ambient component (an effect similar to emphasising reverberation in the recording). When set in the middle, the two are balanced. Note that the parametric processing can be quite aggressive, and if one pushes it to fully direct rendering in a complex multi-source sound scene with FOA signals only, artefacts can easily appear. However, with more balanced settings, such artefacts  should become imperceptible.\u003c/p\u003e\n\u003cp\u003eThe \u0026ldquo;Linear-to-Parametric\u0026rdquo; control allows the user to mix the output between standard linear Ambisonic decoding and the COMPASS parametric decoding. This control can be used in cases where parametric processing sounds too aggressive, or if the user prefers some degree of increased localisation blur, offered by linear Ambisonic decoding.\u003c/p\u003e\n\u003cp\u003eThe plugin is considered by the authors a production tool and, due to its time-frequency processing, requires audio buffer sizes of at least 1024 samples. Hence we do not consider it as a low-latency plugin and therefore it is not suitable for interactive input. For cases such as interactive binaural rendering for VR with head-tracking, see the \u003cb\u003eCOMPASS|Binaural\u003c/b\u003e variant.\u003c/p\u003e\n\u003cp\u003eA video demonstrating the functionality of the plug-in can be viewed here:\u003c/p\u003e\n\u003ciframe src="https://player.vimeo.com/video/312282907" width="600px" height="300px" style="max-width: 100%" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen\u003e\u003c/iframe\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Archontis Politis.\u003c/p\u003e\n\u003ch3 id="binaural"\u003eBinaural\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="Binaural_GUI.png" alt="" style="max-width: 85%"\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003eThis plug-in is related to \u003ca href="../../help/related-publications/politis2018compass.pdf"\u003e\u003cstrong\u003ethis publication\u003c/strong\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThis is an optimised version of the COMPASS decoder for binaural playback, bypassing loudspeaker rendering and using binaural filters (HRTFs) directly, which can be user-provided and personalised with the SOFA format. For the plugin parameters, see the description of the \u003cb\u003eBinaural|Decoder\u003c/b\u003e above. Additionally the plugin can receive OSC rotation angles from a headtracker at a user specified port, in the yaw-pitch-roll convention.\u003c/p\u003e\n\u003cp\u003eThis version is intended mostly for head-tracked binaural playback of Ambisonic content at interactive update rates, usually in conjunction with a head-mounted display (HMD). The plugin requires an audio buffer size of at least 512 samples (~10msec at 48kHz). The averaging parameters can be used to make the parametric analysis and synthesis more or less responsive, providing the user with a means to adjust them optimally for a particular sound scene.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Archontis Politis.\u003c/p\u003e\n\u003ch3 id="binauralvr"\u003eBinauralVR\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="binauralVR_GUI.png" alt="" style="max-width: 95%"\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003eThis plug-in is related to \u003ca href="../../help/related-publications/mccormack2021parametric.pdf"\u003e\u003cstrong\u003ethis publication\u003c/strong\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eSame as the COMPASS|Binaural plug-in, except it also supports listener translation around the receiver position. The user must first select the assumed distance of the sources. For simplicity, it is assumed that all sources are projected onto the surface of a sphere. The plug-in may then be informed of the listener position and orientation, either via its user interface sliders, or by sending  the Cartesian coordinates and rotation angles outputted by an external tracking device; such as a virtual or augmented reality headset.\u003c/p\u003e\n\u003cp\u003eThe listener position and head orientation can be passed via an OSC message: \\xyzypr[6], where x,y,z are in metres, and y,p,r are the yaw-pitch-roll angles in degrees (right-hand-rule).\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Archontis Politis.\u003c/p\u003e\n\u003ch3 id="6dof"\u003e6DoF\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="COMPASS_6DoF_GUI.png" alt="" style="max-width: 100%"\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003eThis plug-in is related to \u003ca href="../../help/related-publications/mccormack2022object.pdf"\u003e\u003cstrong\u003ethis publication\u003c/strong\u003e\u003c/a\u003e, and an example REAPER project based on the scenario described in the publication can be \u003ca href="https://zenodo.org/record/5767394"\u003e\u003cstrong\u003efound here\u003c/strong\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eA parametric six degrees-of-freedom (6DoF) renderer based on multiple Ambisonic receivers as input, which supports listener translation both within and beyond the convex hull of the receiver arrangement. The source positions may either be specified or tracked similarly as described in \u003ca href="../../help/related-publications/mccormack2021real.pdf"\u003e[12]\u003c/a\u003e. The plug-in may be configured to either output only the isolated source object signals (one per track), or to spatialise the sound scene (including reverberation) from the perspective of the listener position/orientation over headphones or as Ambisonics output. If targetting Ambisonics, any Ambisonic decoder may then be used to auralise the translated sound scene.\u003c/p\u003e\n\u003cp\u003eNote that all receiver channels must be stacked on-top of eachother, for example: 3 first-order receivers should be assigned to input channels 1-4, 5-8, 9-12, respectively. Therefore, due to the 64-channel VST2 limitation, the plug-in supports either: 16x first-order, 7x second-order, 4x third-order receivers etc.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Archontis Politis.\u003c/p\u003e\n\u003ch3 id="tracker"\u003eTracker\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="tracker_GUI.png" alt="" style="max-width: 98%"\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003eThis plug-in is related to \u003ca href="../../help/related-publications/mccormack2021real.pdf"\u003e\u003cstrong\u003ethis publication\u003c/strong\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe Tracker plug-in builds on the spatial analysis conducted by the COMPASS framework, but instead of using the information for synthesising loudspeaker or binaural signals, a multi-source tracker is employed to associate the estimated directions with their corresponding sources/targets. Therefore, this VST can be used to visualise the trajectory of multiple sound sources present in an Ambisonic sound scene.\u003c/p\u003e\n\u003cp\u003eOptionally, a beamformer may then be steered to each target direction and outputted either as individual signals (one target signal per output channel; akin to decomposing the scene into its individual \u0026ldquo;stems\u0026rdquo;), or as a binauralisation of these individual \u0026ldquo;stems\u0026rdquo; (spatialised in their respective target directions).\u003c/p\u003e\n\u003cp\u003eNote that multi-source tracking has been an active research topic for several decades, but it is still considered to be a very difficult task. While we are confident in the robustness of this tracker and its implementation, there is still a learning curve to effectively tune the parameters for a specific sound scene/distribution. If the sound scene is very noisy or complex and encoded with first-order/lower resolution, then robust tracking may not even be possible. A general starting point is to first disable \u0026ldquo;Plot Targets\u0026rdquo; and tune the \u0026ldquo;Analysis Settings\u0026rdquo; until the direction estimates (plotted in the colour red) look reasonably clean. If you can make out the trajectories of the sources, then re-enable \u0026ldquo;Plot Targets\u0026rdquo; and try a few of the \u0026ldquo;Tracker Settings\u0026rdquo; presets and tune things from there. Note that each tracker parameter also has a tooltip describing how it influences the tracking, and the more these parameters/priors match the direction estimates distribution, the better the tracking will be.\u003c/p\u003e\n\u003cp\u003eThe tracker implementation builds on the Rao-Blackwelized Monte Carlo Data Association (RBMCDA) framework [11,12]. It should be noted that Sequential Monte Carlo (SMC) methods (also referred to as particle-filtering methods) involve making hundreds/thousands of hypotheses, which are then selected randomly based on their predicted likelihoods. Therefore, every time the tracker is run it will give you a different \u0026ldquo;answer\u0026rdquo; for the same input scene, but if the tracker is tuned well then the result should be substantially similar each time.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack.\u003c/p\u003e\n\u003ch3 id="upmixer"\u003eUpmixer\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="upmixer_GUI.png" alt="" style="max-width: 80%"\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003eThis plug-in employs COMPASS for the task of upmixing a lower-order Ambisonic recording to a higher-order Ambisonic recording. It is intended for users that are already working with a preferred linear Ambisonic decoding workflow of higher-order Ambisonic content, and wish to combine lower-order Ambisonic material with increased spatial resolution. One can upmix first, second, or third-order material (4,9,16 channels) to up-to seventh-order material (64 channels).\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Archontis Politis.\u003c/p\u003e\n\u003ch3 id="sidechain"\u003eSideChain\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="sidechain_GUI.png" alt="" style="max-width: 60%"\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003eThis plug-in is related to \u003ca href="../../help/related-publications/mccormack2021parametric.pdf"\u003e\u003cstrong\u003ethis publication\u003c/strong\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThis plug-in applies the COMPASS analysis on one sound scene (either scene A [channels 1-16] and scene B [channels 17-32]), and uses the estimated spatial parameters to manipulate the signals of the second sound scene. If scene A and B are the same, then the plugin is functionally identical to compass_upmixer.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack.\u003c/p\u003e\n\u003ch3 id="spatedit"\u003eSpatEdit\u003c/h3\u003e\n\u003cimg src="spatEdit_A.png" alt="" style="max-width: 90%"\u003e\n\u003cimg src="spatEdit_B.png" alt="" style="max-width: 90%"\u003e\u003c/br\u003e\n\u003c/br\u003e\n\u003cp\u003eThis plug-in is related to \u003ca href="../../help/related-publications/mccormack2021parametric.pdf"\u003e\u003cstrong\u003ethis publication\u003c/strong\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe SpatEdit plug-in is intended to be used with two instances. The first instance of the plug-in allows the user to place markers on an equirectangular representation of the sphere. Alternatively the markers can automatically follow the directions of sound sources through use of the tracker. The source beamformer signals are then outputted by the first instance of the plug-in (A), where the user can then apply any conventional single-channel audio effect, re-balance their levels, or re-order the signals. These manipulated beamformer signals are then passed to the second instance of the plug-in (B), which also receives the residual signals from the first plug-in instance internally, and the COMPASS synthesis is conducted to obtain the output SH signals. Alternatively, the residual stream signals may be outputted by the first plug-in instance instead (and the beamformer signals passed internally to the second plug-in instance), which instead allows conventional linear  Ambisonics transformations to be applied to only the ambient parts of the scene.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Archontis Politis.\u003c/p\u003e\n\u003ch3 id="gravitator"\u003eGravitator\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="GravitatorGUI_alpha6_src%2076%200%20%20-24%2045%20-80%20-30.png" alt="" style="max-width: 90%"\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003eThis plug-in is related to \u003ca href="../../help/related-publications/mccormack2021parametric.pdf"\u003e\u003cstrong\u003ethis publication\u003c/strong\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eA spatial \u0026ldquo;focussor\u0026rdquo; which pulls only the directional components of the sound scene towards user defined markers, with a certain degree of \u0026ldquo;gravitational-pull\u0026rdquo;. The Ambient components of the sound scene remain unaltered.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Archontis Politis.\u003c/p\u003e\n\u003ch2 id="about-the-developers"\u003eAbout the developers\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLeo McCormack\u003c/strong\u003e: a doctoral candidate at Aalto University.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eArchontis Politis\u003c/strong\u003e post doctorate researcher at Tampere University, specialising in spatial sound recording and reproduction, acoustic scene analysis and microphone array processing.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="license"\u003eLicense\u003c/h2\u003e\n\u003cp\u003eAll of the plug-ins in the COMPASS suite may be used for academic, personal, and/or commercial use.\u003c/p\u003e\n\u003ch2 id="references"\u003eReferences\u003c/h2\u003e\n\u003cp\u003e[1] Politis, A., Tervo S., and Pulkki, V. (2018) \u003ca href="../../help/related-publications/politis2018compass.pdf"\u003e\u003cb\u003eCOMPASS: Coding and Multidirectional Parameterization of Ambisonic Sound Scenes.\u003c/b\u003e\u003c/a\u003e \u003cbr\u003e IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).\u003c/p\u003e\n\u003cp\u003e[2] Pulkki, V. (2007) \u003cb\u003eSpatial sound reproduction with directional audio coding.\u003c/b\u003e \u003cbr\u003e Journal of the Audio Engineering Society 55.6: 503-516.\u003c/p\u003e\n\u003cp\u003e[3] Pulkki, V., Politis, A., Laitinen, M.-V., Vilkamo, J., Ahonen, J. (2017). \u003cb\u003eFirst-order directional audio coding (DirAC).\u003c/b\u003e \u003cbr\u003e \u003ci\u003ein\u003c/i\u003e Parametric Time-Frequency Domain Spatial Audio, Wiley, p.89-138.\u003c/p\u003e\n\u003cp\u003e[4] Berge, S. and Barrett, N. (2010). \u003cb\u003eHigh angular resolution planewave expansion.\u003c/b\u003e \u003cbr\u003e 2nd International Symposium on Ambisonics and Spherical Acoustics.\u003c/p\u003e\n\u003cp\u003e[5] Politis, A., Vilkamo, J., and Pulkki, V. (2015). \u003ca href="../../help/related-publications/politis2015sector.pdf"\u003e \u003cb\u003eSector-based parametric sound field reproduction in the spherical harmonic domain.\u003c/b\u003e\u003c/a\u003e \u003cbr\u003e IEEE Journal of Selected Topics in Signal Processing, 9(5), 852-866.\u003c/p\u003e\n\u003cp\u003e[6] Politis, A., McCormack, L., and Pulkki, V. (2017, October). \u003ca href="../../help/related-publications/politis2017enhancement.pdf"\u003e \u003cb\u003eEnhancement of ambisonic binaural reproduction using directional audio coding with optimal adaptive mixing\u003c/b\u003e\u003c/a\u003e.\u003c/b\u003e \u003cbr\u003e In 2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA) (pp. 379-383). IEEE.\u003c/p\u003e\n\u003cp\u003e[7] Politis, A. and Pulkki, V. (2017). \u003cb\u003eHigher-Order Directional Audio Coding.\u003c/b\u003e \u003cbr\u003e\u003ci\u003ein\u003c/i\u003e Parametric Time-Frequency Domain Spatial Audio, Wiley, p.141.\u003c/p\u003e\n\u003cp\u003e[8] Wabnitz, A., Epain, N., McEwan, A., Jin, C. (2011). \u003cb\u003eUpscaling ambisonic sound scenes using compressed sensing techniques.\u003c/b\u003e \u003cbr\u003e IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA).\u003c/p\u003e\n\u003cp\u003e[9] Kolundzija, M., and Faller, C. (2018). \u003cb\u003eAdvanced B-Format Analysis.\u003c/b\u003e\u003cbr\u003e Audio Engineering Society Convention 144.\u003c/p\u003e\n\u003cp\u003e[10] Schörkhuber, C., and Höldrich, R. (2019, March). \u003cb\u003eLinearly and Quadratically Constrained Least-Squares Decoder for Signal-Dependent Binaural Rendering of Ambisonic Signals.\u003c/b\u003e \u003cbr\u003e In Audio Engineering Society Conference: 2019 AES International Conference on Immersive and Interactive Audio. Audio Engineering Society.\u003c/p\u003e\n\u003cp\u003e[11] Särkkä, S., Vehtari, A. and Lampinen, J., 2004, June. \u003cb\u003eRao-Blackwellized Monte Carlo data association for multiple target tracking. \u003c/b\u003e \u003cbr\u003e \u003ci\u003ein\u003c/i\u003e Proceedings of the seventh international conference on information fusion (Vol. 1, pp. 583-590). I.\u003c/p\u003e\n\u003cp\u003e[12] McCormack, L., Politis, A., Särkkä, S., and Pulkki, V., 2021. \u003ca href="../../help/related-publications/mccormack2021real.pdf"\u003e\u003cb\u003eReal-Time Tracking of Multiple Acoustical Sources Utilising Rao-Blackwellised Particle Filtering. \u003c/b\u003e\u003c/a\u003e \u003cbr\u003e \u003ci\u003ein\u003c/i\u003e 29th European Signal Processing Conference, EUSIPCO 2021, (pp. 206-210).\u003c/p\u003e\n\u003cp\u003e[13] McCormack, L., Politis, A., and Pulkki, V., 2021, September. \u003ca href="../../help/related-publications/mccormack2021parametric.pdf"\u003e\u003cb\u003eParametric Spatial Audio Effects Based on the Multi-Directional Decomposition of Ambisonic Sound Scenes. \u003c/b\u003e\u003c/a\u003e \u003cbr\u003e \u003ci\u003ein\u003c/i\u003e Proceedings of the 24th International Conference on Digital Audio Effects (DAFx20in21), (pp. 214-221).\u003c/p\u003e\n'}).add({id:3,href:"https://leomccormack.github.io/sparta-site/docs/plugins/hades/",title:"HADES",description:"Plug-in description for the hades-renderer.",content:'\u003ch2 id="paper-abstract"\u003ePaper abstract\u003c/h2\u003e\n\u003cp\u003eIn this article, the application of spatial covariance matching is investigated for the task of producing spatially enhanced binaural signals using head-worn microphone arrays. A two-step processing paradigm is followed, whereby an initial estimate of the binaural signals is first produced using one of three suggested binaural rendering approaches. The proposed spatial covariance matching enhancement is then applied to these estimated binaural signals, with the intention of producing refined binaural signals that more closely exhibit the correct spatial cues; as dictated by the employed sound-field model and associated spatial parameters.\nIt is demonstrated, through both objective and subjective evaluations, that the proposed enhancements in the majority of cases produce binaural signals which more closely resemble the spatial characteristics of simulated reference signals, when the enhancement is applied to, and compared against, the three suggested starting binaural rendering approaches. Furthermore, it is shown that the enhancement produces spatially similar output binaural signals when using these three different approaches; thus, indicating that the enhancement is general in nature, and could therefore be employed to enhance the outputs of other similar binaural rendering algorithms.\u003c/p\u003e\n\u003cp\u003e\u003ca href="../../help/related-publications/fernandez2022enhancing.pdf"\u003eThe full paper can be found here →\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id="plug-in-description"\u003ePlug-in description\u003c/h2\u003e\n\u003cp\u003e\u003cimg src="HADES_Renderer_GUI.png" alt="" style="max-width: 95%"/\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003eAn implementation of the general parametric binaural rendering framework described in [1], which primarily concerns head-worn microphone arrays such as: augmented reality (AR) headsets and binaural hearing aids.\u003c/p\u003e\n\u003cp\u003eThe plug-in works by first loading a dense grid of microphone array impulse response (IR) measurements via a SOFA file. Note that the measurement grid will dictate the localisation and spatialisation precision of the algorithms, for example: loading 360 IR, measured for every 1 degree on the horizontal plane, will mean that horizontal performance will be high, but elevated sources will not be localised or spatialised (at least, not particularly well); whereas, loading 1000+ IR measurements for a spherical t-design will obtain similar spatial resolution achieved by e.g. the COMPASS decoders, over the whole sphere.\u003c/p\u003e\n\u003cp\u003eOnce the microphone array IRs have been loaded, the implemented algorithms will binaurally reproduce the captured sound-field using a baseline method, such as: 1) simply taking two reference signals (one for each ear) and routing them to the binaural channels, 2) employing filter-and-sum (FaS) beamformers to better isolate the source signals based on direction-of-arrival estimates, or 3) using binaural minimum-variance distortionless response (BMVDR) beamformers instead. The binaural signals are then spatially enhanced via the spatial covariance matching solution described in [1], which relies on first specifying target inter-aural cues dictated by the assumed sound-field model and estimated spatial parameters. The spatial enhancement operation is then tasked with mixing the baseline binaural signals in an optimal and adaptive manner, such that the resultant binaural signals exhibit these target inter-aural cues; therefore, preserving the localisation cues of all sound sources in the scene, and the inter-aural coherence cues of diffuse reverberation. Note that, optionally, the default HRIRs (of a Kemar dummy head) may also be replaced by loading a SOFA file.\u003c/p\u003e\n\u003cp\u003eSince the method conducts a parameterisation of the input sound-field, the proposed method [1] can accommodate spatial audio effects and sound-field modifications through simple parameter manipulations; such as those described in [2]. In this plug-in, the direct-to-diffuse balance may be manipulated per frequency band, and the gain of directional components may be adjusted for different directions.\u003c/p\u003e\n\u003ch2 id="about-the-developers"\u003eAbout the developers\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLeo McCormack\u003c/strong\u003e: a doctoral candidate at Aalto University.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eJanani Fernandez\u003c/strong\u003e: a doctoral candidate at Aalto University.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="license"\u003eLicense\u003c/h2\u003e\n\u003cp\u003eThis plug-in may be used for academic, personal, and/or commercial use. The source code may also be used for commercial purposes, provided that the terms of the GPLv3 license are fulfilled. This requires that the original code and/or any derived works must also be open-sourced and made available under the same GPLv3 license, if it is to be used for commercial purposes.\u003c/p\u003e\n\u003ch2 id="references"\u003eReferences\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e[1] Fernandez, J., McCormack, L., Hyvärinen, P., Politis, A., and Pulkki V. 2022. \u003ca href="../../help/related-publications/fernandez2022enhancing.pdf"\u003e\u003cb\u003e\u0026ldquo;Enhancing binaural rendering of head-worn microphone arrays through the use of adaptive spatial covariance matching\u0026rdquo;, \u003c/b\u003e\u003c/a\u003e \u003cbr\u003eThe Journal of the Acoustical Society of America 151, 2624-2635 \u003ca href="https://doi.org/10.1121/10.0010109"\u003ehttps://doi.org/10.1121/10.0010109\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e[2] McCormack, L., Politis, A., and Pulkki, V., 2021, September. \u003ca href="../../help/related-publications/mccormack2021parametric.pdf"\u003e\u003cb\u003eParametric Spatial Audio Effects Based on the Multi-Directional Decomposition of Ambisonic Sound Scenes. \u003c/b\u003e\u003c/a\u003e \u003cbr\u003e \u003ci\u003ein\u003c/i\u003e Proceedings of the 24th International Conference on Digital Audio Effects (DAFx20in21), (pp. 214-221).\u003c/li\u003e\n\u003c/ul\u003e\n'}).add({id:4,href:"https://leomccormack.github.io/sparta-site/docs/plugins/hodirac-suite/",title:"HO-DirAC suite",description:"Plug-in descriptions for the HO-DirAC suite.",content:'\u003ch2 id="plug-in-descriptions"\u003ePlug-in descriptions\u003c/h2\u003e\n\u003cp\u003eDirectional Audio Coding (DirAC) has been an active research topic at Aalto University since 2007 [1]. The method operates by estimating spatial parameters to describe the input sound scene over time and frequency, which are then used to conduct the mapping of the input Ambisonic signals to the output loudspeaker/binaural channels in an adaptive and informed manner. While the original intention of DirAC was for low bitrate compression and transmission of Ambisonic recordings, more recent years have instead focussed on using it for the enhancement of Ambisonics reproduction.\u003c/p\u003e\n\u003cp\u003eThese particular audio plug-in implementations use the higher-order analysis designs described in [2,3], which synthesise the target output signals based on the optimal covariance domain rendering framework described in [4].\u003c/p\u003e\n\u003cp\u003eAll plug-ins conform to the Ambisonic Channel Number (ACN) ordering convention and offer support for both orthonormalised (N3D) and semi-normalised (SN3D) scalings (note: AmbiX uses ACN/SN3D). The maximum Ambisonic order for these plug-ins is 3.\u003c/p\u003e\n\u003ch3 id="decoder"\u003eDecoder\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="HODirAC_decoder_GUI.png" alt="" style="max-width: 85%"\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003eThis plug-in is based on the design detailed in \u003ca href="../../help/related-publications/politis2015sector.pdf"\u003e\u003cstrong\u003ethis publication\u003c/strong\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAn Ambisonic loudspeaker decoder based on the Higher-order Directional Audio Coding (HO-DirAC) method.\u003c/p\u003e\n\u003cp\u003eThe plugin first generates intermediate (prototype) loudspeaker signals to serve as a good \u0026ldquo;starting guess\u0026rdquo;. In this case the Energy-Preserving Ambisonic decoder (EPAD) is used (as found in sparta_ambiDEC). The signal statistics (covariance matrices per frequency band) of these prototype loudspeaker signals are then computed, followed by defining new target covariance matrices that are formed based on the analysed spatial parameters. The problem of applying mixing matrices to the prototype signals such that their narrow-band covariance matrices are brought closer to the target covariance matrices, is then solved by using the covariance-domain framework for spatial audio processing (CDF4SAP); also referred to as \u0026ldquo;optimal-mixing\u0026rdquo;. The approach aims to synthesise signals that exhibit the target covariance matrices first via a linear combination of them as much as possible, followed by \u0026ldquo;filling in the gaps\u0026rdquo; with a decorrelated version of the prototype signals; as described by a residual mixing matrix. Such processing aims to improve signal fidelity and also mitigate artefacts arrising due to signal decorrelation. Depending on the scene, and as the number of input channels exceed the number of output channels, the decorrelated signal energy can become negligible. This can be demonstated by using the \u0026ldquo;Enable Residual Stream\u0026rdquo; toggle button, when rendering such scenes.\u003c/p\u003e\n\u003cp\u003eThe \u0026ldquo;Diffuse-to-Direct\u0026rdquo; control allows the user to give more prominence to the direct sound components (an effect similar to de-reverberation), or to the ambient component (an effect similar to emphasising reverberation in the recording), while the \u0026ldquo;Linear-to-Parametric\u0026rdquo; control lets you control the balance between HO-DirAC decoding and EPAD decoding. When using Ambisonic signals derived from microphone arrays, the frequency ranges at which higher-order components can be obtained varies depending on the order; in these cases, the \u0026ldquo;Analysis-order\u0026rdquo; may be specified per frequency band to account for these inperfect input Ambisonic signals.\u003c/p\u003e\n\u003cp\u003eThe plug-in is considered by the authors a production tool and, due to its time-frequency processing, requires internal audio buffer sizes of at least 2048 samples. Hence we do not consider it as a low-latency plug-in and therefore it is not suitable for interactive input. For cases such as interactive binaural rendering for VR with head-tracking, refer to the \u003cb\u003eHO-DirAC|Binaural\u003c/b\u003e version.\u003c/p\u003e\n\u003ch3 id="binaural"\u003eBinaural\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="HODirAC_binaural_GUI.png" alt="" style="max-width: 85%"\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003eThis plug-in is based on the design detailed \u003ca href="../../help/related-publications/politis2017enhancement.pdf"\u003e\u003cstrong\u003ethis publication\u003c/strong\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThis is an optimised version of the HO-DirAC decoder for binaural playback, which bypasses loudspeaker rendering by using binaural filters (HRTFs) directly, which can be user-provided and personalised with the SOFA format. For the plugin parameters, see the description of the \u003cb\u003eHO-DirAC|Decoder\u003c/b\u003e above. Additionally the plugin can receive OSC rotation angles from a headtracker at a user specified port, in the yaw-pitch-roll convention.\u003c/p\u003e\n\u003cp\u003eThis version is intended mostly for head-tracked binaural playback of Ambisonic content at interactive update rates, usually in conjunction with a head-mounted display (HMD). The plugin requires an audio buffer size of at least 512 samples (~10msec at 48kHz). The averaging parameters can be used to make the parametric analysis and synthesis more or less responsive, providing the user with a means to adjust them optimally for a particular sound scene.\u003c/p\u003e\n\u003ch3 id="upmixer"\u003eUpmixer\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="HODirAC_upmixer_GUI.png" alt="" style="max-width: 85%"\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003eThis VST is intended for the task of upmixing a lower-order Ambisonic recording to a higher-order Ambisonic recording. It is essentially a wrapper for HO-DirAC|Decoder, as it first decodes the input to an appropriate uniform arrangement of virtual loudspeakers (e.g. a t-design) followed by re-encoding the virtual loudspeaker signals into the target order. It can be used by users who are already working with a preferred linear Ambisonic decoding workflow of higher-order Ambisonic content, and wish to combine lower-order Ambisonic material with increased spatial resolution. One can upmix first, second, or third-order material (4,9,16 channels) to up-to seventh-order material (64 channels).\u003c/p\u003e\n\u003ch2 id="about-the-developers"\u003eAbout the developers\u003c/h2\u003e\n\u003cp\u003eThese plug-ins were developed by Leo McCormack. However, a number of people were involved in the research of the employed algorithms, and much of the internal C/C++ code is also based on Matlab code written by Archontis Politis and Juha Vilkamo; who, in turn, based their code and research on the code and research of: Mikko-Ville Laitinen, Jukka Ahonen, Tapani Pihlajamäki, and Ville Pulkki. For a detailed history, and for descriptions of both legacy and modern DirAC processing, the reader is referred to [5].\u003c/p\u003e\n\u003ch2 id="license"\u003eLicense\u003c/h2\u003e\n\u003cp\u003eThese VST plug-ins, which incorporate Aalto University\u0026rsquo;s implementation of the DirAC technology, are provided for academic, personal and/or non-commercial use by a non-enterprise end-user. Any exploitation of the plug-ins for other purposes may require a license from Fraunhofer IIS. For commercial implementations of DirAC technology, the user is instead referred to \u003ca href="https://www.iis.fraunhofer.de/en/ff/amm/prod/upHear.html"\u003e\u003cb\u003eupHear\u003c/b\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id="references"\u003eReferences\u003c/h2\u003e\n\u003cp\u003e\u003ca id="dirac_2007"\u003e\u003c/a\u003e[1] Pulkki, V. (2007) \u003cb\u003eSpatial sound reproduction with directional audio coding.\u003c/b\u003e \u003cbr\u003e Journal of the Audio Engineering Society 55.6: 503-516.\u003c/p\u003e\n\u003cp\u003e\u003ca id="hodirac_2015"\u003e\u003c/a\u003e[2] Politis, A., Vilkamo, J., and Pulkki, V. (2015). \u003cb\u003e\u003ca href="../../help/related-publications/politis2015sector.pdf"\u003e\u003cb\u003eSector-based parametric sound field reproduction in the spherical harmonic domain.\u003c/b\u003e\u003c/a\u003e\u003c/b\u003e \u003cbr\u003e IEEE Journal of Selected Topics in Signal Processing, 9(5), 852-866.\u003c/p\u003e\n\u003cp\u003e\u003ca id="hodirac_2017"\u003e\u003c/a\u003e[3] Politis, A., McCormack, L., and Pulkki, V. (2017, October). \u003cb\u003e\u003ca href="../../help/related-publications/politis2017enhancement.pdf"\u003e\u003cb\u003e Enhancement of ambisonic binaural reproduction using directional audio coding with optimal adaptive mixing\u003c/b\u003e\u003c/a\u003e.\u003c/b\u003e \u003cbr\u003e In 2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA) (pp. 379-383). IEEE.\u003c/p\u003e\n\u003cp\u003e[4] Vilkamo, J., Bäckström, T., and Kuntz, A. (2013). \u003cb\u003eOptimized covariance domain framework for time-frequency processing of spatial audio. \u003c/b\u003e \u003cbr\u003e Journal of the Audio Engineering Society, 61(6), 403-411.\u003c/p\u003e\n\u003cp\u003e[5] Pulkki, V., Delikaris-Manias, S., and Politis, A. (Eds.). (2018). \u003cb\u003eParametric time-frequency domain spatial audio. \u003c/b\u003e \u003cbr\u003eJohn Wiley and Sons, Incorporated.\u003c/p\u003e\n'}).add({id:5,href:"https://leomccormack.github.io/sparta-site/docs/plugins/hosirr/",title:"HO-SIRR",description:"Description for the HO-SIRR application.",content:'\u003ch2 id="application-description"\u003eApplication description\u003c/h2\u003e\n\u003cp\u003eHigher-order Spatial Impulse Response Rendering (HO-SIRR) is a rendering method, which can synthesise output loudspeaker array room impulse responses (RIRs) using input spherical harmonic (Ambisonic/B-Format) RIRs of arbitrary order [\u003ca href="../../help/related-publications/mccormack2020higher.pdf"\u003e\u003cb\u003e1\u003c/b\u003e\u003c/a\u003e,\u003ca href="../../help/related-publications/mccormack2019higher.pdf"\u003e\u003cb\u003e2\u003c/b\u003e\u003c/a\u003e]. The method makes assumptions regarding the composition of the sound-field and extracts spatial parameters over time, which allows it to map the input to the output in an adaptive and more informed manner; when compared to purely linear methods such as Ambisonics.\u003c/p\u003e\n\u003cp\u003eThe idea is that you then convolve a monophonic source with this loudspeaker array RIR, and it will be reproduced and exhibit the spatial characteristics of the captured space. Note that the HO-SIRR algorithm is an extention of the original first-order SIRR formulation, first proposed back in 2005 [3,4], by employing the higher-order analysis principles described in [5], which permits higher spatial accuracy during the mapping provided that higher-order components are available.\u003c/p\u003e\n\u003cp\u003eNote that this HO-SIRR application is essentially a direct port of the HO-SIRR MATLAB toolbox, which is slighly more configurable than the C/C++ implementation (and easier to augment) and can be found \u003ca href="https://github.com/leomccormack/HO-SIRR"\u003e\u003cb\u003ehere\u003c/b\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src="HOSIRR_GUI.png" alt="" style="max-width: 85%"\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003eThe suggested workflow is:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMeasure a room impulse response (RIR) of a space with a spherical microphone array (e.g. using \u003ca href="http://eprints.hud.ac.uk/id/eprint/24579/?fbclid=IwAR1EorukJ1MaLojT4Eof8eTSOc9-A479kVhqbF7fHaSycOO7llAD7aIj3TA"\u003e\u003cb\u003eHAART\u003c/b\u003e\u003c/a\u003e), and convert it into an Ambisonic/B-format RIR (e.g. using \u003ca href="../sparta-suite/#array2sh"\u003e\u003cstrong\u003esparta_array2sh\u003c/strong\u003e\u003c/a\u003e).\u003c/li\u003e\n\u003cli\u003eLoad this B-Format/Ambisonic RIR into the HOSIRR App/plug-in and specify your loudspeaker array directions and desired rendering configuration (although, the default should suffice for most purposes).\u003c/li\u003e\n\u003cli\u003eClick \u0026ldquo;Render\u0026rdquo;, and then \u0026ldquo;Save\u0026rdquo;, to export the resulting loudspeaker array RIR as a multi-channel .wav file.\u003c/li\u003e\n\u003cli\u003eThen simply convolve this loudspeaker array RIR with a monophonic source signal, and it will be reproduced over the loudspeaker array (also exhibiting the spatial characteristics of the captured space). Plug-ins such as \u003ca href="http://pcfarina.eng.unipr.it/X-volver.htm"\u003e\u003cb\u003eXvolver\u003c/b\u003e\u003c/a\u003e, \u003ca href="http://www.angelofarina.it/X-MCFX.htm?fbclid=IwAR0AcjaXa_szrrJ4nFHn1lMk_6dmTd8WmcQwokyEb7eD0ULgHyHpoFtTFmk"\u003e\u003cb\u003eX-MCFX\u003c/b\u003e\u003c/a\u003e, \u003ca href="../sparta-suite/#matrixconv"\u003e\u003cstrong\u003esparta_matrixconv\u003c/strong\u003e\u003c/a\u003e (included in the installer), and \u003ca href="http://www.matthiaskronlachner.com/?p=1910"\u003e\u003cb\u003emcfx_convolver\u003c/b\u003e\u003c/a\u003e, are well suited to this convolution task.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="listening-test-results-at-a-glance"\u003eListening test results at a glance\u003c/h2\u003e\n\u003cp\u003eThe perceptual performance of HO-SIRR was evaluated based on formal listening tests in [1], where it was compared to Mode-Matching Ambisonics decoding. It was found that if the mono signal is quite stationary (such as a trombone recording), then first-order SIRR renderings can sound almost equivalent to 5th order Ambisonics. However, if the mono signal is more transient (such as a kick drum or speech sample), then the benefits of the higher-order SIRR renderings are revealed. For an in-depth description of the listening test and a discussion of the results, see: \u003ca href="../../help/related-publications/mccormack2020higher.pdf"\u003e[\u003cb\u003e1\u003c/b\u003e]\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src="JAES_orderTest_post_review.png" alt="" style="max-width: 100%"\u003e \u003c/br\u003e\u003c/p\u003e\n\u003ch2 id="about-the-authors"\u003eAbout the authors\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLeo McCormack\u003c/strong\u003e: a doctoral candidate at Aalto University.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eArchontis Politis\u003c/strong\u003e: post doctorate researcher at Tampere University, specialising in spatial sound recording and reproduction, acoustic scene analysis and microphone array processing.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVille Pulkki\u003c/strong\u003e: Professor at Aalto University, known for VBAP, SIRR, DirAC and eccentric behaviour.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="license"\u003eLicense\u003c/h2\u003e\n\u003cp\u003eThis application may be used for academic, personal, and/or commercial use. The source code may also be used for commercial purposes, provided that the terms of the GPLv3 license are fulfilled. This requires that the original code and/or any derived works must also be open-sourced and made available under the same GPLv3 license, if it is to be used for commercial purposes.\u003c/p\u003e\n\u003ch2 id="references"\u003eReferences\u003c/h2\u003e\n\u003cp\u003e[1] McCormack, L., Pulkki, V., Politis, A., Scheuregger, O. and Marschall, M. (2020). \u003cb\u003e \u003ca href="../../help/related-publications/mccormack2020higher.pdf"\u003e\u003cb\u003eHigher-Order Spatial Impulse Response Rendering: Investigating the Perceived Effects of Spherical Order, Dedicated Diffuse Rendering, and Frequency Resolution\u003c/b\u003e\u003c/a\u003e. \u003c/b\u003e \u003cbr\u003eJournal of the Audio Engineering Society, 68(5), pp.338-354.\u003c/p\u003e\n\u003cp\u003e[2] McCormack, L., Politis, A., Scheuregger, O., and Pulkki, V. (2019). \u003cb\u003e \u003ca href="../../help/related-publications/mccormack2019higher.pdf"\u003e\u003cb\u003eHigher-order processing of spatial impulse responses.\u003c/b\u003e\u003c/a\u003e\u003c/b\u003e \u003cbr\u003e Proceedings of the 23rd International Congress on Acoustics, 9\u0026ndash;13 September 2019 in Aachen, Germany.\u003c/p\u003e\n\u003cp\u003e[3] Merimaa, J. and Pulkki, V. (2005). \u003cb\u003eSpatial impulse response rendering I: Analysis and synthesis\u003c/b\u003e \u003cbr\u003e Journal of the Audio Engineering Society, 53(12), pp.1115-1127.\u003c/p\u003e\n\u003cp\u003e[4] Pulkki, V. and Merimaa, J. (2006). \u003cb\u003eSpatial impulse response rendering II: Reproduction of diffuse sound and listening tests\u003c/b\u003e \u003cbr\u003e Journal of the Audio Engineering Society, 54(1/2), pp.3-20.\u003c/p\u003e\n\u003cp\u003e[5] Politis, A. and Pulkki, V. (2016). \u003cb\u003eAcoustic intensity, energy-density and diffuseness estimation in a directionally-constrained region\u003c/b\u003e \u003cbr\u003e arXiv preprint arXiv:1609.03409.\u003c/p\u003e\n'}).add({id:6,href:"https://leomccormack.github.io/sparta-site/docs/plugins/cropac-binaural/",title:"CroPaC-Binaural",description:"Plug-in description for the CroPaC binaural decoder.",content:'\u003ch2 id="paper-abstract"\u003ePaper abstract\u003c/h2\u003e\n\u003cp\u003eBinaural ambisonics decoding is a means of reproducing a captured or synthesised sound-field, as described by a spherical harmonic representation, over headphones. The majority of ambisonic decoders proposed to date are based on a signal-independent approach; operating via a linear mapping between the input spherical harmonic signals and the output binaural signals. While this approach is computationally efficient, an impractically high input order is often required to deliver a sufficiently accurate rendition of the original spatial cues to the listener. This is especially problematic, as the vast majority of commercially available Ambisonics microphones are first-order, which ultimately results in numerous perceptual deficiencies during reproduction. Therefore, in this paper, a signal-dependent and parametric binaural ambisonic decoder is proposed, which is specifically intended to reproduce first-order input with high perceptual accuracy. The proposed method assumes a sound-field model of one source and one non-isotropic ambient component per narrow-band. It then employs the Cross-Pattern Coherence (CroPaC) post-filter, in order to segregate these components with improved spatial selectivity. Listening test results indicate that the proposed method, when using first-order input, performs similarly to third-order Ambisonics reproduction.\u003c/p\u003e\n\u003cp\u003e\u003ca href="../../help/related-publications/mccormack2019parametric.pdf"\u003eThe full paper can be found here →\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id="overview-of-the-method"\u003eOverview of the method\u003c/h2\u003e\n\u003cp\u003eThe proposed first-order decoder employs a sound-field model comprising one source component and one non-isotropic ambient component per time-frequency tile. The method first estimates the source DoA via steered-response power (SRP) beamforming and subsequent peak-finding. The source stream is then segregated by steering a beamformer toward the estimated DoA, and employing an additional CroPaC [1] post-filtering operation to improve its spatial selectivity. The ambient stream is then simply the residual, once the source component has been subtracted from the input sound-field. The two streams are then binauralised and fed into an optimal mixing unit [2], along with the ambisonic prototype covariance matrix, in order to generate the binaural output. A block diagram of the proposed method is depicted below:\u003c/p\u003e\n\u003cimg src="cropac_binaural.png" alt="" style="max-width: 90%"\u003e\n\u003cp\u003eThe proposed approach is inspired by the COMPASS method [3]. However, along with the CroPaC post-filter, it also employs instantaneous source direction estimation and synthesises the output in a linear manner as much as possible; in order to improve the fidelity of the output signals.\u003c/p\u003e\n\u003ch2 id="plug-in-description"\u003ePlug-in description\u003c/h2\u003e\n\u003cp\u003e\u003cimg src="CroPaC_Binaural_GUI.png" alt="" style="max-width: 85%"\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003eThis implementation employs the magnitude least-squares (magLS) ambisonic decoder [4] as the prototype, contrary to the spatial-resampling decoder employed for the listening tests described in the paper. The plug-in allows the user to import their own HRTFs via SOFA files, and also supports head-tracking via OSC messages. The user may also influence the direct/diffuse balance per frequency band; note that the streams are balanced when set in the middle (default).\u003c/p\u003e\n\u003ch2 id="listening-test-results-at-a-glance"\u003eListening test results at a glance\u003c/h2\u003e\n\u003cp\u003eFormal listening tests indicate that the proposed first-order decoder (CroPaC1) performs similarly to (or exceeds) third-order spatial re-sampling ambisonics decoding (Ambi3), in terms of the perceived spatial and timbral attributes of the reproduction; as shown in the plots below:\u003c/p\u003e\n\u003cimg src="Spatial_results.png" alt="" style="max-width: 90%"\u003e   \n\u003cp\u003e\u003cimg src="Results_Timbral.png" alt="" style="max-width: 90%"\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003eIt should be highlighted that third- order ambisonics employs four times the number of input channels than that of the proposed method. This, therefore, represents a significant reduction in bandwidth, without compromising the perceived spatial accuracy or fidelity.\u003c/p\u003e\n\u003ch2 id="developers"\u003eDevelopers\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLeo McCormack\u003c/strong\u003e: a doctoral candidate at Aalto University.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSymeon Delikaris-Manias\u003c/strong\u003e: post doctorate researcher at Aalto University, specialising in compact microphone array processing for DoA estimation and sound-field reproduction. His doctoral research included work on the Cross-Pattern Coherence (CroPaC) algorithm, which is a spatial post-filter optimised for high noise/reverberant environments.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="license"\u003eLicense\u003c/h2\u003e\n\u003cp\u003eThis plug-in may be used for academic, personal, and/or commercial use. The source code may also be used for commercial purposes, provided that the terms of the GPLv3 license are fulfilled. This requires that the original code and/or any derived works must also be open-sourced and made available under the same GPLv3 license, if it is to be used for commercial purposes.\u003c/p\u003e\n\u003ch2 id="references"\u003eReferences\u003c/h2\u003e\n\u003cp\u003e[1] Delikaris-Manias, S. and Pulkki, V. (2013). \u003cb\u003eCross Pattern Coherence Algorithm for Spatial Filtering Applications Utilizing Microphone Arrays.\u003c/b\u003e \u003cbr\u003e Audio, Speech, and Language Processing, IEEE Transactions on , vol.21, no.11, pp.2356-2367.\u003c/p\u003e\n\u003cp\u003e[2] Vilkamo, J., Bäckström, T., and Kuntz, A. (2013). \u003cb\u003eOptimized covariance domain framework for time\u0026ndash;frequency processing of spatial audio. \u003c/b\u003e \u003cbr\u003eJournal of the Audio Engineering Society, 61(6), 403-411.\u003c/p\u003e\n\u003cp\u003e[3] Politis, A., Tervo S., and Pulkki, V. (2018) \u003ca href="../../help/related-publications/politis2018compass.pdf"\u003e\u003cb\u003eCOMPASS: Coding and Multidirectional Parameterization of Ambisonic Sound Scenes.\u003c/b\u003e\u003c/a\u003e \u003cbr\u003e IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).\u003c/p\u003e\n\u003cp\u003e[4] Schörkhuber, C., Zaunschirm, M., and Höldrich, R. (2018). \u003cb\u003eBinaural Rendering of Ambisonic Signals via Magnitude Least Squares.\u003c/b\u003e \u003cbr\u003e In Proceedings of the DAGA (Vol. 44).\u003c/p\u003e\n'}).add({id:7,href:"https://leomccormack.github.io/sparta-site/docs/plugins/ultrasonicsuperhearing/",title:"UltrasonicSuperHearing",description:"Plug-in description for the UltrasonicSuperHearing.",content:'\u003ch2 id="plug-in-description"\u003ePlug-in description\u003c/h2\u003e\n\u003cp\u003e\u003cimg src="UltrasonicSuperHearing_PluginGUI.png" alt="" style="max-width: 65%"/\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003eUltrasonic sound sources are inaudible to humans. While there do exist devices that can capture, pitch shift and reproduce ultrasonic sound sources for humans to listen to, they generally only employ a single microphone and loudspeaker. Therefore, the listener is unable to localise where these pitch shifted ultrasonic sound sources are located.\u003c/p\u003e\n\u003cp\u003eHowever, by instead capturing the ultrasonic sound sources using an array of microphones, it is possible to estimate their direction of arrival (DoA), and use this information to spatialise a pitch shifted signal in this direction. Therefore, not only is the listener able to perceive these ultrasonic sound sources, but they are also able to localise them in the correct direction.\u003c/p\u003e\n\u003cp\u003eThis plug-in requires 192kHz sampling rate and relies on constructing the ultrasonic microphone array described below. Although, it is noted that the source code could be altered to support other arrays without too much difficulty.\u003c/p\u003e\n\u003ch2 id="building-a-suitable-ultrasonic-microphone-array"\u003eBuilding a suitable ultrasonic microphone array\u003c/h2\u003e\n\u003cp\u003eThe CAD files and drawings used for 3D printing the 6 sensor proof-of-concept array, which was employed for the study detailed in [1], can be found in the \u003ca href="https://github.com/leomccormack/Super-Hearing"\u003e\u003cstrong\u003ecompanion git repository\u003c/strong\u003e\u003c/a\u003e. Further details regarding its construction can also be found in the paper.\u003c/p\u003e\n\u003cp\u003e\u003cimg src="UltrasonicArray.png" alt="UltrasonicArray" width="450" style="max-width: 70%"/\u003e\u003c/br\u003e\u003c/p\u003e\n\u003ch2 id="auralising-the-captured-ultrasonic-sound-sources"\u003eAuralising the captured ultrasonic sound sources\u003c/h2\u003e\n\u003cp\u003eThe audio plugin renders the 6 microphone signals to the 2 binaural channels. It employs the DoA estimation, pitch-shifting, and binauralisation, as described and used for the listening tests conducted in [1].\u003c/p\u003e\n\u003cp\u003e\u003cimg src="UltrasonicArray_ProcessingDiagram.png" alt="" style="max-width: 85%"/\u003e\u003c/br\u003e\u003c/p\u003e\n\u003ch2 id="demo"\u003eDemo\u003c/h2\u003e\n\u003cp\u003eThe proposed ultrasonic super-hearing system is demonstrated in the following video:\u003c/p\u003e\n\u003ciframe width="560" height="315" style="max-width: 100%" src="https://www.youtube.com/embed/HMkZs7a1nQc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen\u003e\u003c/iframe\u003e\n\u003cp\u003e(Please wear headphones)\u003c/p\u003e\n\u003ch2 id="about-the-authors"\u003eAbout the authors\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLeo McCormack\u003c/strong\u003e: a doctoral candidate at Aalto University.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVille Pulkki\u003c/strong\u003e: Professor at Aalto University, known for VBAP, SIRR, DirAC and eccentric behaviour.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRaimundo Gonzalez\u003c/strong\u003e: a doctoral candidate at Aalto University.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="license"\u003eLicense\u003c/h2\u003e\n\u003cp\u003eThis application may be used for academic, personal, and/or commercial use. The source code may also be used for commercial purposes, provided that the terms of the GPLv3 license are fulfilled. This requires that the original code and/or any derived works must also be open-sourced and made available under the same GPLv3 license, if it is to be used for commercial purposes.\u003c/p\u003e\n\u003ch2 id="references"\u003eReferences\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e[1] Pulkki, V., McCormack, L. \u0026amp; Gonzalez, R. 2021. \u003ca href="https://www.nature.com/articles/s41598-021-90829-9"\u003e\u003cstrong\u003eSuperhuman spatial hearing technology for ultrasonic frequencies\u003c/strong\u003e\u003c/a\u003e. Scientific Reports 11, 11608 (2021). https://doi.org/10.1038/s41598-021-90829-9\u003c/li\u003e\n\u003c/ul\u003e\n'}).add({id:8,href:"https://leomccormack.github.io/sparta-site/docs/sourcecode/overview/",title:"Where to find?",description:"Overview of the plug-in source code.",content:'\u003cp\u003eThe collection of plug-ins included in the SPARTA installer are the culmination of numerous software projects.\u003c/p\u003e\n\u003ch2 id="frameworks-and-sdks"\u003eFrameworks and SDKs\u003c/h2\u003e\n\u003cp\u003eAll plug-ins included in the SPARTA suite employ the open-source \u003cstrong\u003eJUCE framework\u003c/strong\u003e for their graphical user-interface (GUI) and for wrapping arround the \u003cstrong\u003eVST2 SDK\u003c/strong\u003e. The open-source \u003cstrong\u003eSpatial_Audio_Framework\u003c/strong\u003e is then used for developing the internal spatial audio signal processing.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSource code for the \u003ca href="https://github.com/leomccormack/Spatial_Audio_Framework"\u003eSpatial_Audio_Framework →\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eSource code for the \u003ca href="https://github.com/juce-framework/JUCE"\u003eJUCE framework →\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eDownload link for the \u003ca href="https://web.archive.org/web/20181016150224/https://download.steinberg.net/sdk_downloads/vstsdk3610_11_06_2018_build_37.zip"\u003eVST2 SDK →\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBoth frameworks are also accompanied by extensive online documentation:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDocumentation for the \u003ca href="https://leomccormack.github.io/Spatial_Audio_Framework"\u003eSpatial_Audio_Framework →\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eDocumentation for the \u003ca href="https://docs.juce.com/master/modules.html"\u003eJUCE framework →\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="sparta-suite-source-code"\u003eSPARTA suite source code\u003c/h2\u003e\n\u003cp\u003eThe source code for the SPARTA plug-ins is made available under the \u003ca href="https://choosealicense.com/licenses/gpl-3.0/"\u003eGNU GPLv3\u003c/a\u003e license. However, it should be noted that the repository contains only code for the plug-in GUIs, with the internal signal processing inherited directly from the \u0026ldquo;examples\u0026rdquo; (\u003ca href="https://choosealicense.com/licenses/isc/"\u003eISC license\u003c/a\u003e) found in the Spatial_Audio_Framework.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSource code for the \u003ca href="https://github.com/leomccormack/SPARTA"\u003eSPARTA suite →\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eDocumentation for the \u003ca href="https://leomccormack.github.io/Spatial_Audio_Framework/examples.html"\u003eSpatial_Audio_Framework examples →\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="hades-source-code"\u003eHADES source code\u003c/h2\u003e\n\u003cp\u003eThe source code for the HADES Renderer is made available under the \u003ca href="https://choosealicense.com/licenses/gpl-3.0/"\u003eGNU GPLv3\u003c/a\u003e license.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSource code for the \u003ca href="https://github.com/jananifernandez/HADES"\u003eHADES Renderer →\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="ho-sirr-source-code"\u003eHO-SIRR source code\u003c/h2\u003e\n\u003cp\u003eThe source code for the HO-SIRR application is made available under the \u003ca href="https://choosealicense.com/licenses/gpl-3.0/"\u003eGNU GPLv3\u003c/a\u003e license. It is essentially a direct C port of the MATLAB toolbox code with JUCE GUI.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSource code for the \u003ca href="https://github.com/leomccormack/HO-SIRR-GUI"\u003eHO-SIRR application →\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eSource code for the \u003ca href="https://github.com/leomccormack/HO-SIRR"\u003eHO-SIRR MATLAB toolbox →\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="cropac-binaural-source-code"\u003eCroPaC-Binaural source code\u003c/h2\u003e\n\u003cp\u003eThe source code for the CroPaC-Binaural decoder is made available under the \u003ca href="https://choosealicense.com/licenses/gpl-3.0/"\u003eGNU GPLv3\u003c/a\u003e license.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSource code for the \u003ca href="https://github.com/leomccormack/CroPaC-Binaural"\u003eCroPaC-Binaural decoder →\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="ultrasonicsuperhearing-source-code"\u003eUltrasonicSuperHearing source code\u003c/h2\u003e\n\u003cp\u003eThe source code for the UltrasonicSuperHearing plugin is made available under the \u003ca href="https://choosealicense.com/licenses/isc"\u003eISC\u003c/a\u003e and \u003ca href="https://choosealicense.com/licenses/gpl-3.0/"\u003eGNU GPLv3\u003c/a\u003e licenses (source-file dependent).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSource code for the \u003ca href="https://github.com/leomccormack/Super-Hearing"\u003eSuper-Hearing →\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="other-related-source-code"\u003eOther related source code\u003c/h2\u003e\n\u003cp\u003eThe following repositories were created by the same developers of SPARTA:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSource code for the reference MATLAB implementation \u003ca href="https://github.com/polarch/COMPASS-ref"\u003eCOMPASS-ref →\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eSource code for the \u003ca href="https://github.com/chris-hld/spaudiopy"\u003eSpatial Audio Python Package →\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eSource code for the MATLAB library \u003ca href="https://github.com/polarch/Spherical-Harmonic-Transform"\u003eSpherical-Harmonic-Transform →\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eSource code for the MATLAB library \u003ca href="https://github.com/polarch/Spherical-Array-Processing"\u003eSpherical-Array-Processing →\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eSource code for the MATLAB library \u003ca href="https://github.com/polarch/Higher-Order-Ambisonics"\u003eHigher-Order-Ambisonics →\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eSource code for the MATLAB library \u003ca href="https://github.com/polarch/Vector-Base-Amplitude-Panning"\u003eVector-Base-Amplitude-Panning →\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eSource code for the MATLAB library \u003ca href="https://github.com/polarch/Array-Response-Simulator"\u003eArray-Response-Simulator →\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eSource code for the MATLAB library \u003ca href="https://github.com/polarch/shoebox-roomsim"\u003eshoebox-roomsim →\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eSource code for JavaScript/Web Audio Ambisonics playback \u003ca href="https://github.com/polarch/JSAmbisonics"\u003eJSAmbisonics →\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n'}).add({id:9,href:"https://leomccormack.github.io/sparta-site/docs/help/install-instructions/",title:"Install instructions",description:"Instructions on how to install the included VST plug-ins.",content:'\u003ch2 id="macos-x86-users"\u003eMacOS (x86) users\u003c/h2\u003e\n\u003cp\u003eMacOS users can simply unzip and run the installer.\u003c/p\u003e\n\u003cp\u003eFor MacOS versions 10.15 and above, you may need to give explicit permission for the installer to run via Settings-\u0026gt;Security-\u0026gt;\u0026ldquo;Open anyway\u0026rdquo;. In rare cases, you may need to use the following command in the terminal for each plug-in:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo xattr -d com.apple.quarantine /Library/Audio/Plug-Ins/VST/AmbiBIN.vst\n...\nsudo xattr -d com.apple.quarantine /Library/Audio/Plug-Ins/VST/Spreader.vst\n... etc.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe installer places the VSTs in the following folder:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e/Library/Audio/Plug-Ins/VST/\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd also installs the following files:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e/usr/local/lib/libsaf_mkl_custom_lp64.dylib\n/usr/local/lib/libsaf_ipp_custom.dylib\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo uninstall, simply delete these files.\u003c/p\u003e\n\u003ch2 id="windows-x86-users"\u003eWindows (x86) users\u003c/h2\u003e\n\u003cp\u003eWindows users can simply unzip and run the installer.\u003c/p\u003e\n\u003cp\u003eThe installer places the VSTs in the following folder:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eC:/Program Files/VSTPlugins\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd also installs the following files:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e/Windows/System32/saf_mkl_custom_lp64.dll\n/Windows/System32/saf_ipp_custom.dll\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo uninstall, simply delete these files.\u003c/p\u003e\n\u003ch2 id="linux-x86-users"\u003eLinux (x86) users\u003c/h2\u003e\n\u003cp\u003eLinux users can download, unzip and copy the plugins into any folder that is scanned by the VST host for VST plug-ins. For example:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e~/.vst\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe following included files must then also be copied into:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e/usr/local/lib/libsaf_mkl_custom_lp64.so\n/usr/local/lib/libsaf_ipp_custom.so\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo uninstall, simply delete these files.\u003c/p\u003e\n\u003ch2 id="raspberry-pi-and-arm-users"\u003eRaspberry Pi and ARM users\u003c/h2\u003e\n\u003cdiv class="alert alert-warning d-flex" role="alert"\u003e\n  \u003cdiv class="flex-shrink-1 alert-icon"\u003e👉 \u003c/div\u003e\n  \n    \u003cdiv class="w-100"\u003ePlease note that only the Raspberry Pi 3B and Raspberry Pi 4 have been tested by the developers \u003c/div\u003e\n  \n\u003c/div\u003e\n\n\u003cp\u003eThere are currently no pre-built binaries for Raspberry Pi or ARM-based architectures, however, there are build instructions, which can be found \u003ca href="https://github.com/leomccormack/SPARTA/blob/master/docs/RaspberryPi_instructions.md"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n'}).add({id:10,href:"https://leomccormack.github.io/sparta-site/docs/help/download-archive/",title:"Download archive",description:"Where to find previous releases of the SPARTA plug-ins.",content:'\u003cul\u003e\n\u003cli\u003eOld versions of the SPARTA plug-ins are still hosted on the original server, which \u003ca href="http://research.spa.aalto.fi/projects/sparta_vsts/download/"\u003ecan be found here\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eNewer versions are instead \u003ca href="https://github.com/leomccormack/SPARTA/releases"\u003ehosted on GitHub\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src="https://img.shields.io/github/v/release/leomccormack/SPARTA" alt="GitHub release"\u003e\n\u003cimg src="https://img.shields.io/github/release-date/leomccormack/SPARTA" alt="GitHub release date"\u003e\n\u003cimg src="https://img.shields.io/github/downloads/leomccormack/SPARTA/latest/total" alt="Downloads Latest"\u003e \u003cimg src="https://img.shields.io/github/downloads/leomccormack/SPARTA/total" alt="Downloads Total"\u003e\u003c/p\u003e\n'}).add({id:11,href:"https://leomccormack.github.io/sparta-site/docs/help/related-publications/",title:"Related publications",description:"A list of publications related to the included plug-ins.",content:'\u003ch2 id="2022"\u003e2022\u003c/h2\u003e\n\u003ch3 id="mccormack2022parametric_b"\u003emccormack2022parametric_b\u003c/h3\u003e\n\u003cp\u003eMcCormack, L., Gonzalez, R.,  Fernandez, J., Hold, C., and Politis, A. 2022. \u003cstrong\u003e\u0026ldquo;Parametric Ambisonic Encoding using a Microphone Array with a One-plus-Three Configuration\u0026rdquo;\u003c/strong\u003e, In Audio Engineering Society Conference: AES 2022 International Audio for Virtual and Augmented Reality Conference. Audio Engineering Society.\u003c/p\u003e\n\u003cobject data="mccormack2022parametric_b.pdf" type="application/pdf" width="700px" height="700px" style="max-width: 100%"\u003e\n    \u003cembed src="mccormack2022parametric_b.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="mccormack2022parametric_b.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{mccormack2022parametric_b,\n  title={Parametric Ambisonic Encoding using a Microphone Array with a One-plus-Three Configuration},\n  author={McCormack, Leo and Gonzalez, Raimundo and Fernandez, Janani and Hold, Christoph and Politis, Archontis},\n  booktitle={Audio Engineering Society Conference: AES 2022 International Audio for Virtual and Augmented Reality Conference},\n  year={2022},\n  organization={Audio Engineering Society}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="mccormack2022parametric"\u003emccormack2022parametric\u003c/h3\u003e\n\u003cp\u003eMcCormack, L., Politis, A., Gonzalez, R., Lokki, T., and Pulkki, V. 2022. \u003cstrong\u003e\u0026ldquo;Parametric Ambisonic Encoding of Arbitrary Microphone Arrays\u0026rdquo;\u003c/strong\u003e, IEEE/ACM Transactions on Audio, Speech, and Language Processing 30, 2062-2075 \u003ca href="https://doi.org/10.1109/TASLP.2022.3182857"\u003ehttps://doi.org/10.1109/TASLP.2022.3182857\u003c/a\u003e\u003c/p\u003e\n\u003cobject data="mccormack2022parametric.pdf" type="application/pdf" width="700px" height="700px" style="max-width: 100%"\u003e\n    \u003cembed src="mccormack2022parametric.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="mccormack2022parametric.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@article{mccormack2022parametric,\n  title={Parametric Ambisonic Encoding of Arbitrary Microphone Arrays}, \n  author={McCormack, Leo and Politis, Archontis and Gonzalez, Raimundo and Lokki, Tapio and Pulkki, Ville},\n  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},\n  volume = {30},\n  number = {},\n  pages = {2062-2075},\n  year = {2022},\n  doi = {10.1109/TASLP.2022.3182857}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="mccormack2022object"\u003emccormack2022object\u003c/h3\u003e\n\u003cp\u003eMcCormack, L., Politis, A., McKenzie, T., Hold, C., and Pulkki V. 2022. \u003cstrong\u003e\u0026ldquo;Object-Based Six-Degrees-of-Freedom Rendering of Sound Scenes Captured with Multiple Ambisonic Receivers\u0026rdquo;\u003c/strong\u003e, The Journal of the Audio Engineering Society 70(5), 355-372 \u003ca href="https://doi.org/10.17743/jaes.2022.0010"\u003ehttps://doi.org/10.17743/jaes.2022.0010\u003c/a\u003e\u003c/p\u003e\n\u003cobject data="mccormack2022object.pdf" type="application/pdf" width="700px" height="700px" style="max-width: 100%"\u003e\n    \u003cembed src="mccormack2022object.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="mccormack2022object.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@article{mccormack2022object,\n  title={Object-Based Six-Degrees-of-Freedom Rendering of Sound Scenes Captured with Multiple Ambisonic Receivers}, \n  author={McCormack, Leo and Politis, Archontis and McKenzie, Thomas and Hold, Christoph and Pulkki, Ville},\n  journal = {The Journal of the Audio Engineering Society},\n  volume = {70},\n  number = {5},\n  pages = {355-372},\n  year = {2022},\n  doi = {https://doi.org/10.17743/jaes.2022.0010}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="fernandez2022enhancing"\u003efernandez2022enhancing\u003c/h3\u003e\n\u003cp\u003eFernandez, J., McCormack, L., Hyvärinen, P., Politis, A., and Pulkki V. 2022. \u003cstrong\u003e\u0026ldquo;Enhancing binaural rendering of head-worn microphone arrays through the use of adaptive spatial covariance matching\u0026rdquo;\u003c/strong\u003e, The Journal of the Acoustical Society of America 151(4), 2624-2635 \u003ca href="https://doi.org/10.1121/10.0010109"\u003ehttps://doi.org/10.1121/10.0010109\u003c/a\u003e\u003c/p\u003e\n\u003cobject data="fernandez2022enhancing.pdf" type="application/pdf" width="700px" height="700px" style="max-width: 100%"\u003e\n    \u003cembed src="fernandez2022enhancing.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="fernandez2022enhancing.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@article{fernandez2022enhancing,\n  title={Enhancing binaural rendering of head-worn microphone arrays through the use of adaptive spatial covariance matching}, \n  author={Fernandez, Janani and McCormack, Leo and Hyvärinen, Petteri and Politis, Archontis and Pulkki, Ville},\n  journal = {The Journal of the Acoustical Society of America},\n  volume = {151},\n  number = {4},\n  pages = {2624-2635},\n  year = {2022},\n  doi = {10.1121/10.0010109}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="2021"\u003e2021\u003c/h2\u003e\n\u003ch3 id="mckenzie2021dataset"\u003emckenzie2021dataset\u003c/h3\u003e\n\u003cp\u003eMcKenzie, T., McCormack, L., and Hold, C. 2021, November. \u003cstrong\u003eDataset of Spatial Room Impulse Responses in a Variable Acoustics Room for Six Degrees-of-Freedom Rendering and Analysis.\u003c/strong\u003e arXiv:2111.11882v1 [eess.AS].\u003c/p\u003e\n\u003cobject data="mckenzie2021dataset.pdf" type="application/pdf" width="700px" height="700px" style="max-width: 100%"\u003e\n    \u003cembed src="mckenzie2021dataset.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="mckenzie2021dataset.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@misc{mckenzie2021dataset,\n  title={Dataset of Spatial Room Impulse Responses in a Variable Acoustics Room for Six Degrees-of-Freedom Rendering and Analysis}, \n  author={McKenzie, Thomas and McCormack, Leo and Hold, Christoph},\n  year={2021},\n  eprint={2111.11882},\n  archivePrefix={arXiv},\n  primaryClass={eess.AS}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="mccrea2021sound"\u003emccrea2021sound\u003c/h3\u003e\n\u003cp\u003eMcCrea, M., McCormack, L., and Pulkki, V. 2021, November. \u003cstrong\u003eSound Source Localization Using Sector-Based Analysis With Multiple Receivers.\u003c/strong\u003e In 2nd Nordic Sound and Music Conference (NordicSMC).\u003c/p\u003e\n\u003cobject data="mccrea2021sound.pdf" type="application/pdf" width="700px" height="700px" style="max-width: 100%"\u003e\n    \u003cembed src="mccrea2021sound.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="mccrea2021sound.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{mccrea2021sound,\n  title={Sound Source Localization Using Sector-Based Analysis With Multiple Receivers},\n  author={McCrea, Michael and McCormack, Leo and Pulkki, Ville},\n  booktitle={2nd Nordic Sound and Music Conference},\n  year={2021}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="mccormack2021rendering"\u003emccormack2021rendering\u003c/h3\u003e\n\u003cp\u003eMcCormack, L. Politis, A., and Pulkki, V., 2021, October. \u003cstrong\u003eRendering of source spread for arbitrary playback setups based on spatial covariance matching.\u003c/strong\u003e In 2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). IEEE.\u003c/p\u003e\n\u003cobject data="mccormack2021rendering.pdf" type="application/pdf" width="700px" height="700px" style="max-width: 100%"\u003e\n    \u003cembed src="mccormack2021rendering.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="mccormack2021rendering.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{mccormack2021rendering,\n  title={Rendering of source spread for arbitrary playback setups based on spatial covariance matching},\n  author={McCormack, Leo and Politis, Archontis and Pulkki, Ville},\n  booktitle={2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},\n  year={2021},\n  organization={IEEE}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eConference presentation:\u003c/p\u003e\n\u003ciframe width="560" height="315" style="max-width: 100%" src="https://www.youtube.com/embed/A3hnPRwyECQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen\u003e\u003c/iframe\u003e\n\u003ch3 id="mccormack2021parametric"\u003emccormack2021parametric\u003c/h3\u003e\n\u003cp\u003eMcCormack, L., Politis, A., and Pulkki, V., 2021, September. \u003cstrong\u003eParametric Spatial Audio Effects Based on the Multi-Directional Decomposition of Ambisonic Sound Scenes.\u003c/strong\u003e In Proceedings of the 24th International Conference on Digital Audio Effects (DAFx20in21), (pp. 214-221).\u003c/p\u003e\n\u003cobject data="mccormack2021parametric.pdf" type="application/pdf" width="700px" height="700px" style="max-width: 100%"\u003e\n    \u003cembed src="mccormack2021parametric.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="mccormack2021parametric.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{mccormack2021parametric,\n  title={Parametric Spatial Audio Effects Based on the Multi-Directional Decomposition of Ambisonic Sound Scenes},\n  author={McCormack, Leo and Politis, Archontis and Pulkki, Ville},\n  booktitle={Proceedings of the 24th International Conference on Digital Audio Effects (DAFx20in21)},\n  pages={214--221},\n  year={2021}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eConference presentation:\u003c/p\u003e\n\u003ciframe width="560" height="315" style="max-width: 100%" src="https://www.youtube.com/embed/Bp4TlDZx6mA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen\u003e\u003c/iframe\u003e\n\u003ch3 id="mccormack2021real"\u003emccormack2021real\u003c/h3\u003e\n\u003cp\u003eMcCormack, L., Politis, A., Särkkä, S., and Pulkki, V., 2021. \u003cstrong\u003eReal-Time Tracking of Multiple Acoustical Sources Utilising Rao-Blackwellised Particle Filtering.\u003c/strong\u003e In 29th European Signal Processing Conference (EUSIPCO), (pp. 206-210).\u003c/p\u003e\n\u003cobject data="mccormack2021real.pdf" type="application/pdf" width="700px" height="700px" style="max-width: 100%"\u003e\n    \u003cembed src="mccormack2021real.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="mccormack2021real.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{mccormack2021real,\n  title={Real-Time Tracking of Multiple Acoustical Sources Utilising Rao-Blackwellised Particle Filtering},\n  author={McCormack, Leo and Politis, Archontis and Särkkä, Simo and Pulkki, Ville},\n  booktitle={29th European Signal Processing Conference (EUSIPCO)},\n  pages={206--210},\n  year={2021},\n  organization={EURASIP}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eConference presentation:\u003c/p\u003e\n\u003ciframe width="560" height="315" style="max-width: 100%" src="https://www.youtube.com/embed/owThCIunY4w" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen\u003e\u003c/iframe\u003e\n\u003ch3 id="hold2021spatial"\u003ehold2021spatial\u003c/h3\u003e\n\u003cp\u003eHold, C., Politis, A., McCormack, L., and Pulkki, V., 2021. \u003cstrong\u003eSpatial Filter Bank Design in the Spherical Harmonic Domain.\u003c/strong\u003e In 29th European Signal Processing Conference (EUSIPCO), (pp. 106-110).\u003c/p\u003e\n\u003cobject data="hold2021spatial.pdf" type="application/pdf" width="700px" height="700px" style="max-width: 100%"\u003e\n    \u003cembed src="hold2021spatial.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="hold2021spatial.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{hold2021spatial,\n  title={Spatial Filter Bank Design in the Spherical Harmonic Domain},\n  author={Hold, Christoph and Politis, Archontis and McCormack, Leo and Pulkki, Ville},\n  booktitle={29th European Signal Processing Conference (EUSIPCO)},\n  pages={106--110},\n  year={2021},\n  organization={EURASIP}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="pulkki2021superhuman"\u003epulkki2021superhuman\u003c/h3\u003e\n\u003cp\u003ePulkki, V., McCormack, L. and Gonzalez, R., 2021. \u003cstrong\u003eSuperhuman spatial hearing technology for ultrasonic frequencies\u003c/strong\u003e. Scientific Reports, 11(1), pp.1-10.\u003c/p\u003e\n\u003cobject data="pulkki2021superhuman.pdf" type="application/pdf" width="700px" height="700px" style="max-width: 100%"\u003e\n    \u003cembed src="pulkki2021superhuman.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="pulkki2021superhuman.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@article{pulkki2021superhuman,\n  title={Superhuman spatial hearing technology for ultrasonic frequencies},\n  author={Pulkki, Ville and McCormack, Leo and Gonzalez, Raimundo},\n  journal={Scientific Reports},\n  volume={11},\n  number={1},\n  pages={1--10},\n  year={2021},\n  publisher={Nature Publishing Group}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="2020"\u003e2020\u003c/h2\u003e\n\u003ch3 id="mccormack2020higher"\u003emccormack2020higher\u003c/h3\u003e\n\u003cp\u003eMcCormack, L., Pulkki, V., Politis, A., Scheuregger, O. and Marschall, M., 2020. \u003cstrong\u003eHigher-order spatial impulse response rendering: Investigating the perceived effects of spherical order, dedicated diffuse rendering, and frequency resolution.\u003c/strong\u003e Journal of the Audio Engineering Society, 68(5), pp.338-354.\u003c/p\u003e\n\u003cobject data="mccormack2020higher.pdf" type="application/pdf" width="700px" height="700px" style="max-width: 100%"\u003e\n    \u003cembed src="mccormack2020higher.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="mccormack2020higher.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@article{mccormack2020higher,\n  title={Higher-order spatial impulse response rendering: Investigating the perceived effects of spherical order, dedicated diffuse rendering, and frequency resolution},\n  author={McCormack, Leo and Pulkki, Ville and Politis, Archontis and Scheuregger, Oliver and Marschall, Marton},\n  journal={Journal of the Audio Engineering Society},\n  volume={68},\n  number={5},\n  pages={338--354},\n  year={2020},\n  publisher={Audio Engineering Society}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="2019"\u003e2019\u003c/h2\u003e\n\u003ch3 id="mccormack2019applications"\u003emccormack2019applications\u003c/h3\u003e\n\u003cp\u003eMcCormack, L., Delikaris-Manias, S., Politis, A., Pavlidi, D., Farina, A., Pinardi, D. and Pulkki, V., 2019. \u003cstrong\u003eApplications of spatially localized active-intensity vectors for sound-field visualization.\u003c/strong\u003e Journal of the Audio Engineering Society, 67(11), pp.840-854.\u003c/p\u003e\n\u003cobject data="mccormack2019applications.pdf" type="application/pdf" width="700px" height="700px" style="max-width: 100%"\u003e\n    \u003cembed src="mccormack2019applications.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="mccormack2019applications.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@article{mccormack2019applications,\n  title={Applications of spatially localized active-intensity vectors for sound-field visualization},\n  author={McCormack, Leo and Delikaris-Manias, Symeon and Politis, Archontis and Pavlidi, Despoina and Farina, Angelo and Pinardi, Daniel and Pulkki, Ville},\n  journal={Journal of the Audio Engineering Society},\n  volume={67},\n  number={11},\n  pages={840--854},\n  year={2019},\n  publisher={Audio Engineering Society}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="mccormack2019higher"\u003emccormack2019higher\u003c/h3\u003e\n\u003cp\u003eMcCormack, L., Politis, A., Scheuregger, O. and Pulkki, V., 2019. \u003cstrong\u003eHigher-order processing of spatial impulse responses.\u003c/strong\u003e In 23rd International Congress on Acoustics (ICA).\u003c/p\u003e\n\u003cobject data="mccormack2019higher.pdf" type="application/pdf" width="700px" height="700px" style="max-width: 100%"\u003e\n    \u003cembed src="mccormack2019higher.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="mccormack2019higher.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{mccormack2019higher,\n  title={Higher-order processing of spatial impulse responses},\n  author={McCormack, Leo and Politis, Archontis and Scheuregger, Oliver and Pulkki, Ville},\n  year={2019},\n  booktitle={23rd International Congress on Acoustics (ICA)}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="mccormack2019parametric"\u003emccormack2019parametric\u003c/h3\u003e\n\u003cp\u003eMcCormack, L. and Delikaris-Manias, S., 2019, September. \u003cstrong\u003eParametric first-order ambisonic decoding for headphones utilising the cross-pattern coherence algorithm.\u003c/strong\u003e In EAA Spatial Audio Signal Processing Symposium (pp. 173-178).\u003c/p\u003e\n\u003cobject data="mccormack2019parametric.pdf" type="application/pdf" width="700px" height="700px" style="max-width: 100%"\u003e\n    \u003cembed src="mccormack2019parametric.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="mccormack2019parametric.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{mccormack2019parametric,\n  title={Parametric first-order ambisonic decoding for headphones utilising the cross-pattern coherence algorithm},\n  author={McCormack, Leo and Delikaris-Manias, Symeon},\n  booktitle={EAA Spatial Audio Signal Processing Symposium},\n  pages={173--178},\n  year={2019}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="hold2019improving"\u003ehold2019improving\u003c/h3\u003e\n\u003cp\u003eHold, C., Gamper, H., Pulkki, V., Raghuvanshi, N. and Tashev, I.J., 2019, May. \u003cstrong\u003eImproving binaural ambisonics decoding by spherical harmonics domain tapering and coloration compensation\u003c/strong\u003e. In International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 261-265). IEEE.\u003c/p\u003e\n\u003cobject data="hold2019improving.pdf" type="application/pdf" width="700px" height="700px" style="max-width: 100%"\u003e\n    \u003cembed src="hold2019improving.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="hold2019improving.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{hold2019improving,\n  title={Improving binaural ambisonics decoding by spherical harmonics domain tapering and coloration compensation},\n  author={Hold, Christoph and Gamper, Hannes and Pulkki, Ville and Raghuvanshi, Nikunj and Tashev, Ivan J},\n  booktitle={International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n  pages={261--265},\n  year={2019},\n  organization={IEEE}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="mccormack2019sharpening"\u003emccormack2019sharpening\u003c/h3\u003e\n\u003cp\u003eMcCormack, L., Politis, A. and Pulkki, V., 2019, May. \u003cstrong\u003eSharpening of Angular Spectra Based on a Directional Re-assignment Approach for Ambisonic Sound-field Visualisation.\u003c/strong\u003e In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 576-580). IEEE.\u003c/p\u003e\n\u003cobject data="mccormack2019sharpening.pdf" type="application/pdf" width="700px" height="700px" style="max-width: 100%"\u003e\n    \u003cembed src="mccormack2019sharpening.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="mccormack2019sharpening.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{mccormack2019sharpening,\n  title={Sharpening of Angular Spectra Based on a Directional Re-assignment Approach for Ambisonic Sound-field Visualisation},\n  author={McCormack, Leo and Politis, Archontis and Pulkki, Ville},\n  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n  pages={576--580},\n  year={2019},\n  organization={IEEE}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="mccormack2019sparta"\u003emccormack2019sparta\u003c/h3\u003e\n\u003cp\u003eMcCormack, L. and Politis, A., 2019, March. \u003cstrong\u003eSPARTA \u0026amp; COMPASS: Real-time implementations of linear and parametric spatial audio reproduction and processing methods.\u003c/strong\u003e In Audio Engineering Society Conference: 2019 AES International Conference on Immersive and Interactive Audio. Audio Engineering Society.\u003c/p\u003e\n\u003cobject data="mccormack2019sparta.pdf" type="application/pdf" width="700px" height="700px" style="max-width: 100%"\u003e\n    \u003cembed src="mccormack2019sparta.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="mccormack2019sparta.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{mccormack2019sparta,\n  title={SPARTA \\\u0026amp; COMPASS: Real-time implementations of linear and parametric spatial audio reproduction and processing methods},\n  author={McCormack, Leo and Politis, Archontis},\n  booktitle={Audio Engineering Society Conference: 2019 AES International Conference on Immersive and Interactive Audio},\n  year={2019},\n  organization={Audio Engineering Society}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="2018"\u003e2018\u003c/h2\u003e\n\u003ch3 id="politis2018compass"\u003epolitis2018compass\u003c/h3\u003e\n\u003cp\u003ePolitis, A., Tervo, S. and Pulkki, V., 2018, April. \u003cstrong\u003eCOMPASS: Coding and multidirectional parameterization of ambisonic sound scenes.\u003c/strong\u003e In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 6802-6806). IEEE.\u003c/p\u003e\n\u003cobject data="politis2018compass.pdf" type="application/pdf" width="700px" height="700px" style="max-width: 100%"\u003e\n    \u003cembed src="politis2018compass.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="politis2018compass.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{politis2018compass,\n  title={Compass: Coding and multidirectional parameterization of ambisonic sound scenes},\n  author={Politis, Archontis and Tervo, Sakari and Pulkki, Ville},\n  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n  pages={6802--6806},\n  year={2018},\n  organization={IEEE}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="mccormack2018real"\u003emccormack2018real\u003c/h3\u003e\n\u003cp\u003eMcCormack, L., Delikaris-Manias, S., Farina, A., Pinardi, D. and Pulkki, V., 2018, May. \u003cstrong\u003eReal-time conversion of sensor array signals into spherical harmonic signals with applications to spatially localized sub-band sound-field analysis.\u003c/strong\u003e In Audio Engineering Society Convention 144. Audio Engineering Society.\u003c/p\u003e\n\u003cobject data="mccormack2018real.pdf" type="application/pdf" width="700px" height="700px" style="max-width: 100%"\u003e\n    \u003cembed src="mccormack2018real.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="mccormack2018real.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{mccormack2018real,\n  title={Real-time conversion of sensor array signals into spherical harmonic signals with applications to spatially localized sub-band sound-field analysis},\n  author={McCormack, Leo and Delikaris-Manias, Symeon and Farina, Angelo and Pinardi, Daniel and Pulkki, Ville},\n  booktitle={Audio Engineering Society Convention 144},\n  year={2018},\n  organization={Audio Engineering Society}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="delikaris2018real"\u003edelikaris2018real\u003c/h3\u003e\n\u003cp\u003eDelikaris-Manias, S., McCormack, L., Huhtakallio, I. and Pulkki, V., 2018, May. \u003cstrong\u003eReal-time underwater spatial audio: a feasibility study\u003c/strong\u003e. In Audio Engineering Society Convention 144. Audio Engineering Society.\u003c/p\u003e\n\u003cobject data="delikaris2018real.pdf" type="application/pdf" width="700px" height="700px" style="max-width: 100%"\u003e\n    \u003cembed src="delikaris2018real.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="delikaris2018real.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{delikaris2018real,\n  title={Real-time underwater spatial audio: a feasibility study},\n  author={Delikaris-Manias, Symeon and McCormack, Leo and Huhtakallio, Ilkka and Pulkki, Ville},\n  booktitle={Audio Engineering Society Convention 144},\n  year={2018},\n  organization={Audio Engineering Society}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="2017"\u003e2017\u003c/h2\u003e\n\u003ch3 id="politis2017enhancement"\u003epolitis2017enhancement\u003c/h3\u003e\n\u003cp\u003ePolitis, A., McCormack, L. and Pulkki, V., 2017, October. \u003cstrong\u003eEnhancement of ambisonic binaural reproduction using directional audio coding with optimal adaptive mixing.\u003c/strong\u003e In 2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA) (pp. 379-383). IEEE.\u003c/p\u003e\n\u003cobject data="politis2017enhancement.pdf" type="application/pdf" width="700px" height="700px" style="max-width: 100%"\u003e\n    \u003cembed src="politis2017enhancement.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="politis2017enhancement.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{politis2017enhancement,\n  title={Enhancement of ambisonic binaural reproduction using directional audio coding with optimal adaptive mixing},\n  author={Politis, Archontis and McCormack, Leo and Pulkki, Ville},\n  booktitle={2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},\n  pages={379--383},\n  year={2017},\n  organization={IEEE}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="mccormack2017parametric"\u003emccormack2017parametric\u003c/h3\u003e\n\u003cp\u003eMcCormack, L., Delikaris-Manias, S. and Pulkki, V., 2017, September. \u003cstrong\u003eParametric acoustic camera for real-time sound capture, analysis and tracking.\u003c/strong\u003e In Proceedings of the 20th International Conference on Digital Audio Effects (DAFx-17) (pp. 412-419).\u003c/p\u003e\n\u003cobject data="mccormack2017parametric.pdf" type="application/pdf" width="700px" height="700px" style="max-width: 100%"\u003e\n    \u003cembed src="mccormack2017parametric.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="mccormack2017parametric.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{mccormack2017parametric,\n  title={Parametric acoustic camera for real-time sound capture, analysis and tracking},\n  author={McCormack, Leo and Delikaris-Manias, Symeon and Pulkki, Ville},\n  booktitle={Proceedings of the 20th International Conference on Digital Audio Effects (DAFx-17)},\n  pages={412--419},\n  year={2017}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="mccormack2017fft"\u003emccormack2017fft\u003c/h3\u003e\n\u003cp\u003eMcCormack, L., Välimäki, V. 2017, July. \u003cstrong\u003eFFT-based dynamic range compression.\u003c/strong\u003e In Proceedings of the 14th Sound and Music Computing Conference (pp. 42-49).\u003c/p\u003e\n\u003cobject data="mccormack2017fft.pdf" type="application/pdf" width="700px" height="700px" style="max-width: 100%"\u003e\n    \u003cembed src="mccormack2017fft.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="mccormack2017fft.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{mccormack2017fft,\n  title={{FFT}-based dynamic range compression},\n  author={McCormack, Leo and V{\\\u0026quot;a}lim{\\\u0026quot;a}ki, Vesa and others},\n  booktitle={Proceedings of the 14th Sound and Music Computing Conference, July},\n  pages={42--49},\n  year={2017}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="2015"\u003e2015\u003c/h2\u003e\n\u003ch3 id="politis2015sector"\u003epolitis2015sector\u003c/h3\u003e\n\u003cp\u003ePolitis, A., Vilkamo, J. and Pulkki, V., 2015. \u003cstrong\u003eSector-based parametric sound field reproduction in the spherical harmonic domain.\u003c/strong\u003e IEEE Journal of Selected Topics in Signal Processing, 9(5), pp.852-866.\u003c/p\u003e\n\u003cobject data="politis2015sector.pdf" type="application/pdf" width="700px" height="700px" style="max-width: 100%"\u003e\n    \u003cembed src="politis2015sector.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="politis2015sector.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@article{politis2015sector,\n  title={Sector-based parametric sound field reproduction in the spherical harmonic domain},\n  author={Politis, Archontis and Vilkamo, Juha and Pulkki, Ville},\n  journal={IEEE Journal of Selected Topics in Signal Processing},\n  volume={9},\n  number={5},\n  pages={852--866},\n  year={2015},\n  publisher={IEEE}\n}\n\u003c/code\u003e\u003c/pre\u003e\n'}).add({id:12,href:"https://leomccormack.github.io/sparta-site/docs/help/faq/",title:"FAQ",description:"Answers to frequently asked questions.",content:'\u003ch2 id="which-vst2-hosts-do-you-recommend-for-these-plug-ins"\u003eWhich VST2 hosts do you recommend for these plug-ins?\u003c/h2\u003e\n\u003cp\u003eReaper \u0026gt; MaxMSP = Plogue Bidule \u0026gt; Other.\u003c/p\u003e\n\u003ch2 id="what-does-this-sliderbuttondrop-down-menu-do"\u003eWhat does this slider/button/drop-down menu do?\u003c/h2\u003e\n\u003cp\u003eThe vast majority of sliders, toggle buttons, and drop-down menus have detailed tooltips. Simply hover your mouse over the parameter in question for a few seconds, and the respective tooltip will appear.\u003c/p\u003e\n\u003ch2 id="can-i-use-the-plug-ins-for-commercial-purposes"\u003eCan I use the plug-ins for commercial purposes?\u003c/h2\u003e\n\u003cp\u003eAll of the plug-ins in the SPARTA suite, COMPASS suite, the HO-SIRR application and the CroPaC-Binaural may be used for commercial purposes.\u003c/p\u003e\n\u003cp\u003eTheir source code may also be used for commercial purposes, provided that you follow the terms of the employed license. For example, for GPLv3 licensed code, commercial use is only possible if the derived works are also open-source and provided under the terms of the same GPLv3 license.\u003c/p\u003e\n\u003cp\u003eThe exception to the above is with the HO-DirAC suite, which can only be used for academic, personal, and/or non-commerical use. For more information, please refer to the \u003ca href="../../plugins/hodirac-suite/#license"\u003efull license terms\u003c/a\u003e for the HO-DirAC plug-ins.\u003c/p\u003e\n\u003ch2 id="why-are-there-no-vst3-versions-of-the-plug-ins"\u003eWhy are there no VST3 versions of the plug-ins?\u003c/h2\u003e\n\u003cp\u003eBuilding VST3 versions of the plugins is indeed possible. However, unfortunately, switching from VST2 to VST3 would be a step back in terms of being able to flexibly change the channel layouts and would restrict the maximum Ambisonic order and/or number of channels.\u003c/p\u003e\n\u003cp\u003eIn a nutshell, the VST2 SDK can support flexibly changing the number of channels between 1 and 64 (i.e. up to 7th order Ambisonics, and loudspeaker arrangements of up to 64 channels), whereas the VST3 versions instead supports only up to 24 channels (i.e. 3rd order Ambisonics, and up to 24 loudspeakers), which is quite a significant downgrade.\u003c/p\u003e\n\u003cp\u003eWhile Steinberg appear to be content with their self-imposed limitation for their own plugins, we are unable to do so as we frequently use these plugins with loudspeaker arrays comprising more than 24 channels (and 4th order+) for our own research and teaching purposes at Aalto University.\u003c/p\u003e\n\u003cp\u003eIf the VST3 SDK is updated, or a VST4 SDK comes along, which restores the features of the VST2 SDK, then we are happy to switch over. However, until then, we will have to stick with the VST2 SDK for our pre-compiled releases.\u003c/p\u003e\n\u003cp\u003eFor more information regarding the issue, please refer to the following Steinberg forum posts and GitHub issues:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href="https://sdk.steinberg.net/viewtopic.php?f=4\u0026amp;t=549"\u003ehttps://sdk.steinberg.net/viewtopic.php?f=4\u0026amp;t=549\u003c/a\u003e (18 April 2018)\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://sdk.steinberg.net/viewtopic.php?f=4\u0026amp;t=624"\u003ehttps://sdk.steinberg.net/viewtopic.php?f=4\u0026amp;t=6249\u003c/a\u003e (6 December 2018)\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://github.com/steinbergmedia/vst3sdk/issues/28"\u003ehttps://github.com/steinbergmedia/vst3sdk/issues/28\u003c/a\u003e (17 January 2019)\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://github.com/leomccormack/SPARTA/issues/28"\u003ehttps://github.com/leomccormack/SPARTA/issues/28\u003c/a\u003e (6 March 2021)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="where-can-i-learn-more-about-ambisonics"\u003eWhere can I learn more about Ambisonics?\u003c/h2\u003e\n\u003cp\u003eFor newcomers to the field, we recommend starting with \u003ca href="https://www.springer.com/gp/book/9783030172060"\u003ethis book\u003c/a\u003e, which is free to download and covers Ambisonics from a practical point-of-view.\u003c/p\u003e\n\u003ch2 id="are-there-any-online-communites-where-i-can-discuss-topics-related-to-spatial-audio"\u003eAre there any online communites where I can discuss topics related to spatial audio?\u003c/h2\u003e\n\u003cp\u003eThere a number of active Facebook groups, with some of the more popular ones including:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href="https://www.facebook.com/groups/SpatialAudioVRARMR"\u003eSpatial Audio in VR/AR/MR\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://www.facebook.com/groups/ambisonic"\u003e3D, Ambisonic, Binaural \u0026amp; Surround Sound Production\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://www.facebook.com/groups/Ambisonics.VR.360.Audio"\u003eAmbisonics VR 360 Audio\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOutside of these Facebook groups, there is also the \u003ca href="https://mail.music.vt.edu/mailman/listinfo/sursound"\u003eSurSound mailing list\u003c/a\u003e and the \u003ca href="https://www.reddit.com/r/SpatialAudio/"\u003eSpatialAudio subreddit\u003c/a\u003e.\u003c/p\u003e\n'}).add({id:13,href:"https://leomccormack.github.io/sparta-site/docs/sourcecode/",title:"Source code",description:"Source code for the included plug-ins.",content:""}).add({id:14,href:"https://leomccormack.github.io/sparta-site/docs/help/",title:"Help",description:"Help documentation.",content:""}).add({id:15,href:"https://leomccormack.github.io/sparta-site/docs/plugins/",title:"The plug-ins",description:"All audio plug-ins included in the SPARTA installer.",content:""}).add({id:16,href:"https://leomccormack.github.io/sparta-site/docs/",title:"Documentation",description:"Documentation for the SPARTA installer plug-ins.",content:""}),userinput.addEventListener('input',b,!0),suggestions.addEventListener('click',c,!0);function b(){var d,e;const c=5;d=this.value,e=a.search(d,{limit:c,enrich:!0}),suggestions.classList.remove('d-none'),suggestions.innerHTML="";const b={};e.forEach(a=>{a.result.forEach(a=>{b[a.doc.href]=a.doc})});for(const d in b){const e=b[d],a=document.createElement('div');if(a.innerHTML='<a href><span></span><span></span></a>',a.querySelector('a').href=d,a.querySelector('span:first-child').textContent=e.title,a.querySelector('span:nth-child(2)').textContent=e.description,suggestions.appendChild(a),suggestions.childElementCount==c)break}}function c(){while(suggestions.lastChild)suggestions.removeChild(suggestions.lastChild);return!1}})()