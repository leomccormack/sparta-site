var suggestions=document.getElementById('suggestions'),userinput=document.getElementById('userinput');document.addEventListener('keydown',inputFocus);function inputFocus(a){a.keyCode===191&&(a.preventDefault(),userinput.focus()),a.keyCode===27&&(userinput.blur(),suggestions.classList.add('d-none'))}document.addEventListener('click',function(a){var b=suggestions.contains(a.target);b||suggestions.classList.add('d-none')}),document.addEventListener('keydown',suggestionFocus);function suggestionFocus(b){const d=suggestions.querySelectorAll('a'),e=[...d],a=e.indexOf(document.activeElement);let c=0;b.keyCode===38?(b.preventDefault(),c=a>0?a-1:0,d[c].focus()):b.keyCode===40&&(b.preventDefault(),c=a+1<e.length?a+1:a,d[c].focus())}(function(){var b=new FlexSearch({preset:'score',cache:!0,doc:{id:'id',field:['title','description','content'],store:['href','title','description']}}),c=[{id:0,href:"https://doks-child-theme.netlify.app/docs/plugins/overview/",title:"Overview",description:"What's included in the SPARTA installer?",content:'\u003cp\u003eThe SPARTA installer also includes the COMPASS suite, the HO-DirAC suite, CroPaC Binaural Decoder, and the HO-SIRR room impulse response renderer.\nThese plug-ins employ parametric processing and are signal-dependent. They aim to go beyond conventional linear Ambisonics algorithms, used by the baseline SPARTA plug-ins, by extracting meaningful parameters over time and subsequently employing them to map the input to the output in a more informed manner.\u003c/p\u003e\n\u003ch2 id="all-included-plug-ins"\u003eAll included plug-ins\u003c/h2\u003e\n\u003ch3 id="sparta-plug-ins"\u003eSPARTA plug-ins\u003c/h3\u003e\n\u003cp\u003eMore detailed descriptions can be found in \u003ca href="/docs/plugins/sparta-suite/"\u003eSPARTA Suite â†’\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href="../sparta-suite/#ambibin"\u003e\u003cstrong\u003esparta_ambiBIN\u003c/strong\u003e\u003c/a\u003e - A binaural ambisonic decoder (up to 7th order) with a built-in SOFA loader and head-tracking support via OSC messages. Includes: Least-Squares (LS), spatial re-sampling (SPR), time-alignment (TA), and magnitude least-squares (Mag-LS) decoding options.\u003c/li\u003e\n\u003cli\u003e\u003ca href="../sparta-suite/#ambidec"\u003e\u003cstrong\u003esparta_ambiDEC\u003c/strong\u003e\u003c/a\u003e - A frequency-dependent loudspeaker ambisonic decoder (up to 7th order) with user specifiable loudspeaker directions (up to 64), which may be optionally imported via JSON configuration files. Includes: All-Round (AllRAD), Energy-Preserving (EPAD), Spatial (SAD), and Mode-Matching (MMD) ambisonic decoding options. The loudspeaker signals may also be binauralised for headphone playback.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esparta_ambiDRC\u003c/strong\u003e - A frequency-dependent dynamic range compressor for ambisonic signals (up to 7th order).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esparta_ambiENC\u003c/strong\u003e - An ambisonic encoder/panner (up to 7th order), with support for up to 64 input channels; the directions for which may also be imported via JSON configuration files.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esparta_ambiRoomSim\u003c/strong\u003e - A shoebox room simulator based on the image-source method, supporting multiple sources and ambisonic receivers..\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esparta_array2sh\u003c/strong\u003e - A microphone array spatial encoder (up to 7th order), with presets for several commercially available A-format and higher-order microphone arrays. The plug-in can also present objective evaluation metrics for the currently selected configuration.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esparta_beamformer\u003c/strong\u003e - A spherical harmonic domain beamforming plug-in with multiple beamforming strategies (up to 64 output beams).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esparta_binauraliser\u003c/strong\u003e - A binaural panner (up to 64 input channels) with a built-in SOFA loader and head-tracking support via OSC messages.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esparta_decorrelator\u003c/strong\u003e - A simple multi-channel signal decorrelator (up to 64 input channels).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esparta_dirass\u003c/strong\u003e - A sound-field visualiser based on re-assigning the energy of beamformers. This re-assigment is based on DoA estimates extracted from \u0026ldquo;spatially-constrained\u0026rdquo; regions, which are centred around each beamformer look-direction.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esparta_matrixconv\u003c/strong\u003e - A basic matrix convolver with an optional partitioned convolution mode. The user need only specify the number of inputs and load the filters via a wav file.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esparta_multiconv\u003c/strong\u003e - A basic multi-channel convolver with an optional partitioned convolution mode. Unlike \u0026ldquo;MatrixConv\u0026rdquo;, this plug-in does not perform any matrixing. Instead, each input channel is convolved with the respective filter; i.e. numInputs = numFilters = numOutputs.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esparta_panner\u003c/strong\u003e - A frequency-dependent 3-D panner using the VBAP method (up to 64 inputs and outputs).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esparta_powermap\u003c/strong\u003e - A sound-field visualisation plug-in based on ambisonic signals as input (up to 7th order), with PWD/MVDR/MUSIC/Min-Norm options.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esparta_rotator\u003c/strong\u003e - A flexible ambisonic rotator (up to 7th order) with head-tracking support via OSC messages.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esparta_sldoa\u003c/strong\u003e - A frequency-dependent sound-field visualiser (up to 7th order), based on depicting the direction-of-arrival (DoA) estimates derived from spatially localised active-intensity vectors. The low frequency estimates are shown with blue icons, mid-frequencies with green, and high-frequencies with red.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esparta_spreader\u003c/strong\u003e - An arbitrary array (e.g. HRIRs or microphone array IRs) panner with coherent and incoherent spreading options.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id="compass-plug-ins"\u003eCOMPASS plug-ins\u003c/h3\u003e\n\u003cp\u003e\u003cdiv class="alert alert-warning d-flex" role="alert"\u003e\r\n  \u003cdiv class="flex-shrink-1 alert-icon"\u003eðŸ‘‰ \u003c/div\u003e\r\n  \r\n    \u003cdiv class="w-100"\u003eThese plug-ins are intended for experienced users. \u003c/div\u003e\r\n  \r\n\u003c/div\u003e\r\n\nMore detailed descriptions can be found in \u003ca href="/docs/plugins/compass-suite/"\u003eCOMPASS Suite â†’\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ecompass_binaural\u003c/strong\u003e - A binaural ambisonic decoder (up to 3rd order input) based on the parametric COMPASS model, with a built-in SOFA loader and head-tracking support via OSC messages.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ecompass_binauralVR\u003c/strong\u003e - Same as the compass_binaural plugin, but also supporting listener translation around the receiver position and support for multiple simultaneous listeners.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ecompass_decoder\u003c/strong\u003e - A parametrically enhanced loudspeaker ambisonic decoder (up to 3rd order input).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ecompass_gravitator\u003c/strong\u003e - A parametric sound-field focussing plug-in.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ecompass_sidechain\u003c/strong\u003e - A plug-in that manipulates the spatial properties of one Ambisonic recording based on the spatial analysis of a different recording.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ecompass_spatedit\u003c/strong\u003e - A flexible spatial editing plug-in.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ecompass_tracker\u003c/strong\u003e - A multiple target acoustic tracker which can optionally steer a beamformer towards each target.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ecompass_upmixer\u003c/strong\u003e - An Ambisonic upmixer (1-3rd order input, 2-7th order output).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id="ho-dirac-plug-ins"\u003eHO-DirAC plug-ins\u003c/h3\u003e\n\u003cdiv class="alert alert-warning d-flex" role="alert"\u003e\r\n  \u003cdiv class="flex-shrink-1 alert-icon"\u003eðŸ‘‰ \u003c/div\u003e\r\n  \r\n    \u003cdiv class="w-100"\u003eThese plug-ins are intended for experienced users. \u003c/div\u003e\r\n  \r\n\u003c/div\u003e\r\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ehodirac_binaural\u003c/strong\u003e - A binaural ambisonic decoder (up to 3rd order input) based on the parametric HO-DirAC model, with a built-in SOFA loader and head-tracking support via OSC messages.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ehodirac_decoder\u003c/strong\u003e - A parametrically enhanced loudspeaker ambisonic decoder (up to 3rd order input).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ehodirac_upmixer\u003c/strong\u003e - An Ambisonic upmixer (1-3rd order input, 2-7th order output).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id="other-plug-ins"\u003eOther plug-ins\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ecropac_decoder\u003c/strong\u003e - A binaural 1st order ambisonic decoder based on the parametric CroPaC model, with a built-in SOFA loader and head-tracking support via OSC messages.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHOSIRR\u003c/strong\u003e - An Ambisonic room impulse response (RIR) renderer for arbitrary loudspeaker setups, based on the Higher-order Spatial Impulse Response Rendering (HO-SIRR) algorithm; more information can be found here.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="help"\u003eHelp\u003c/h2\u003e\n\u003cp\u003eGet help on Doks. \u003ca href="/docs/help/how-to-update/"\u003eHelp â†’\u003c/a\u003e\u003c/p\u003e\n'},{id:1,href:"https://doks-child-theme.netlify.app/docs/plugins/sparta-suite/",title:"SPARTA Suite",description:"Plug-in descriptions for the SPARTA suite.",content:'\u003ch2 id="plug-in-descriptions"\u003ePlug-in descriptions\u003c/h2\u003e\n\u003cp\u003eAll plug-ins are tested using REAPER (64-bit), which is a very affordable and flexible DAW and is currently the only recommended host for these plug-ins; although, other hosts are also known to work. Currently, the plug-ins support sampling rates of 44.1 or 48kHz. All spherical harmonic-related plug-ins conform to the Ambisonic Channel Number (ACN) ordering convention and offer support for both orthonormalised (N3D) and semi-normalised (SN3D) normalisation schemes; note that the AmbiX format uses ACN/SN3D. The maximum transform order for these plug-ins is 7.\u003c/p\u003e\n\u003cp\u003eYou may also hover your mourse cursor over any of the combo boxes/sliders/toggle buttons etc., in order to be presented with helpful tooltips regarding the purpose of the parameter.\u003c/p\u003e\n\u003cp\u003eThanks to help from Daniel Rudrich, the relevant plug-ins now also support importing and exporting of loudspeaker, source, and sensors directions via .json configuration files; allowing for cross-compatibility between SPARTA and the IEM Ambisonics plug-in suite. More information regarding the structure of these files can be found here.\u003c/p\u003e\n\u003cp\u003eThe default HRIR set is an 836-point simulation of a Kemar Dummy head, courtesy of Genelec AuralID.\u003c/p\u003e\n\u003ch3 id="ambibin"\u003eAmbiBIN\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="AmbiBIN_GUI.png" alt="AmbiBIN_GUI" caption="\u003cem\u003eAmbiBIN_GUI\u003c/em\u003e" class="border-0" width="550"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eA binaural Ambisonic decoder for headphone playback of Ambisonic signals (aka spherical harmonic or B-format signals), with a built-in rotator and head-tracking support via OSC messages. The rotation angles are updated after the time-frequency transform, which allows for reduced latency compared to its loudspeaker counterpart \u0026lsquo;AmbiDEC\u0026rsquo; when paired with \u0026lsquo;Rotator\u0026rsquo;. The plug-in also allows the user to import their own HRIRs via the SOFA standard. The plug-in offers a variety of different decoding methods, including: Least-Squares (LS), Spatial re-sampling (SPR), Time-Alignment (TA) [11], and Magnitude Least-Squares (MagLS) [12]. It can also impose a diffuse-coherence contraint/correction on the current decoder, as described in [11].\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack, Archontis Politis, and Christoph Hold.\u003c/p\u003e\n\u003ch3 id="ambidec"\u003eAmbiDEC\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="AmbiDEC_GUI.png" alt=""  width="550"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eA frequency-dependent Ambisonic decoder for loudspeakers. The loudspeaker directions can be user-specified for up to 64 channels, or alternatively presets for popular 2D and 3D set-ups can be selected. For headphone reproduction, the loudspeaker audio is convolved with interpolated HRTFs for each loudspeaker direction (i.e. virtual loudspeaker decoding). The plug-in also permits importing custom HRIRs via the SOFA standard.\u003c/p\u003e\n\u003cp\u003eThe plug-in employs a dual decoding approach, whereby different decoder settings may be selected for the low and high frequencies; the cross-over frequency may be dictated by the user. Several ambisonic decoders have been integrated, including more perceptually motivated methods such as the All-Round Ambisonic Decoder (AllRAD) [1] and Energy-Preserving Ambisonic Decoder (EPAD) [2]. The max-rE weighting [1] may also be enabled for either decoder. Furthermore, in the case of non-ideal Ambisonic signals as input (i.e. those derived from physical/simulated microphone arrays), the decoding order may be specified for the appropriate frequency ranges; energy-preserving (EP) or amplitude-preserving (AP) normalisation is then used to maintain consistent loudness between different decoding orders. However, this feature may also be used creatively. For example, one can reduce the decoding order only for a certain frequency-range, thereby making the reproduction more spatially spread at these specific frequencies (due to the inherently lower spatial resolution when using lower-order Ambisonic signals for decoding).\u003c/p\u003e\n\u003cp\u003eNote that when the loudspeakers are uniformly distributed (e.g. a t-design), all of the decoding approaches implemented in the plug-in are equivelent. This can be effectively demonstrated by selecting a T-design loudspeaker set-up (a nearly-uniform distribution of points on a sphere). The benefits of the Mode-Matching decoding (MMD), AllRAD and EPAD approaches can be observed for non-uniform arrangements (22.x for example).\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Archontis Politis.\u003c/p\u003e\n\u003ch3 id="ambidrc"\u003eAmbiDRC\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="AmbiDRC_GUI.png" alt="" width="450"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eThe AmbiDRC plug-in is based on the design proposed in \u003ca href="https://www.researchgate.net/publication/321319451_FFT-based_Dynamic_Range_Compression"\u003ethis publication\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eA frequency-dependent Ambisonic dynamic range compressor (DRC). The gain factors are derived by analysing the omnidirectional component for each frequency band, which are then applied also to the higher-order components. The spatial properties of the original signals remains unchanged; although, your perception of them after decoding may change. The implementation also keeps track of the frequency-dependent gain factors for the omnidirectional component over time, which is then plotted on the user interface for visual feedback.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack.\u003c/p\u003e\n\u003ch3 id="ambienc"\u003eAmbiENC\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="AmbiENC_GUI.png" alt="" width="550"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eA bare-bones Ambisonic encoder which takes input signals (up to 64 channels) and encodes them into Ambisonic signals at specified directions. Essentially, these Ambisonic signals describe a synthetic sound-field, where the spatial resolution of this encoding is determined by the transform order. Several presets have been included for convenience (which allow for 22.x etc. audio to be encoded into 1-7th order Ambisonics, for example). The panning window is also fully mouse driven, and uses an equirectangular respresentation of the sphere to depict the azimuth and elevation angles of each source.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack.\u003c/p\u003e\n\u003ch3 id="ambiroomsim"\u003eAmbiRoomSim\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="ambiRoomSim_GUI.png" alt="" width="700"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eAn Ambisonic room encoder that includes room reflections. It is based on the image source method using a shoebox room model. It permits multiple sources, and also multiple Ambisonic receivers up to 64 channels in total; e.g. 16x first-order or 4x third-order receivers, or 1x 7th order receiver. The output receiver channels are stacked, e.g. 1-4 channels are for the 1st first-order receiver, 5-8 for the second etc.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack.\u003c/p\u003e\n\u003ch3 id="array2sh"\u003eArray2SH\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="Array2SH_GUI.png" alt="" width="700"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eThe Array2SH plug-in is related to \u003ca href="http://research.spa.aalto.fi/projects/sparta_vsts/publications/mccormack2018real.pdf"\u003ethis publication\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u0026lsquo;Array2SH\u0026rsquo; spatially encodes spherical/cylindrical array signals into spherical harmonic signals (aka: Ambisonic or B-Format signals). The plug-in utilises analytical descriptors, which ascertain the frequency and order-dependent influence that the physical properties of the array have on the plane-waves arriving at its surface. The plug-in allows the user to specify: the array type (spherical or cylindrical), whether the array has an open or rigid enclosure, the radius of the array, the radius of the sensors (in cases where they protrude out from the array), the sensor coordinates (up to 64 channels), sensor directivity (omni-dipole-cardioid), the speed of sound, and the acoustical admittance of the array material (in the case of rigid arrays). The plug-in then determines the order-dependent equalisation curves that need to be imposed onto the initial spherical harmonic signals estimate, in order to remove the influence of the array itself. However, especially for higher-orders, this generally results in a large amplification of the low frequencies (including the sensor noise at these frequencies that accompanies it); therefore, four different regularisation approaches have been integrated into the plug-in, which allow the user to make a compromise between noise amplification and transform accuracy. These target and regularised equalisation curves are depicted on the user interface to provide visual feedback.\u003c/p\u003e\n\u003cp\u003eThe plug-in also allows the user to \u0026lsquo;Analyse\u0026rsquo; the spatial encoding performance using objective measures described in [8,10], namely: the spatial correlation and the level difference. Here, the encoding matrices are applied to a simulated array, which is described by multichannel transfer functions of plane waves for 812 points on the surface of the spherical/cylindrical array. The resulting encoded array responses should ideally resemble spherical harmonic functions at the grid points. The spatial correlation is then derived by comparing the patterns of these responses with the patterns of ideal spherical harmonics, where \u0026lsquo;1\u0026rsquo; means they are perfect, and \u0026lsquo;0\u0026rsquo; completely uncorrelated; the spatial aliasing frequency can therefore be observed for each order, as the point where the spatial correlation tends towards 0. The level difference is then the mean level difference over all directions (diffuse level difference) between the ideal and simulated components. One can observe that higher permitted amplification limits [Max Gain (dB)] will result in noisier signals; however, this will also result in a wider frequency range of useful spherical harmonic components at each order. This analysis is primarily based on code written for publication [10], which compared the performance of various regularisation approaches of encoding filters, based on both theoretical and measured array responses.\u003c/p\u003e\n\u003cp\u003eNote that this ability to balance the noise amplification with the accuracy of the spatial encoding (to better suit a given application) is very important, for example: the perceived fidelity of Ambisonic decoded audio can be rather poor if the noise amplification is set too high; therefore, typically a much lower amplification regularisation limit is used in Ambisonics reproduction when compared to sound-field visualisation algorithms, or beamformers that employ appropriate post-filtering.\u003c/p\u003e\n\u003cp\u003eFor convenience, the specifications for several commercially available microphone arrays have been integrated as presets; including: MH Acoustic\u0026rsquo;s Eigenmike, the Zylia array, and various A-format microphone arrays. Additionally, by releasing this plug-in, one now has the ability to build/3-D print thier own spherical and cylindrical array, while having a convenient means of obtaining the corresponing spherical harmonic siganls; for example, a four capsule open-body hydrophone array was presented in [9], which utilised this Array2SH plug-in as the first step in visualising and auralising an underwater sound scene in real-time.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack, Symeon Delikaris-Manias and Archontis Politis.\u003c/p\u003e\n\u003ch3 id="beamformer"\u003eBeamformer\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="Beamformer_GUI.png" alt="" width="550"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eA simple beamforming plug-in. Currently includes static beam patterns only (cardioid, hyper-cardioid or max_rE weighted hyper-cardioid). More pattern options to follow in future.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack.\u003c/p\u003e\n\u003ch3 id="binauraliser"\u003eBinauraliser\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="Binauraliser_GUI.png" alt="" width="700"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cdiv class="alert alert-warning d-flex" role="alert"\u003e\r\n  \u003cdiv class="flex-shrink-1 alert-icon"\u003eðŸ‘‰ \u003c/div\u003e\r\n  \r\n    \u003cdiv class="w-100"\u003ePlease note that this plug-in is only suitable for HRTF-based convolution. \u003c/div\u003e\r\n  \r\n\u003c/div\u003e\r\n\n\u003cp\u003eA plug-in which convolves input audio (up to 64 channels) with interpolated HRTFs in the time-frequency domain. The HRTFs are interpolated by applying amplitude-normalised VBAP gains [4] to the HRTF magnitude responses and the estimated inter-aural time differences (ITDs) individually, before being re-combined. The plug-in also allows the user to specify an external SOFA file for the convolution. Presets for popular 2D and 3D formats are included for convenience; however, the directions for up to 64 channels can be independently controlled. Head-tracking is also supported via OSC messages in the same manner as with the Rotator plug-in.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Archontis Politis.\u003c/p\u003e\n\u003ch3 id="decorrelator"\u003eDecorrelator\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="decorrelator_GUI.png" alt="" width="450"/\u003e \u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eA simple multi-channel signal decorrelator (up to 64 channels) based on randomised time-frequency delays and cascaded all-pass filters.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack.\u003c/p\u003e\n\u003ch3 id="dirass"\u003eDirASS\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="DirASS_GUI.png" alt="" width="700"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eThe DirASS plug-in is related to these publications: \u003ca href="http://research.spa.aalto.fi/projects/sparta_vsts/publications/mccormack2019applications.pdf"\u003eLink1\u003c/a\u003e, \u003ca href="http://research.spa.aalto.fi/projects/sparta_vsts/publications/mccormack2019sharpening.pdf"\u003eLink2\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eA sound-field visualiser, which is based on the directional re-assignment of beamformer energy. This energy re-assignment is based on local DoA estimates for each scanning direction, and may be quantised to the nearest direction or upscaled to a higher-order than the input; resulting in sharper activity-maps. For example, a second-order input may be displayed with (up to) 20th order output resolution. The plug-in also allows the user to place real-time video footage behind the activity-map, in order to create a make-shift acoustic camera.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Archontis Politis.\u003c/p\u003e\n\u003ch3 id="matrixconv"\u003eMatrixConv\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="MatrixConv_GUI.png" alt="" width="450"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eA simple matrix convolver with an (optional) partitioned-convolution mode. The matrix of filters should be concatenated for each output channel and loaded as a .wav file. You need only inform the plug-in of the number if input channels, and it will take care of the rest.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExample 1, \u003cstrong\u003espatial reverberation\u003c/strong\u003e: if you have a B-Format/Ambisonic room impulse response (RIR), you may convolve it with a monophonic input signal and the output will exhibit (much of) the spatial characteristics of the measured room. Simply load this Ambisonic RIR into the plug-in and set the number of input channels to 1. You may then decode the resulting Ambisonic output to your loudspeaker array (e.g. using SPARTA|AmbiDEC) or to headphones (e.g. using SPARTA|AmbiBIN). However, please note that the limitations of lower-order Ambisonics for signals (namely, colouration and poor spatial accuracy) will also be present with lower-order Ambisonic RIRs; at least, when applied in this manner. Consider referring to Example 3, for a more spatially accurate method of reproducing the spatial characteristics of rooms, which are captured as B-Format/Ambisonic RIRs.\u003c/li\u003e\n\u003cli\u003eExample 2, \u003cstrong\u003emicrophone array to Ambisonics encoding\u003c/strong\u003e: if you have a matrix of filters to go from an Eigenmike (32 channel) recording to 4th order Ambisonics (25 channel), then the plugin requires a 25-channel wav file to be loaded, and the number of input channels to be set to 32. In this case: the first 32 filters will map the input to the first output channel, filters 33-64 will map the input to the second output channel, \u0026hellip; , and the last 32 filters will map the input to the 25th output channel. An example of such an encoding matrix may be downloaded from \u003ca href="http://research.spa.aalto.fi/projects/sparta_vsts/extras/M_eigen2sh_radinv_o4_ACN_N3D_15dB_max.wav"\u003ehere\u003c/a\u003e. Note that these example filters employ the ACN/\u003cstrong\u003eN3D\u003c/strong\u003e convention, Tikhonov regularisation, and 15dB of maximum gain amplification; using the Matlab scripts from \u003ca href="https://github.com/polarch/Spherical-Array-Processing"\u003e\u003cstrong\u003ehere\u003c/strong\u003e\u003c/a\u003e. This should be the same as SPARTA|Array2SH when it is set to the Eigenmike preset and default settings (except N3D not SN3D).\u003c/li\u003e\n\u003cli\u003eExample 3, \u003cstrong\u003emore advanced spatial reverberation\u003c/strong\u003e: if you have a monophonic recording of a trumpet and you wish to reproduce it as if it were in your favourite concert hall, first measure a B-Format/Ambisonic room impulse response (RIR) of the hall, and then convert this Ambisonic RIR to your loudspeaker set-up using \u003ca href="http://research.spa.aalto.fi/projects/sparta_vsts/hosirr.html"\u003eHO-SIRR\u003c/a\u003e. Then load the resulting rendered loudspeaker array RIR into the plug-in and set the number of input channels to 1. Note that you may prefer to use HO-SIRR (which is a parametric renderer), to convert your arbitrary order B-Format/Ambisonic IRs to arbitrary loudspeaker array IRs, as the resulting output will generally be much more spatially accurate when compared to linear (non-parametric) Ambisonic decoding; as described in Example 1. For the curious reader, an example of a 12point T-design loudspeaker array IR, made using a simulation [15] of the Vienna Musikverein concert hall, may be downloaded from \u003ca href="http://research.spa.aalto.fi/projects/sparta_vsts/extras/Vienna__12point-Tdesign.wav"\u003e\u003cstrong\u003ehere\u003c/strong\u003e\u003c/a\u003e. To listen to the convolved output, either arrange 12 loudspeakers in a t-design for the playback (a bit cumbersome), or use the SPARTA|Binauraliser plug-in set to \u0026ldquo;T-Design (12)\u0026rdquo; and listen over headphones.\u003c/li\u003e\n\u003cli\u003eExample 4, \u003cstrong\u003evirtual monitoring of a multichannel setup\u003c/strong\u003e: if you have a set of binaural head-related impulse responses (BRIRs) which correspond to the loudspeaker directions of a measured listening room, you may use this 2 x L matrix of filters to reproduce loudspeaker mixes (L-channels) over headphones. Simply concatenate the BRIRs for each input channel into a two channel wav file and load them into the plugin, then set the number of inputs to be the number of BRIRs/loudspeakers in the mix.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Archontis Politis.\u003c/p\u003e\n\u003ch3 id="multiconv"\u003eMultiConv\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="MultiConv_GUI.png" alt="" width="450"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eA simple multi-channel convolver with an (optional) partitioned-convolution mode. The plugin will convolve each input channel with the respective filter up to the maximum of 64 channels/filters. The filters are loaded as a multi-channel .wav file.\u003c/p\u003e\n\u003cp\u003ePlease note that this is not to be confused with the MatrixConv plug-in. For this plug-in, the number inputs = the number of filters = the number of outputs. i.e. no matrixing is applied.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExample, \u003cstrong\u003eheadphone equalisation\u003c/strong\u003e: post binauraliser/AmbiBIN etc., you may minimise the effect that your headphones have on the binaural output, by also convolving with (regularised) inverse filters. These filters may either be based on measurements of your own head and headphones, or you may download generic ones. For example, you can find equalisation filters for many commerically available headphones from \u003ca href="https://audiogroup.web.th-koeln.de/ku100hrir.html"\u003e\u003cstrong\u003ehere\u003c/strong\u003e\u003c/a\u003e; which have been measured using a dummy head (more information can be found in [16]).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Archontis Politis.\u003c/p\u003e\n\u003ch3 id="panner"\u003ePanner\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="Panner_GUI.png" alt="" width="700"/\u003e \u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eA frequency-dependent 3D panner based on the Vector-base Amplitude Panning (VBAP) method [4]. Presets for popular 2D and 3D formats are included for convenience; however, the directions for up to 64 channels can be independently controlled for both inputs and outputs; allowing, for example, 9.x input audio to be panned for a 22.2 setup. The panning is frequency-dependent to accommodate the method described in [5], which allows for more consistent loudness when sources are panned in-between the loudspeaker directions.\u003c/p\u003e\n\u003cp\u003eSet the \u0026ldquo;Room Coeff\u0026rdquo; parameter to 0 for standard power-normalisation, 0.5 for a listening room, and 1 for an anechoic chamber.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack, Archontis Politis and Ville Pulkki.\u003c/p\u003e\n\u003ch3 id="powermap"\u003ePowerMap\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="Powermap_GUI.png" alt="" width="700"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eThe PowerMap plug-in is a modified version of the plug-in described in \u003ca href="http://research.spa.aalto.fi/projects/sparta_vsts/publications/mccormack2017parametric.pdf"\u003e\u003cstrong\u003ethis publication\u003c/strong\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u0026lsquo;PowerMap\u0026rsquo; is a plug-in that represents the relative sound energy, or the statistical likelihood of a source, arriving at the listening position from a particular direction, using a colour gradient; where yellow indicates high sound energy/likelihood and blue indicates low sound energy/likelihood. The plug-in integrates a variety of different approaches, including: standard Plane-Wave Decomposition (PWD) beamformer-based, Minimum-Variance Distortionless Response (MVDR) beamformer-based, Multiple Signal Classification (MUSIC) pseudo-spectrum-based, and the Cross-Pattern Coherence (CroPaC) algorithm [3]; all of which are written to operate on Ambisonic signals up to 7th order. Note that the analysis order per frequency band is entirely user definable, and presets for higher order microphone arrays have been included for convience (which provide some rough yet appropriate starting values). The plug-in utilises a 812 point uniformly-distributed spherical grid, which is then interpolated into a 2D powermap using amplitude-normalised VBAP gains (i.e. triangular interpolation). The plug-in also allows the user to place real-time video footage behind the activity-map, in order to create a make-shift acoustic camera.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Symeon Delikaris-Manias.\u003c/p\u003e\n\u003ch3 id="rotator"\u003eRotator\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="Rotator_GUI.png" alt="" width="450"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eThis plug-in applies a Ambisonic rotation matrix [6] to the input Ambisonic signals. The rotation angles can be controlled using a head tracker via OSC messages. Simply configure the headtracker to send a vector: \u0026lsquo;\\ypr[3]\u0026rsquo; to OSC port 9000 (default); where \\ypr[0], \\ypr[1], \\ypr[2] are the yaw-pitch-roll angles, respectively. The angles can also be flipped +/- in order to support a wider range of devices. The rotation order (yaw-pitch-roll (default) or roll-pitch-yaw) can also be specified. Alternatively, the rotation can be based on a Quaternion by sending vector: \u0026lsquo;\\quaternion[4]\'; where \\quaternion[0], \\quaternion[1], \\quaternion[2], \\quaternion[3], are the W, X, Y, Z parts of the Quaternion, respectively.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack.\u003c/p\u003e\n\u003ch3 id="sldoa"\u003eSLDoA\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="SLDoA_GUI.png" alt="" width="600"/\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eThe SLDoA plug-in is related to these publications: \u003ca href="http://research.spa.aalto.fi/projects/sparta_vsts/publications/mccormack2018real.pdf"\u003e\u003cstrong\u003eLink1\u003c/strong\u003e\u003c/a\u003e, \u003ca href="http://research.spa.aalto.fi/projects/sparta_vsts/publications/mccormack2019applications.pdf"\u003e\u003cstrong\u003eLink2\u003c/strong\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eA spatially localised direction-of-arrival (DoA) estimator. The plug-in first uses VBAP beam patterns (for directions that are uniformly distributed on the surface of a shere) to obtain spatially-biased zeroth and first-order signals, which are subsequently used for the active-intensity vector estimation; therefore, allowing for DoA estimation in several spatially-constrained sectors for each sub-band. The low frequency estimates are then depicted with blue icons, mid-frequencies with green, and high-frequencies with red. The size of the icon and its opacity correspond to the energy of the sector, which are normalised and scaled in ascending order for each frequency band. The plug-in employs two times as many sectors as the analysis order, with the exception of the first-order analysis, which uses the traditional active-intensity approach. The analysis order per frequency band is user definable, as is the frequency range at which to analyse. This approach to sound-field visualisation/DoA estimation represents a much more computationally efficient option, when compared to the algorithms that are integrated into the \u0026lsquo;Powermap\u0026rsquo; plug-in, for instance. The plug-in also allows the user to place real-time video footage behind the activity-map, in order to create a make-shift acoustic camera.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Symeon Delikaris-Manias.\u003c/p\u003e\n\u003ch3 id="spreader"\u003eSpreader\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="spreader_GUI.png" alt="" width="700"/\u003e \u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eAn arbitrary array (e.g., HRIRs or microphone array IRs) panner with coherent and incoherent spreading options.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Archontis Politis.\u003c/p\u003e\n\u003ch2 id="about-the-developers"\u003eAbout the developers\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLeo McCormack\u003c/strong\u003e: a doctoral candidate at Aalto University.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSymeon Delikaris-Manias\u003c/strong\u003e: post doctorate researcher at Aalto University, specialising in compact microphone array processing for DoA estimation and sound-field reproduction. His doctoral research included work on the Cross-Pattern Coherence (CroPaC) algorithm, which is a spatial post-filter optimised for high noise/reverberant environments.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eArchontis Politis\u003c/strong\u003e: post doctorate researcher at Tampere University, specialising in spatial sound recording and reproduction, acoustic scene analysis and microphone array processing.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVille Pulkki\u003c/strong\u003e: Professor at Aalto University, known for VBAP, SIRR, DirAC and eccentric behaviour.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eChristoph Hold\u003c/strong\u003e: a doctoral candidate at Aalto University.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="references"\u003eReferences\u003c/h2\u003e\n\u003cp\u003e[1] Zotter, F., Frank, M. (2012). \u003cstrong\u003eAll-Round Ambisonic Panning and Decoding.\u003c/strong\u003e \u003cbr/\u003e Journal of the Audio Engineering Society, 60(10), 807-820.\u003c/p\u003e\n\u003cp\u003e[2] Zotter, F., Pomberger, H., Noisternig, M. (2012). \u003cstrong\u003eEnergy-Preserving Ambisonic Decoding.\u003c/strong\u003e \u003cbr\u003e Acta Acustica United with Acustica, 98(1), 37-47.\u003c/p\u003e\n\u003cp\u003e[3] Delikaris-Manias, S., Pulkki, V. (2013). \u003cstrong\u003eCross pattern coherence algorithm for spatial filtering applications utilizing microphone arrays.\u003c/strong\u003e \u003cbr\u003e IEEE Transactions on Audio, Speech, and Language Processing, 21(11), 2356-2367.\u003c/p\u003e\n\u003cp\u003e[4] Pulkki, V. (1997). \u003cstrong\u003eVirtual Sound Source Positioning Using Vector Base Amplitude Panning.\u003c/strong\u003e \u003cbr\u003eJournal of the Audio Engineering Society, 45(6), 456-466.\u003c/p\u003e\n\u003cp\u003e[5] Laitinen, M., Vilkamo, J., Jussila, K., Politis, A., Pulkki, V. (2014). \u003cstrong\u003eGain normalization in amplitude panning as a function of frequency and room reverberance.\u003c/strong\u003e \u003cbr\u003e55th International Conference of the AES. Helsinki, Finland.\u003c/p\u003e\n\u003cp\u003e[6] Ivanic, J., Ruedenberg, K. (1998). Rotation Matrices for Real Spherical Harmonics. Direct Determination by Recursion Page: Additions and Corrections. \u003c/b\u003e \u003cbr\u003eJournal of Physical Chemistry A, 102(45), 9099?9100.\u003c/p\u003e\n\u003cp\u003e[7] Faller, C. (2006). \u003cstrong\u003eMultiple-loudspeaker playback of stereo signals.\u003c/strong\u003e \u003cbr\u003e Journal of the Audio Engineering Society, 54(11), 1051-1064.\u003c/p\u003e\n\u003cp\u003e[8] Moreau, S., Daniel, J., Bertet, S. (2006). \u003cstrong\u003e3D sound field recording with higher order ambisonics-objective measurements and validation of spherical microphone.\u003c/strong\u003e \u003cbr\u003e in Audio Engineering Society Convention 120, Audio Engineering Society\u003c/p\u003e\n\u003cp\u003e[9] Delikaris-Manias, S., McCormack, L., Huhtakallio, I., and Pulkki, V. (2018) \u003cstrong\u003eReal-time underwater spatial audio: a feasibility study.\u003c/strong\u003e \u003cbr\u003e in Audio Engineering Society Convention 144, Audio Engineering Society.\u003c/p\u003e\n\u003cp\u003e[10] Politis, A., Gamper, H. (2017). \u003cstrong\u003eComparing Modelled And Measurement-Based Spherical Harmonic Encoding Filters For Spherical Microphone Arrays.\u003c/strong\u003e \u003cbr\u003e In IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA).\u003c/p\u003e\n\u003cp\u003e[11] Zaunschirm, M., SchÃ¶rkhuber, C., and HÃ¶ldrich, R. (2018). \u003cstrong\u003eBinaural rendering of Ambisonic signals by head-related impulse response time alignment and a diffuseness constraint.\u003c/strong\u003e \u003cbr\u003e The Journal of the Acoustical Society of America, 143(6), 3616-3627.\u003c/p\u003e\n\u003cp\u003e[12] SchÃ¶rkhuber, C., Zaunschirm, M., and HÃ¶ldrich, R. (2018). \u003cstrong\u003eBinaural Rendering of Ambisonic Signals via Magnitude Least Squares.\u003c/strong\u003e \u003cbr\u003e In Proceedings of the DAGA (Vol. 44).\u003c/p\u003e\n\u003cp\u003e[13] Politis, A., Tervo S., and Pulkki, V. (2018) \u003cstrong\u003eCOMPASS: Coding and Multidirectional Parameterization of Ambisonic Sound Scenes.\u003c/strong\u003e \u003cbr\u003e IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).\u003c/p\u003e\n\u003cp\u003e[14] McCormack, L., and Delikaris-Manias, S. (2019) \u003cstrong\u003eParametric First-order Ambisonic Decoding for Headphones Utilising the Cross-Pattern Coherence Algorithm\u003c/strong\u003e \u003cbr\u003e In Proceedings of the 1st EAA Spatial Audio Signal Processing Symposium, Paris, France, September 6-7th 2019.\u003c/p\u003e\n\u003cp\u003e[15] Favrot, S. and Buchholz, J.M., (2019).  \u003cstrong\u003eLoRA: A loudspeaker-based room auralization system.\u003c/strong\u003e \u003cbr\u003e Acta Acustica united with Acustica, 96(2), pp.364-375.\u003c/p\u003e\n\u003cp\u003e[16] Bernschutz, B., (2013). \u003cstrong\u003eA spherical far field HRIR/HRTF compilation of the Neumann KU 100.\u003c/strong\u003e \u003cbr\u003e In Proceedings of the 40th Italian (AIA) annual conference on acoustics and the 39th German annual conference on acoustics (DAGA) conference on acoustics (p. 29). AIA/DAGA.\u003c/p\u003e\n\u003ch2 id="other-included-plug-ins"\u003eOther included plug-ins\u003c/h2\u003e\n\u003cp\u003eThe SPARTA installer also includes:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe COMPASS suite \u003ca href="/docs/plugins/compass-suite/"\u003eCOMPASS â†’\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eThe HO-DirAC suite \u003ca href="/docs/plugins/compass-suite/"\u003eHO-DirAC â†’\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n'},{id:2,href:"https://doks-child-theme.netlify.app/docs/plugins/compass-suite/",title:"COMPASS suite",description:"Plug-in descriptions for the COMPASS suite.",content:'\u003ch2 id="plug-in-descriptions"\u003ePlug-in descriptions\u003c/h2\u003e\n\u003cp\u003eCOMPASS VSTs is a collection of flexible VST audio plug-ins for spatial audio production, manipulation, and reproduction, developed by Dr. Archontis Politis, Leo McCormack and Dr. Sakari Tervo in the Acoustics Lab at Aalto University.\u003c/p\u003e\n\u003cp\u003eCOMPASS is a framework for parametric spatial audio processing of sound scenes captured in the Ambisonics format. Parametric methods, such as Directional Audio Coding (DirAC) or HARPEX have gained notoriety recently for being able to achieve sharpness or envelopment beyond first or lower-order traditional Ambisonics playback, using the same lower-order Ambisonics signals. Contrary to the time-invariant linear processing of Ambisonics, which does not consider the sound components that comprise the sound scene, parametric methods assume a sound-field model for the sound scene and track the model parameters in the Ambisonics recording, in both time and frequency. The parameters are then used to render or upmix the sound scene flexibly to any playback system, without the constraints of lower-order Ambisonics. Furthermore, the spatial parameters allow flexible manipulation of the sound scene content in ways that are not possible with traditional Ambisonics processing.\u003c/p\u003e\n\u003cp\u003eThe COMPASS framework has been developed by Dr. Archontis Politis with contributions from Dr. Sakari Tervo and Leo McCormack, and published in \u003ca href="#ref_compass"\u003e[1]\u003c/a\u003e. The method is quite general in its model and estimates multiple direct sound components in every time-frequency block, and an ambient component capturing reverberation and other diffuse sounds. Here is a table of the COMPASS model compared to other published parametric techniques (note that M is the number of channels):\u003c/p\u003e\n\u003cp\u003e\u003cimg src="parametric_ambisonic_models.png" alt="" width="700"\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003eIn COMPASS, the ambient component is also spatial and can have directionality, contrary to previous models that force it to be isotropic. The VST plugins apply this framework to different spatial audio production tasks. Note that the plugins are still work in progress and we expect to keep improving them in the future, however, we believe that they can already prove useful to users and creators.\u003c/p\u003e\n\u003ch3 id="decoder"\u003eDecoder\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="Decoder_GUI.png" alt="" width="600;"\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003eThe COMPASS decoder is a parametric decoder for first, second, and third-order Ambisonics to arbitrary loudspeaker setups. The plugin offers the following functionality:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUser-specified loudspeaker angles for up to 64 channels, or alternatively, presets for popular 2D and 3D set-ups.\u003c/li\u003e\n\u003cli\u003eHeadphone binaural monitoring of the loudspeaker outputs, with support for user-provided personalised binaural filters (HRTFs) in the SOFA format.\u003c/li\u003e\n\u003cli\u003eBalance control between the extracted direct sound components and the ambient component, in frequency bands.\u003c/li\u003e\n\u003cli\u003eMixing control between fully parametric decoding and linear Ambisonic decoding, in frequency bands.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe \u0026ldquo;Diffuse-to-Direct\u0026rdquo; control allows the user to give more prominence to the direct sound components (an effect similar to subtle dereverberation), or to the ambient component (an effect similar to emphasising reverberation in the recording). When set in the middle, the two are balanced. Note that the parametric processing can be quite aggressive, and if one pushes it to fully direct rendering in a complex multi-source sound scene with FOA signals only, artefacts can easily appear. However, with more balanced settings, such artefacts  should become imperceptible.\u003c/p\u003e\n\u003cp\u003eThe \u0026ldquo;Linear-to-Parametric\u0026rdquo; control allows the user to mix the output between standard linear Ambisonic decoding and the COMPASS parametric decoding. This control can be used in cases where parametric processing sounds too aggressive, or if the user prefers some degree of increased localisation blur, offered by linear Ambisonic decoding.\u003c/p\u003e\n\u003cp\u003eThe plugin is considered by the authors a production tool and, due to its time-frequency processing, requires audio buffer sizes of at least 1024 samples. Hence we do not consider it as a low-latency plugin and therefore it is not suitable for interactive input. For cases such as interactive binaural rendering for VR with head-tracking, see the \u003cb\u003eCOMPASS|Binaural\u003c/b\u003e variant.\u003c/p\u003e\n\u003cp\u003eA video showing the plugin in action and demonstrating its functionality can be found here:\u003c/p\u003e\n\u003ciframe src="https://player.vimeo.com/video/312282907" width="640" height="360" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen\u003e\u003c/iframe\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Archontis Politis.\u003c/p\u003e\n\u003ch3 id="binaural"\u003eBinaural\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="Binaural_GUI.png" alt="" width="600;"\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003eThis is an optimised version of the COMPASS decoder for binaural playback, bypassing loudspeaker rendering and using binaural filters (HRTFs) directly, which can be user-provided and personalised with the SOFA format. For the plugin parameters, see the description of the \u003cb\u003eBinaural|Decoder\u003c/b\u003e above. Additionally the plugin can receive OSC rotation angles from a headtracker at a user specified port, in the yaw-pitch-roll convention.\u003c/p\u003e\n\u003cp\u003eThis version is intended mostly for head-tracked binaural playback of Ambisonic content at interactive update rates, usually in conjunction with a head-mounted display (HMD). The plugin requires an audio buffer size of at least 512 samples (~10msec at 48kHz). The averaging parameters can be used to make the parametric analysis and synthesis more or less responsive, providing the user with a means to adjust them optimally for a particular sound scene.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Archontis Politis.\u003c/p\u003e\n\u003ch3 id="binauralvr"\u003eBinauralVR\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="binauralVR_GUI.png" alt="" width="700;"\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003eSame as the COMPASS|Binaural plug-in, except it also supports listener translation around the receiver position. The user must first select the assumed distance of the sources. For simplicity, it is assumed that all sources are projected onto the surface of a sphere. The plug-in may then be informed of the listener position and orientation, either via its user interface sliders, or by sending  the Cartesian coordinates and rotation angles outputted by an external tracking device; such as a virtual or augmented reality headset.\u003c/p\u003e\n\u003cp\u003eThe listener position and head orientation can be passed via an OSC message: \\xyzypr[6], where x,y,z are in metres, and y,p,r are the yaw-pitch-roll angles in degrees (right-hand-rule).\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Archontis Politis.\u003c/p\u003e\n\u003ch3 id="tracker"\u003eTracker\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="tracker_GUI.png" alt="" width="720;"\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003eThis VST builds on the spatial analysis conducted by the Coding and Multidirectional Parameterisation of Ambisonic Sound Scenes (COMPASS) framework, but instead of using the information for synthesising loudspeaker or binaural signals, a multi-source tracker is employed to associate the estimated directions with their corresponding sources/targets. Therefore, this VST can be used to visualise the trajectory of sound sources present in an Ambisonic sound scene.\u003c/p\u003e\n\u003cp\u003eOptionally, a beamformer may then be steered to each target direction and outputted either as individual signals (one target signal per output channel; akin to decomposing the scene into its individual \u0026ldquo;stems\u0026rdquo;), or as a binauralisation of these individual \u0026ldquo;stems\u0026rdquo; (spatialised in their respective target directions).\u003c/p\u003e\n\u003cp\u003eNote that multi-source tracking has been an active research topic for several decades, but it is still considered to be a very difficult task. While we are confident in the robustness of this tracker and its implementation, there is a quite large learning curve in order to effectively tune the parameters for a specific sound scene/distribution. If the sound scene is very noisy or complex and encoded with first-order/lower resolution, then robust tracking may not even be possible. A general starting point is to first disable \u0026ldquo;Plot Targets\u0026rdquo; and tune the \u0026ldquo;Analysis Settings\u0026rdquo; until the direction estimates (plotted in the colour red) look reasonably clean. If you can make out the trajectories of the sources, then re-enable \u0026ldquo;Plot Targets\u0026rdquo; and try a few of the \u0026ldquo;Tracker Settings\u0026rdquo; presets and tune things from there. Note that each tracker parameter also has a tooltip describing how it influences the tracking, and the more these parameters/priors match the direction estimates distribution, the better the tracking will be.\u003c/p\u003e\n\u003cp\u003eThe tracker implementation builds on the Rao-Blackwelized Monte Carlo Data Association (RBMCDA) framework [11]. It should be noted that Sequential Monte Carlo (SMC) methods (also referred to as particle-filtering methods) involve making hundreds/thousands of hypotheses, which are then selected randomly based on their predicted likelihoods. Therefore, every time the tracker is run it will give you a different \u0026ldquo;answer\u0026rdquo; for the same input scene, but if the tracker is tuned well then the result should be substantially similar each time.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack.\u003c/p\u003e\n\u003ch3 id="upmixer"\u003eUpmixer\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="upmixer_GUI.png" alt="" width="530;"\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003eThis VST employs COMPASS for the task of upmixing a lower-order Ambisonic recording to a higher-order Ambisonic recording. It is intended for users that are already working with a preferred linear Ambisonic decoding workflow of higher-order Ambisonic content, and wish to combine lower-order Ambisonic material with increased spatial resolution. One can upmix first, second, or third-order material (4,9,16 channels) to up-to seventh-order material (64 channels).\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Archontis Politis.\u003c/p\u003e\n\u003ch3 id="sidechain"\u003eSideChain\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="sidechain_GUI.png" alt="" width="400;"\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003eThis VST is a bit of an experiment. Applying the COMPASS analysis to two different sound scenes (scene A [channels 1-16] and scene B [channels 17-32]), and using the estimated spatial parameters from one scene to manipulate the signals of the other scene.\u003c/p\u003e\n\u003cp\u003eNote that, if scene A and B are the same, then the plugin is functionally identical to compass_upmixer.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack.\u003c/p\u003e\n\u003ch3 id="spatedit"\u003eSpatEdit\u003c/h3\u003e\n\u003cimg src="spatEdit_A.png" alt="" width="650;"\u003e\n\u003cimg src="spatEdit_B.png" alt="" width="650;"\u003e\u003c/br\u003e\n\u003cp\u003eThe SpatEdit plug-in is intended to be used with two instances. The first instance of the plug-in allows the user to place markers on an equirectangular representation of the sphere. Alternatively the markers can automatically follow the directions of sound sources through use of the tracker. The source beamformer signals are then outputted by the first instance of the plug-in (A), where the user can then apply any conventional single-channel audio effect, re-balance their levels, or re-order the signals. These manipulated beamformer signals are then passed to the second instance of the plug-in (B), which also receives the residual signals from the first plug-in instance internally, and the COMPASS synthesis is conducted to obtain the output SH signals. Alternatively, the residual stream signals may be outputted by the first plug-in instance instead (and the beamformer signals passed internally to the second plug-in instance), which instead allows conventional linear  Ambisonics transformations to be applied to only the ambient parts of the scene.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Archontis Politis.\u003c/p\u003e\n\u003ch3 id="gravitator"\u003eGravitator\u003c/h3\u003e\n\u003cp\u003e\u003cimg src="GravitatorGUI_alpha6_src%2076%200%20%20-24%2045%20-80%20-30.png" alt="" width="650;"\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003eA spatial focussor which pulls only the directional components of the sound scene towards user defined markers, with a certain degree of \u0026ldquo;gravitational-pull\u0026rdquo;. The Ambient components of the sound scene remain unaltered.\u003c/p\u003e\n\u003cp\u003eThis plug-in was developed by Leo McCormack and Archontis Politis.\u003c/p\u003e\n\u003ch2 id="about-the-developers"\u003eAbout the developers\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLeo McCormack\u003c/strong\u003e: a doctoral candidate at Aalto University.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eArchontis Politis\u003c/strong\u003e post doctorate researcher at Tampere University, specialising in spatial sound recording and reproduction, acoustic scene analysis and microphone array processing.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="references"\u003eReferences\u003c/h2\u003e\n\u003cp\u003e\u003ca id="ref_compass"\u003e\u003c/a\u003e[1] Politis, A., Tervo S., and Pulkki, V. (2018) \u003ca href="http://research.spa.aalto.fi/projects/sparta_vsts/publications/politis2019compass.pdf"\u003e\u003cb\u003eCOMPASS: Coding and Multidirectional Parameterization of Ambisonic Sound Scenes.\u003c/b\u003e\u003c/a\u003e \u003cbr\u003e IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).\u003c/p\u003e\n\u003cp\u003e[2] Pulkki, V. (2007) \u003cb\u003eSpatial sound reproduction with directional audio coding.\u003c/b\u003e \u003cbr\u003e Journal of the Audio Engineering Society 55.6: 503-516.\u003c/p\u003e\n\u003cp\u003e[3] Pulkki, V., Politis, A., Laitinen, M.-V., Vilkamo, J., Ahonen, J. (2017). \u003cb\u003eFirst-order directional audio coding (DirAC).\u003c/b\u003e \u003cbr\u003e \u003ci\u003ein\u003c/i\u003e Parametric Time-Frequency Domain Spatial Audio, Wiley, p.89-138.\u003c/p\u003e\n\u003cp\u003e[4] Berge, S. and Barrett, N. (2010). \u003cb\u003eHigh angular resolution planewave expansion.\u003c/b\u003e \u003cbr\u003e 2nd International Symposium on Ambisonics and Spherical Acoustics.\u003c/p\u003e\n\u003cp\u003e\u003ca id="hodirac_2015"\u003e\u003c/a\u003e[5] Politis, A., Vilkamo, J., and Pulkki, V. (2015). \u003cb\u003e\u003ca href="http://research.spa.aalto.fi/projects/sparta_vsts/publications/politis2015sector.pdf"\u003e\u003cb\u003eSector-based parametric sound field reproduction in the spherical harmonic domain.\u003c/b\u003e\u003c/a\u003e\u003c/b\u003e \u003cbr\u003e\nIEEE Journal of Selected Topics in Signal Processing, 9(5), 852-866.\u003c/p\u003e\n\u003cp\u003e\u003ca id="hodirac_2017"\u003e\u003c/a\u003e[6] Politis, A., McCormack, L., and Pulkki, V. (2017, October). \u003cb\u003e\u003ca href="http://research.spa.aalto.fi/projects/sparta_vsts/publications/politis2017enhancement.pdf"\u003e\u003cb\u003e Enhancement of ambisonic binaural reproduction using directional audio coding with optimal adaptive mixing\u003c/b\u003e\u003c/a\u003e.\u003c/b\u003e \u003cbr\u003e In 2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA) (pp. 379-383). IEEE.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u0026lt;p\u0026gt;[7] Politis, A. and Pulkki, V. (2017). \u0026lt;b\u0026gt;Higher-Order Directional Audio Coding.\u0026lt;/b\u0026gt; \u0026lt;br\u0026gt;\n\u0026lt;i\u0026gt;in\u0026lt;/i\u0026gt; Parametric Time-Frequency Domain Spatial Audio, Wiley, p.141.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;[8] Wabnitz, A., Epain, N., McEwan, A., Jin, C. (2011). \u0026lt;b\u0026gt;Upscaling ambisonic sound scenes using compressed sensing techniques.\u0026lt;/b\u0026gt; \u0026lt;br\u0026gt; IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA).\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;[9] Kolundzija, M., and Faller, C. (2018). \u0026lt;b\u0026gt;Advanced B-Format Analysis.\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt; Audio Engineering Society Convention 144.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;[10] Sch\u0026amp;ouml;rkhuber, C., and H\u0026amp;ouml;ldrich, R. (2019, March). \u0026lt;b\u0026gt;Linearly and Quadratically Constrained Least-Squares Decoder for Signal-Dependent Binaural Rendering of Ambisonic Signals.\u0026lt;/b\u0026gt; \n    \u0026lt;br\u0026gt; \u0026lt;i\u0026gt;in\u0026lt;/i\u0026gt; Audio Engineering Society Conference: 2019 AES International Conference on Immersive and Interactive Audio. Audio Engineering Society. \u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;[11] S\u0026amp;auml;rkk\u0026amp;auml;, S., Vehtari, A. and Lampinen, J., 2004, June. \u0026lt;b\u0026gt;Rao-Blackwellized Monte Carlo data association for multiple target tracking. \u0026lt;/b\u0026gt; \u0026lt;br\u0026gt;\u0026lt;i\u0026gt;in\u0026lt;/i\u0026gt; Proceedings of the seventh international conference on information fusion (Vol. 1, pp. 583-590). I.\u0026lt;/p\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n'},{id:3,href:"https://doks-child-theme.netlify.app/docs/help/how-to-update/",title:"How to Update",description:"Regularly update the installed npm packages to keep your Doks website stable, usable, and secure.",content:'\u003cdiv class="alert alert-warning d-flex" role="alert"\u003e\r\n  \u003cdiv class="flex-shrink-1 alert-icon"\u003eðŸ’¡ \u003c/div\u003e\r\n  \r\n    \u003cdiv class="w-100"\u003eLearn more about \u003ca href="https://docs.npmjs.com/about-semantic-versioning"\u003esemantic versioning\u003c/a\u003e and \u003ca href="https://docs.npmjs.com/cli/v6/using-npm/semver#advanced-range-syntax"\u003eadvanced range syntax\u003c/a\u003e. \u003c/div\u003e\r\n  \r\n\u003c/div\u003e\r\n\n\u003ch2 id="check-for-outdated-packages"\u003eCheck for outdated packages\u003c/h2\u003e\n\u003cp\u003eThe \u003ca href="https://docs.npmjs.com/cli/v7/commands/npm-outdated"\u003e\u003ccode\u003enpm outdated\u003c/code\u003e\u003c/a\u003e command will check the registry to see if any (or, specific) installed packages are currently outdated:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003enpm outdated [[\u0026lt;@scope\u0026gt;/]\u0026lt;pkg\u0026gt; ...]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="update-packages"\u003eUpdate packages\u003c/h2\u003e\n\u003cp\u003eThe \u003ca href="https://docs.npmjs.com/cli/v7/commands/npm-update"\u003e\u003ccode\u003enpm update\u003c/code\u003e\u003c/a\u003e command will update all the packages listed to the latest version (specified by the tag config), respecting semver:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003enpm update [\u0026lt;pkg\u0026gt;...]\n\u003c/code\u003e\u003c/pre\u003e\n'},{id:4,href:"https://doks-child-theme.netlify.app/docs/help/related-publications/",title:"Related publications",description:"A list of publications related to the included plug-ins.",content:'\u003ch2 id="mccormack2020higher"\u003emccormack2020higher\u003c/h2\u003e\n\u003cp\u003eMcCormack, L., Pulkki, V., Politis, A., Scheuregger, O. and Marschall, M., 2020. \u003cstrong\u003eHigher-order spatial impulse response rendering: Investigating the perceived effects of spherical order, dedicated diffuse rendering, and frequency resolution.\u003c/strong\u003e Journal of the Audio Engineering Society, 68(5), pp.338-354.\u003c/p\u003e\n\u003cobject data="mccormack2020higher.pdf" type="application/pdf" width="700px" height="700px"\u003e\n    \u003cembed src="mccormack2020higher.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="mccormack2020higher.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@article{mccormack2020higher,\n  title={Higher-order spatial impulse response rendering: Investigating the perceived effects of spherical order, dedicated diffuse rendering, and frequency resolution},\n  author={McCormack, Leo and Pulkki, Ville and Politis, Archontis and Scheuregger, Oliver and Marschall, Marton},\n  journal={Journal of the Audio Engineering Society},\n  volume={68},\n  number={5},\n  pages={338--354},\n  year={2020},\n  publisher={Audio Engineering Society}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="mccormack2019sparta"\u003emccormack2019sparta\u003c/h2\u003e\n\u003cp\u003eMcCormack, L. and Politis, A., 2019, March. SPARTA \u0026amp; COMPASS: Real-time implementations of linear and parametric spatial audio reproduction and processing methods. In Audio Engineering Society Conference: 2019 AES International Conference on Immersive and Interactive Audio. Audio Engineering Society.\u003c/p\u003e\n\u003cobject data="mccormack2019sparta.pdf" type="application/pdf" width="700px" height="700px"\u003e\n    \u003cembed src="mccormack2019sparta.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="mccormack2019sparta.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{mccormack2019sparta,\n  title={SPARTA \\\u0026amp; COMPASS: Real-time implementations of linear and parametric spatial audio reproduction and processing methods},\n  author={McCormack, Leo and Politis, Archontis},\n  booktitle={Audio Engineering Society Conference: 2019 AES International Conference on Immersive and Interactive Audio},\n  year={2019},\n  organization={Audio Engineering Society}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="mccormack2019sharpening"\u003emccormack2019sharpening\u003c/h2\u003e\n\u003cp\u003eMcCormack, L., Politis, A. and Pulkki, V., 2019, May. \u003cstrong\u003eSharpening of Angular Spectra Based on a Directional Re-assignment Approach for Ambisonic Sound-field Visualisation.\u003c/strong\u003e In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 576-580). IEEE.\u003c/p\u003e\n\u003cobject data="mccormack2019sharpening.pdf" type="application/pdf" width="700px" height="700px"\u003e\n    \u003cembed src="mccormack2019sharpening.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="mccormack2019sharpening.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{mccormack2019sharpening,\n  title={Sharpening of Angular Spectra Based on a Directional Re-assignment Approach for Ambisonic Sound-field Visualisation},\n  author={McCormack, Leo and Politis, Archontis and Pulkki, Ville},\n  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n  pages={576--580},\n  year={2019},\n  organization={IEEE}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="mccormack2019parametric"\u003emccormack2019parametric\u003c/h2\u003e\n\u003cp\u003eMcCormack, L. and Delikaris-Manias, S., 2019, September. \u003cstrong\u003eParametric first-order ambisonic decoding for headphones utilising the cross-pattern coherence algorithm.\u003c/strong\u003e In EAA Spatial Audio Signal Processing Symposium (pp. 173-178).\u003c/p\u003e\n\u003cobject data="mccormack2019parametric.pdf" type="application/pdf" width="700px" height="700px"\u003e\n    \u003cembed src="mccormack2019parametric.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="mccormack2019parametric.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{mccormack2019parametric,\n  title={Parametric first-order ambisonic decoding for headphones utilising the cross-pattern coherence algorithm},\n  author={McCormack, Leo and Delikaris-Manias, Symeon},\n  booktitle={EAA Spatial Audio Signal Processing Symposium},\n  pages={173--178},\n  year={2019}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="mccormack2019higher"\u003emccormack2019higher\u003c/h2\u003e\n\u003cp\u003eMcCormack, L., Politis, A., Scheuregger, O. and Pulkki, V., 2019. \u003cstrong\u003eHigher-order processing of spatial impulse responses.\u003c/strong\u003e UniversitÃ¤tsbibliothek der RWTH Aachen.\u003c/p\u003e\n\u003cobject data="mccormack2019higher.pdf" type="application/pdf" width="700px" height="700px"\u003e\n    \u003cembed src="mccormack2019higher.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="mccormack2019higher.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@book{mccormack2019higher,\n  title={Higher-order processing of spatial impulse responses},\n  author={McCormack, Leo and Politis, Archontis and Scheuregger, Oliver and Pulkki, Ville},\n  year={2019},\n  publisher={Universit{\\\u0026quot;a}tsbibliothek der RWTH Aachen}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="mccormack2019applications"\u003emccormack2019applications\u003c/h2\u003e\n\u003cp\u003eMcCormack, L., Delikaris-Manias, S., Politis, A., Pavlidi, D., Farina, A., Pinardi, D. and Pulkki, V., 2019. \u003cstrong\u003eApplications of spatially localized active-intensity vectors for sound-field visualization.\u003c/strong\u003e Journal of the Audio Engineering Society, 67(11), pp.840-854.\u003c/p\u003e\n\u003cobject data="mccormack2019applications.pdf" type="application/pdf" width="700px" height="700px"\u003e\n    \u003cembed src="mccormack2019applications.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="mccormack2019applications.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@article{mccormack2019applications,\n  title={Applications of spatially localized active-intensity vectors for sound-field visualization},\n  author={McCormack, Leo and Delikaris-Manias, Symeon and Politis, Archontis and Pavlidi, Despoina and Farina, Angelo and Pinardi, Daniel and Pulkki, Ville},\n  journal={Journal of the Audio Engineering Society},\n  volume={67},\n  number={11},\n  pages={840--854},\n  year={2019},\n  publisher={Audio Engineering Society}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="politis2018compass"\u003epolitis2018compass\u003c/h2\u003e\n\u003cp\u003ePolitis, A., Tervo, S. and Pulkki, V., 2018, April. \u003cstrong\u003eCOMPASS: Coding and multidirectional parameterization of ambisonic sound scenes.\u003c/strong\u003e In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 6802-6806). IEEE.\u003c/p\u003e\n\u003cobject data="politis2018compass.pdf" type="application/pdf" width="700px" height="700px"\u003e\n    \u003cembed src="politis2018compass.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="politis2018compass.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{politis2018compass,\n  title={Compass: Coding and multidirectional parameterization of ambisonic sound scenes},\n  author={Politis, Archontis and Tervo, Sakari and Pulkki, Ville},\n  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n  pages={6802--6806},\n  year={2018},\n  organization={IEEE}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="mccormack2018real"\u003emccormack2018real\u003c/h2\u003e\n\u003cp\u003eMcCormack, L., Delikaris-Manias, S., Farina, A., Pinardi, D. and Pulkki, V., 2018, May. \u003cstrong\u003eReal-time conversion of sensor array signals into spherical harmonic signals with applications to spatially localized sub-band sound-field analysis.\u003c/strong\u003e In Audio Engineering Society Convention 144. Audio Engineering Society.\u003c/p\u003e\n\u003cobject data="mccormack2018real.pdf" type="application/pdf" width="700px" height="700px"\u003e\n    \u003cembed src="mccormack2018real.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="mccormack2018real.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{mccormack2018real,\n  title={Real-time conversion of sensor array signals into spherical harmonic signals with applications to spatially localized sub-band sound-field analysis},\n  author={McCormack, Leo and Delikaris-Manias, Symeon and Farina, Angelo and Pinardi, Daniel and Pulkki, Ville},\n  booktitle={Audio Engineering Society Convention 144},\n  year={2018},\n  organization={Audio Engineering Society}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="mccormack2017parametric"\u003emccormack2017parametric\u003c/h2\u003e\n\u003cp\u003eMcCormack, L., Delikaris-Manias, S. and Pulkki, V., 2017, September. \u003cstrong\u003eParametric acoustic camera for real-time sound capture, analysis and tracking.\u003c/strong\u003e In Proceedings of the 20th International Conference on Digital Audio Effects (DAFx-17) (pp. 412-419).\u003c/p\u003e\n\u003cobject data="mccormack2017parametric.pdf" type="application/pdf" width="700px" height="700px"\u003e\n    \u003cembed src="mccormack2017parametric.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="mccormack2017parametric.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{mccormack2017parametric,\n  title={Parametric acoustic camera for real-time sound capture, analysis and tracking},\n  author={McCormack, Leo and Delikaris-Manias, Symeon and Pulkki, Ville},\n  booktitle={Proceedings of the 20th International Conference on Digital Audio Effects (DAFx-17)},\n  pages={412--419},\n  year={2017}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="politis2017enhancement"\u003epolitis2017enhancement\u003c/h2\u003e\n\u003cp\u003ePolitis, A., McCormack, L. and Pulkki, V., 2017, October. \u003cstrong\u003eEnhancement of ambisonic binaural reproduction using directional audio coding with optimal adaptive mixing.\u003c/strong\u003e In 2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA) (pp. 379-383). IEEE.\u003c/p\u003e\n\u003cobject data="politis2017enhancement.pdf" type="application/pdf" width="700px" height="700px"\u003e\n    \u003cembed src="politis2017enhancement.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="politis2017enhancement.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{politis2017enhancement,\n  title={Enhancement of ambisonic binaural reproduction using directional audio coding with optimal adaptive mixing},\n  author={Politis, Archontis and McCormack, Leo and Pulkki, Ville},\n  booktitle={2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},\n  pages={379--383},\n  year={2017},\n  organization={IEEE}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="politis2015sector"\u003epolitis2015sector\u003c/h2\u003e\n\u003cp\u003ePolitis, A., Vilkamo, J. and Pulkki, V., 2015. \u003cstrong\u003eSector-based parametric sound field reproduction in the spherical harmonic domain.\u003c/strong\u003e IEEE Journal of Selected Topics in Signal Processing, 9(5), pp.852-866.\u003c/p\u003e\n\u003cobject data="politis2015sector.pdf" type="application/pdf" width="700px" height="700px"\u003e\n    \u003cembed src="politis2015sector.pdf"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href="politis2015sector.pdf"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e\n    \u003c/embed\u003e\n\u003c/object\u003e\n\u003cpre\u003e\u003ccode\u003e@article{politis2015sector,\n  title={Sector-based parametric sound field reproduction in the spherical harmonic domain},\n  author={Politis, Archontis and Vilkamo, Juha and Pulkki, Ville},\n  journal={IEEE Journal of Selected Topics in Signal Processing},\n  volume={9},\n  number={5},\n  pages={852--866},\n  year={2015},\n  publisher={IEEE}\n}\n\u003c/code\u003e\u003c/pre\u003e\n'},{id:5,href:"https://doks-child-theme.netlify.app/docs/help/faq/",title:"FAQ",description:"Answers to frequently asked questions.",content:'\u003ch2 id="hyas"\u003eHyas?\u003c/h2\u003e\n\u003cp\u003eDoks is a \u003ca href="https://gethyas.com/themes/"\u003eHyas theme\u003c/a\u003e build by the creator of Hyas.\u003c/p\u003e\n\u003ch2 id="footer-notice"\u003eFooter notice?\u003c/h2\u003e\n\u003cp\u003ePlease keep it in place.\u003c/p\u003e\n\u003ch2 id="keyboard-shortcuts-for-search"\u003eKeyboard shortcuts for search?\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003efocus: \u003ccode\u003e/\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eselect: \u003ccode\u003eâ†“\u003c/code\u003e and \u003ccode\u003eâ†‘\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eopen: \u003ccode\u003eEnter\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eclose: \u003ccode\u003eEsc\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="other-documentation"\u003eOther documentation?\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href="https://docs.netlify.com/"\u003eNetlify\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://gohugo.io/documentation/"\u003eHugo\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="can-i-get-support"\u003eCan I get support?\u003c/h2\u003e\n\u003cp\u003eCreate a topic:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href="https://community.netlify.com/"\u003eNetlify Community\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://discourse.gohugo.io/"\u003eHugo Forums\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://github.com/h-enk/doks/discussions"\u003eDoks Discussions\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="contact-the-creator"\u003eContact the creator?\u003c/h2\u003e\n\u003cp\u003eSend \u003ccode\u003eh-enk\u003c/code\u003e a message:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href="https://community.netlify.com/"\u003eNetlify Community\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://discourse.gohugo.io/"\u003eHugo Forums\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://github.com/h-enk/doks/discussions"\u003eDoks Discussions\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n'},{id:6,href:"https://doks-child-theme.netlify.app/docs/help/",title:"Help",description:"Help Doks.",content:""},{id:7,href:"https://doks-child-theme.netlify.app/docs/plugins/",title:"The plug-ins",description:"All audio plug-ins included in the SPARTA installer.",content:""},{id:8,href:"https://doks-child-theme.netlify.app/docs/",title:"Docs",description:"Docs Doks.",content:""}];b.add(c),userinput.addEventListener('input',e,!0),suggestions.addEventListener('click',f,!0);function e(){var g=this.value,e=b.search(g,5),f=suggestions.childNodes,h=0,i=e.length,c;for(suggestions.classList.remove('d-none'),e.forEach(function(b){c=document.createElement('div'),c.innerHTML='<a href><span></span><span></span></a>',a=c.querySelector('a'),t=c.querySelector('span:first-child'),d=c.querySelector('span:nth-child(2)'),a.href=b.href,t.textContent=b.title,d.textContent=b.description,suggestions.appendChild(c)});f.length>i;)suggestions.removeChild(f[h])}function f(){while(suggestions.lastChild)suggestions.removeChild(suggestions.lastChild);return!1}})()